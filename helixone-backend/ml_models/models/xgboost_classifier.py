"""
XGBoost Classifier - Pr√©diction de direction (UP/DOWN/FLAT)

Objectifs:
- Pr√©cision >72%
- Pr√©dire direction sur 1 jour, 3 jours, 7 jours
- Hyperparameter tuning avec Optuna
- SHAP explainability

Classes:
- UP: Price change > +1%
- DOWN: Price change < -1%
- FLAT: Price change entre -1% et +1%
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Optional
import logging
import xgboost as xgb
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import optuna
import joblib
from pathlib import Path
import json
from datetime import datetime

logger = logging.getLogger(__name__)


class XGBoostClassifier:
    """
    Classificateur XGBoost pour pr√©dire la direction du prix
    """

    def __init__(
        self,
        prediction_horizon: int = 1,
        up_threshold: float = 0.01,
        down_threshold: float = -0.01
    ):
        """
        Args:
            prediction_horizon: Nombre de jours √† pr√©dire (1, 3, 7)
            up_threshold: Seuil pour classe UP (d√©faut +1%)
            down_threshold: Seuil pour classe DOWN (d√©faut -1%)
        """
        self.prediction_horizon = prediction_horizon
        self.up_threshold = up_threshold
        self.down_threshold = down_threshold

        self.model = None
        self.feature_names = []
        self.best_params = {}
        self.training_metrics = {}

        logger.info(f"XGBoost Classifier initialis√© (horizon={prediction_horizon}j)")

    def prepare_labels(self, df: pd.DataFrame) -> pd.Series:
        """
        Cr√©e les labels UP/DOWN/FLAT

        Args:
            df: DataFrame avec colonne 'close'

        Returns:
            Series avec labels (0=DOWN, 1=FLAT, 2=UP)
        """
        # Calculer le return futur
        future_return = df['close'].pct_change(periods=self.prediction_horizon).shift(-self.prediction_horizon)

        # Cr√©er les classes
        labels = pd.Series(1, index=df.index)  # Par d√©faut FLAT
        labels[future_return > self.up_threshold] = 2  # UP
        labels[future_return < self.down_threshold] = 0  # DOWN

        # Statistiques
        value_counts = labels.value_counts()
        logger.info(f"Distribution des labels:")
        logger.info(f"   DOWN (0): {value_counts.get(0, 0)} ({value_counts.get(0, 0)/len(labels)*100:.1f}%)")
        logger.info(f"   FLAT (1): {value_counts.get(1, 0)} ({value_counts.get(1, 0)/len(labels)*100:.1f}%)")
        logger.info(f"   UP (2): {value_counts.get(2, 0)} ({value_counts.get(2, 0)/len(labels)*100:.1f}%)")

        return labels

    def optimize_hyperparameters(
        self,
        X_train: pd.DataFrame,
        y_train: pd.Series,
        n_trials: int = 50
    ) -> Dict:
        """
        Optimise les hyperparam√®tres avec Optuna

        Args:
            X_train: Features d'entra√Ænement
            y_train: Labels d'entra√Ænement
            n_trials: Nombre d'essais Optuna

        Returns:
            Meilleurs param√®tres
        """
        logger.info(f"Optimisation hyperparam√®tres avec {n_trials} trials...")

        def objective(trial):
            params = {
                'max_depth': trial.suggest_int('max_depth', 3, 10),
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
                'n_estimators': trial.suggest_int('n_estimators', 100, 1000, step=100),
                'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
                'gamma': trial.suggest_float('gamma', 0, 0.5),
                'subsample': trial.suggest_float('subsample', 0.6, 1.0),
                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
                'reg_alpha': trial.suggest_float('reg_alpha', 0, 1),
                'reg_lambda': trial.suggest_float('reg_lambda', 0, 1),
                'random_state': 42,
                'tree_method': 'hist',
                'eval_metric': 'mlogloss',
                'num_class': 3
            }

            # Cross-validation time series
            tscv = TimeSeriesSplit(n_splits=3)
            scores = []

            for train_idx, val_idx in tscv.split(X_train):
                X_tr = X_train.iloc[train_idx]
                y_tr = y_train.iloc[train_idx]
                X_val = X_train.iloc[val_idx]
                y_val = y_train.iloc[val_idx]

                model = xgb.XGBClassifier(**params)
                model.fit(
                    X_tr, y_tr,
                    eval_set=[(X_val, y_val)],
                    verbose=False
                )

                y_pred = model.predict(X_val)
                score = accuracy_score(y_val, y_pred)
                scores.append(score)

            return np.mean(scores)

        # Optimisation
        study = optuna.create_study(direction='maximize')
        study.optimize(objective, n_trials=n_trials, show_progress_bar=True)

        logger.info(f"‚úÖ Meilleure accuracy: {study.best_value:.4f}")
        logger.info(f"   Meilleurs params: {study.best_params}")

        self.best_params = study.best_params
        return study.best_params

    def train(
        self,
        X_train: pd.DataFrame,
        y_train: pd.Series,
        X_val: Optional[pd.DataFrame] = None,
        y_val: Optional[pd.Series] = None,
        optimize: bool = True,
        n_trials: int = 50
    ):
        """
        Entra√Æne le mod√®le XGBoost

        Args:
            X_train: Features d'entra√Ænement
            y_train: Labels d'entra√Ænement
            X_val: Features de validation (optionnel)
            y_val: Labels de validation (optionnel)
            optimize: Si True, optimise les hyperparam√®tres
            n_trials: Nombre de trials pour Optuna
        """
        logger.info(f"Entra√Ænement XGBoost (samples={len(X_train)}, features={len(X_train.columns)})...")

        self.feature_names = list(X_train.columns)

        # Optimisation hyperparam√®tres
        if optimize:
            params = self.optimize_hyperparameters(X_train, y_train, n_trials=n_trials)
        else:
            params = {
                'max_depth': 6,
                'learning_rate': 0.1,
                'n_estimators': 300,
                'min_child_weight': 3,
                'gamma': 0.1,
                'subsample': 0.8,
                'colsample_bytree': 0.8,
                'reg_alpha': 0.1,
                'reg_lambda': 0.1,
                'random_state': 42,
                'tree_method': 'hist',
                'eval_metric': 'mlogloss',
                'num_class': 3
            }

        # Entra√Ænement final
        self.model = xgb.XGBClassifier(**params)

        eval_set = [(X_train, y_train)]
        if X_val is not None and y_val is not None:
            eval_set.append((X_val, y_val))

        self.model.fit(
            X_train,
            y_train,
            eval_set=eval_set,
            verbose=True
        )

        # M√©triques d'entra√Ænement
        y_train_pred = self.model.predict(X_train)
        train_accuracy = accuracy_score(y_train, y_train_pred)

        self.training_metrics = {
            'train_accuracy': train_accuracy,
            'train_samples': len(X_train),
            'n_features': len(self.feature_names),
            'prediction_horizon': self.prediction_horizon
        }

        logger.info(f"‚úÖ Entra√Ænement termin√©!")
        logger.info(f"   Train accuracy: {train_accuracy:.4f}")

        # Validation
        if X_val is not None and y_val is not None:
            y_val_pred = self.model.predict(X_val)
            val_accuracy = accuracy_score(y_val, y_val_pred)
            self.training_metrics['val_accuracy'] = val_accuracy
            self.training_metrics['val_samples'] = len(X_val)

            logger.info(f"   Val accuracy: {val_accuracy:.4f}")

            # Classification report
            logger.info("\n" + classification_report(
                y_val,
                y_val_pred,
                target_names=['DOWN', 'FLAT', 'UP']
            ))

    def predict(self, X: pd.DataFrame) -> np.ndarray:
        """
        Pr√©dit les classes

        Args:
            X: Features

        Returns:
            Array de pr√©dictions (0=DOWN, 1=FLAT, 2=UP)
        """
        if self.model is None:
            raise ValueError("Mod√®le non entra√Æn√©! Appeler .train() d'abord.")

        return self.model.predict(X)

    def predict_proba(self, X: pd.DataFrame) -> np.ndarray:
        """
        Pr√©dit les probabilit√©s pour chaque classe

        Args:
            X: Features

        Returns:
            Array de shape (n_samples, 3) avec probas [DOWN, FLAT, UP]
        """
        if self.model is None:
            raise ValueError("Mod√®le non entra√Æn√©! Appeler .train() d'abord.")

        return self.model.predict_proba(X)

    def get_feature_importance(self, top_n: int = 20) -> pd.DataFrame:
        """
        Retourne l'importance des features

        Args:
            top_n: Nombre de top features √† retourner

        Returns:
            DataFrame avec features et importances
        """
        if self.model is None:
            raise ValueError("Mod√®le non entra√Æn√©!")

        importances = pd.DataFrame({
            'feature': self.feature_names,
            'importance': self.model.feature_importances_
        })

        importances = importances.sort_values('importance', ascending=False)

        return importances.head(top_n)

    def save(self, path: str):
        """
        Sauvegarde le mod√®le

        Args:
            path: Chemin du fichier (sans extension)
        """
        if self.model is None:
            raise ValueError("Aucun mod√®le √† sauvegarder!")

        path = Path(path)
        path.parent.mkdir(parents=True, exist_ok=True)

        # Sauvegarder le mod√®le XGBoost
        model_path = path.with_suffix('.json')
        self.model.save_model(str(model_path))

        # Sauvegarder les m√©tadonn√©es
        metadata = {
            'feature_names': self.feature_names,
            'best_params': self.best_params,
            'training_metrics': self.training_metrics,
            'prediction_horizon': self.prediction_horizon,
            'up_threshold': self.up_threshold,
            'down_threshold': self.down_threshold,
            'saved_at': datetime.now().isoformat()
        }

        metadata_path = path.with_suffix('.meta.json')
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)

        logger.info(f"‚úÖ Mod√®le sauvegard√©: {model_path}")

    def load(self, path: str):
        """
        Charge un mod√®le sauvegard√©

        Args:
            path: Chemin du fichier (sans extension)
        """
        path = Path(path)

        # Charger le mod√®le
        model_path = path.with_suffix('.json')
        self.model = xgb.XGBClassifier()
        self.model.load_model(str(model_path))

        # Charger les m√©tadonn√©es
        metadata_path = path.with_suffix('.meta.json')
        with open(metadata_path, 'r') as f:
            metadata = json.load(f)

        self.feature_names = metadata['feature_names']
        self.best_params = metadata['best_params']
        self.training_metrics = metadata['training_metrics']
        self.prediction_horizon = metadata['prediction_horizon']
        self.up_threshold = metadata['up_threshold']
        self.down_threshold = metadata['down_threshold']

        logger.info(f"‚úÖ Mod√®le charg√©: {model_path}")
        logger.info(f"   Sauvegard√© le: {metadata['saved_at']}")
        logger.info(f"   Train accuracy: {self.training_metrics.get('train_accuracy', 'N/A')}")

    def get_signal(self, X: pd.DataFrame) -> Dict:
        """
        G√©n√®re un signal de trading

        Args:
            X: Features (derni√®re ligne = position actuelle)

        Returns:
            Dict avec signal et probabilit√©s
        """
        if self.model is None:
            raise ValueError("Mod√®le non entra√Æn√©!")

        # Pr√©diction
        pred = self.predict(X.tail(1))[0]
        proba = self.predict_proba(X.tail(1))[0]

        # Classes
        classes = ['DOWN', 'FLAT', 'UP']
        pred_class = classes[pred]

        # Confiance = probabilit√© de la classe pr√©dite
        confidence = proba[pred]

        signal = {
            'prediction': pred_class,
            'confidence': float(confidence),
            'probabilities': {
                'DOWN': float(proba[0]),
                'FLAT': float(proba[1]),
                'UP': float(proba[2])
            },
            'horizon_days': self.prediction_horizon,
            'action': 'BUY' if pred == 2 else ('SELL' if pred == 0 else 'HOLD')
        }

        return signal


# ============================================================================
# MULTI-HORIZON CLASSIFIER
# ============================================================================

class MultiHorizonClassifier:
    """
    Combine 3 classifiers pour pr√©dictions multi-horizons (1j, 3j, 7j)
    """

    def __init__(self):
        self.classifiers = {
            '1d': XGBoostClassifier(prediction_horizon=1),
            '3d': XGBoostClassifier(prediction_horizon=3),
            '7d': XGBoostClassifier(prediction_horizon=7)
        }

        logger.info("MultiHorizonClassifier initialis√© (1d, 3d, 7d)")

    def train_all(
        self,
        df: pd.DataFrame,
        features: List[str],
        train_split: float = 0.8,
        optimize: bool = True,
        n_trials: int = 30
    ):
        """
        Entra√Æne les 3 classifiers

        Args:
            df: DataFrame complet avec features et 'close'
            features: Liste des colonnes features
            train_split: Ratio train/test
            optimize: Optimiser hyperparam√®tres
            n_trials: Trials Optuna
        """
        logger.info("=" * 80)
        logger.info("ENTRA√éNEMENT MULTI-HORIZON")
        logger.info("=" * 80)

        # Split train/test (time series)
        split_idx = int(len(df) * train_split)
        df_train = df.iloc[:split_idx]
        df_test = df.iloc[split_idx:]

        logger.info(f"\nDataset:")
        logger.info(f"   Total: {len(df)} jours")
        logger.info(f"   Train: {len(df_train)} jours ({train_split*100:.0f}%)")
        logger.info(f"   Test:  {len(df_test)} jours ({(1-train_split)*100:.0f}%)")

        # Entra√Æner chaque classifier
        results = {}

        for horizon, clf in self.classifiers.items():
            logger.info(f"\n{'='*80}")
            logger.info(f"HORIZON: {horizon}")
            logger.info(f"{'='*80}")

            # Pr√©parer labels
            y_train = clf.prepare_labels(df_train)
            y_test = clf.prepare_labels(df_test)

            # Supprimer NaN (derni√®res lignes)
            valid_train = ~y_train.isna()
            valid_test = ~y_test.isna()

            X_train = df_train.loc[valid_train, features]
            y_train = y_train[valid_train]
            X_test = df_test.loc[valid_test, features]
            y_test = y_test[valid_test]

            # Entra√Æner
            clf.train(
                X_train,
                y_train,
                X_val=X_test,
                y_val=y_test,
                optimize=optimize,
                n_trials=n_trials
            )

            # Feature importance
            logger.info(f"\nüìä Top 10 features ({horizon}):")
            top_features = clf.get_feature_importance(top_n=10)
            for idx, row in top_features.iterrows():
                logger.info(f"   {row['feature']}: {row['importance']:.4f}")

            results[horizon] = clf.training_metrics

        logger.info("\n" + "=" * 80)
        logger.info("‚úÖ ENTRA√éNEMENT TERMIN√â")
        logger.info("=" * 80)

        # R√©sum√©
        logger.info("\nüìä R√âSUM√â DES PERFORMANCES:")
        for horizon, metrics in results.items():
            logger.info(f"\n{horizon}:")
            logger.info(f"   Train accuracy: {metrics['train_accuracy']:.4f}")
            if 'val_accuracy' in metrics:
                logger.info(f"   Val accuracy:   {metrics['val_accuracy']:.4f}")

    def get_multi_horizon_signal(self, X: pd.DataFrame) -> Dict:
        """
        G√©n√®re des signaux pour tous les horizons

        Args:
            X: Features (derni√®re ligne = position actuelle)

        Returns:
            Dict avec signaux pour 1d, 3d, 7d
        """
        signals = {}

        for horizon, clf in self.classifiers.items():
            signals[horizon] = clf.get_signal(X)

        # Consensus
        predictions = [s['prediction'] for s in signals.values()]

        # Score consensus (toutes les pr√©dictions sont identiques)
        consensus_score = len(set(predictions)) == 1

        return {
            'signals': signals,
            'consensus': predictions[0] if consensus_score else 'MIXED',
            'consensus_score': 100 if consensus_score else 50
        }

    def save_all(self, base_path: str):
        """Sauvegarde tous les classifiers"""
        for horizon, clf in self.classifiers.items():
            path = Path(base_path) / f"xgb_{horizon}"
            clf.save(str(path))

    def load_all(self, base_path: str):
        """Charge tous les classifiers"""
        for horizon, clf in self.classifiers.items():
            path = Path(base_path) / f"xgb_{horizon}"
            clf.load(str(path))


# ============================================================================
# SINGLETON
# ============================================================================

_multi_horizon_classifier = None

def get_multi_horizon_classifier() -> MultiHorizonClassifier:
    """Retourne instance singleton"""
    global _multi_horizon_classifier
    if _multi_horizon_classifier is None:
        _multi_horizon_classifier = MultiHorizonClassifier()
    return _multi_horizon_classifier


# ============================================================================
# EXEMPLE D'UTILISATION
# ============================================================================

if __name__ == '__main__':
    logging.basicConfig(level=logging.INFO)

    print("=" * 80)
    print("XGBOOST CLASSIFIER - Test")
    print("=" * 80)

    # Cr√©er donn√©es synth√©tiques
    dates = pd.date_range(start='2020-01-01', end='2023-12-31', freq='D')
    np.random.seed(42)

    df = pd.DataFrame({
        'close': 100 + np.cumsum(np.random.randn(len(dates)) * 2),
        'volume': np.random.uniform(1e6, 5e6, len(dates)),
        'rsi_14': np.random.uniform(30, 70, len(dates)),
        'macd': np.random.randn(len(dates)),
        'bb_width': np.random.uniform(0, 5, len(dates))
    }, index=dates)

    features = ['volume', 'rsi_14', 'macd', 'bb_width']

    print(f"\nüìä Dataset: {len(df)} jours, {len(features)} features")

    # Test classifier 1 jour
    print("\n" + "=" * 80)
    print("TEST 1: Classifier 1 jour")
    print("=" * 80)

    clf = XGBoostClassifier(prediction_horizon=1)

    # Pr√©parer donn√©es
    y = clf.prepare_labels(df)
    valid = ~y.isna()

    X = df.loc[valid, features]
    y = y[valid]

    # Split
    split = int(len(X) * 0.8)
    X_train, X_test = X.iloc[:split], X.iloc[split:]
    y_train, y_test = y.iloc[:split], y.iloc[split:]

    # Entra√Æner (sans optimization pour aller vite)
    clf.train(X_train, y_train, X_test, y_test, optimize=False)

    # Test pr√©diction
    signal = clf.get_signal(X_test)
    print(f"\nüìà Signal de test:")
    print(f"   Pr√©diction: {signal['prediction']}")
    print(f"   Confiance: {signal['confidence']:.2%}")
    print(f"   Action: {signal['action']}")

    print("\n‚úÖ Test termin√©!")
