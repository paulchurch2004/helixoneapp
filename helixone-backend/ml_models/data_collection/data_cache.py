"""
Data Cache - Interface unifiÃ©e pour Yahoo Finance + FRED + Sentiment

Cette classe combine toutes les sources de donnÃ©es et fournit un dataset
ML-ready avec toutes les features nÃ©cessaires.

Utilisation:
    cache = DataCache()
    dataset = cache.get_ml_dataset(
        tickers=['AAPL', 'MSFT'],
        start_date='2014-01-01',
        include_sentiment=True
    )
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import List, Optional, Dict
import logging
from pathlib import Path

from .yahoo_finance_downloader import YahooFinanceDownloader
from .fred_macro_downloader import FREDMacroDownloader

logger = logging.getLogger(__name__)


class DataCache:
    """
    Cache centralisÃ© combinant toutes les sources de donnÃ©es
    """

    def __init__(
        self,
        yahoo_cache_path: str = 'data/yahoo_finance_cache.db',
        fred_cache_path: str = 'data/fred_macro_cache.db',
        fred_api_key: Optional[str] = None
    ):
        """
        Args:
            yahoo_cache_path: Chemin cache Yahoo Finance
            fred_cache_path: Chemin cache FRED
            fred_api_key: ClÃ© API FRED (optionnelle)
        """
        # CrÃ©er le dossier data s'il n'existe pas
        Path('data').mkdir(exist_ok=True)

        self.yahoo_downloader = YahooFinanceDownloader(cache_path=yahoo_cache_path)
        self.fred_downloader = FREDMacroDownloader(
            api_key=fred_api_key,
            cache_path=fred_cache_path
        )

        logger.info("DataCache initialisÃ©")

    def get_ml_dataset(
        self,
        tickers: List[str],
        start_date: str = None,
        end_date: str = None,
        include_macro: bool = True,
        include_sentiment: bool = False,
        use_cache: bool = True
    ) -> Dict[str, pd.DataFrame]:
        """
        CrÃ©e un dataset ML-ready pour une liste de tickers

        Args:
            tickers: Liste de tickers
            start_date: Date dÃ©but
            end_date: Date fin
            include_macro: Inclure donnÃ©es macro FRED
            include_sentiment: Inclure sentiment (nÃ©cessite services actifs)
            use_cache: Utiliser cache

        Returns:
            Dict {ticker: DataFrame avec toutes les features}
        """
        if start_date is None:
            start_date = (datetime.now() - timedelta(days=365*10)).strftime('%Y-%m-%d')

        if end_date is None:
            end_date = datetime.now().strftime('%Y-%m-%d')

        logger.info(f"ðŸ“¦ CrÃ©ation dataset ML pour {len(tickers)} tickers")

        # 1. TÃ©lÃ©charger donnÃ©es prix
        logger.info("ðŸ“Š TÃ©lÃ©chargement donnÃ©es prix...")
        price_data = self.yahoo_downloader.download_historical_data(
            tickers=tickers,
            start_date=start_date,
            end_date=end_date,
            use_cache=use_cache
        )

        # 2. TÃ©lÃ©charger donnÃ©es macro
        macro_data = None
        if include_macro:
            logger.info("ðŸŒ TÃ©lÃ©chargement donnÃ©es macro...")
            macro_data = self.fred_downloader.download_all_indicators(
                start_date=start_date,
                end_date=end_date,
                use_cache=use_cache
            )

        # 3. Combiner tout
        logger.info("ðŸ”— Fusion des donnÃ©es...")
        ml_dataset = {}

        for ticker, df_price in price_data.items():
            # Commencer avec prix
            df = df_price.copy()

            # Ajouter donnÃ©es macro (merge sur date)
            if macro_data is not None and not macro_data.empty:
                # Reindexer macro sur les mÃªmes dates que prix
                macro_aligned = macro_data.reindex(df.index, method='ffill')
                df = pd.concat([df, macro_aligned], axis=1)

            # TODO: Ajouter sentiment si demandÃ©
            # if include_sentiment:
            #     sentiment_data = self._get_sentiment_features(ticker, df.index)
            #     df = pd.concat([df, sentiment_data], axis=1)

            ml_dataset[ticker] = df

        logger.info(f"âœ… Dataset crÃ©Ã©: {len(ml_dataset)} tickers")
        if ml_dataset:
            first_ticker = list(ml_dataset.keys())[0]
            logger.info(f"   Exemple {first_ticker}: {len(ml_dataset[first_ticker])} jours, {len(ml_dataset[first_ticker].columns)} features")

        return ml_dataset

    def update_all_caches(self, tickers: Optional[List[str]] = None):
        """
        Met Ã  jour tous les caches (Yahoo + FRED)

        Args:
            tickers: Liste de tickers Ã  mettre Ã  jour (None = tous)
        """
        logger.info("ðŸ”„ Mise Ã  jour des caches...")

        # Update Yahoo
        logger.info("ðŸ“Š Mise Ã  jour Yahoo Finance...")
        self.yahoo_downloader.update_cache(tickers)

        # Update FRED (pas de tickers, ce sont des sÃ©ries macro)
        if self.fred_downloader.fred:
            logger.info("ðŸŒ FRED dÃ©jÃ  en cache (pas de mise Ã  jour nÃ©cessaire)")

        logger.info("âœ… Caches mis Ã  jour")

    def get_latest_data(self, ticker: str) -> Optional[pd.Series]:
        """
        RÃ©cupÃ¨re les derniÃ¨res donnÃ©es pour un ticker

        Args:
            ticker: Ticker

        Returns:
            Series avec derniÃ¨res donnÃ©es ou None
        """
        try:
            # TÃ©lÃ©charger dernier jour
            data = self.yahoo_downloader.download_historical_data(
                tickers=[ticker],
                start_date=(datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d'),
                use_cache=False
            )

            if ticker in data and not data[ticker].empty:
                return data[ticker].iloc[-1]

            return None

        except Exception as e:
            logger.error(f"Erreur rÃ©cupÃ©ration derniÃ¨res donnÃ©es {ticker}: {e}")
            return None

    def get_cache_info(self) -> Dict:
        """
        Retourne des informations sur l'Ã©tat des caches

        Returns:
            Dict avec statistiques
        """
        info = {
            'yahoo': {},
            'fred': {}
        }

        try:
            import sqlite3

            # Yahoo Finance cache
            conn = sqlite3.connect(self.yahoo_downloader.cache_path)
            cursor = conn.cursor()

            # Nombre de tickers
            cursor.execute("SELECT COUNT(DISTINCT ticker) FROM download_metadata")
            info['yahoo']['num_tickers'] = cursor.fetchone()[0]

            # Total records
            cursor.execute("SELECT SUM(total_records) FROM download_metadata")
            info['yahoo']['total_records'] = cursor.fetchone()[0] or 0

            # PÃ©riode couverte
            cursor.execute("SELECT MIN(start_date), MAX(end_date) FROM download_metadata")
            result = cursor.fetchone()
            info['yahoo']['period'] = f"{result[0]} â†’ {result[1]}"

            conn.close()

            # FRED cache
            conn = sqlite3.connect(self.fred_downloader.cache_path)
            cursor = conn.cursor()

            # Nombre de sÃ©ries
            cursor.execute("SELECT COUNT(DISTINCT series_id) FROM series_metadata")
            info['fred']['num_series'] = cursor.fetchone()[0]

            # Total records
            cursor.execute("SELECT SUM(total_records) FROM series_metadata")
            info['fred']['total_records'] = cursor.fetchone()[0] or 0

            conn.close()

        except Exception as e:
            logger.error(f"Erreur rÃ©cupÃ©ration info cache: {e}")

        return info


# ============================================================================
# SINGLETON
# ============================================================================

_data_cache_instance = None

def get_data_cache(fred_api_key: Optional[str] = None) -> DataCache:
    """Retourne une instance singleton du cache"""
    global _data_cache_instance
    if _data_cache_instance is None:
        _data_cache_instance = DataCache(fred_api_key=fred_api_key)
    return _data_cache_instance


# ============================================================================
# EXEMPLE D'UTILISATION
# ============================================================================

if __name__ == '__main__':
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )

    print("=" * 80)
    print("DATA CACHE - Test Complet")
    print("=" * 80)

    # CrÃ©er cache
    cache = DataCache()

    # TÃ©lÃ©charger dataset pour 5 stocks
    tickers = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'NVDA']

    print(f"\nðŸ“¥ TÃ©lÃ©chargement dataset pour {len(tickers)} stocks (5 ans)...")
    dataset = cache.get_ml_dataset(
        tickers=tickers,
        start_date='2019-01-01',
        include_macro=True,
        include_sentiment=False
    )

    print(f"\nâœ… Dataset crÃ©Ã© pour {len(dataset)} tickers")

    # Afficher exemple
    for ticker, df in dataset.items():
        print(f"\nðŸ“Š {ticker}:")
        print(f"   PÃ©riode: {df.index.min()} â†’ {df.index.max()}")
        print(f"   Jours: {len(df)}")
        print(f"   Features: {len(df.columns)}")
        print(f"   Colonnes: {', '.join(df.columns[:10])}...")
        print(f"\n   AperÃ§u (5 derniers jours):")
        print(df[['close', 'volume', 'DFF', 'UNRATE']].tail())
        break  # Afficher seulement le premier

    # Info cache
    print("\nðŸ’¾ Info caches:")
    info = cache.get_cache_info()
    print(f"   Yahoo: {info['yahoo']['num_tickers']} tickers, {info['yahoo']['total_records']} records")
    print(f"   PÃ©riode: {info['yahoo']['period']}")
    print(f"   FRED: {info['fred']['num_series']} sÃ©ries, {info['fred']['total_records']} records")

    print("\nâœ… Test terminÃ©!")
