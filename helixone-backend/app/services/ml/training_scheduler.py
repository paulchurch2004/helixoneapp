"""
Training Scheduler - Scheduler pour re-entraÃ®nements pÃ©riodiques

FonctionnalitÃ©s :
- Re-entraÃ®nement hebdomadaire automatique
- PrÃ©-entraÃ®nement au dÃ©marrage
- Configuration via variables d'environnement
"""

import logging
import asyncio
from typing import Optional, List
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.cron import CronTrigger
import os

from .auto_trainer import AutoTrainer, get_auto_trainer

logger = logging.getLogger(__name__)


class TrainingScheduler:
    """
    Scheduler pour entraÃ®nements pÃ©riodiques des modÃ¨les ML

    Usage:
        scheduler = TrainingScheduler(auto_trainer)
        scheduler.start()
        await scheduler.pretrain_top_stocks()
    """

    def __init__(self, auto_trainer: Optional[AutoTrainer] = None):
        """
        Args:
            auto_trainer: Instance de AutoTrainer (utilise singleton si None)
        """
        self.auto_trainer = auto_trainer or get_auto_trainer()
        self.scheduler = AsyncIOScheduler()
        self._is_running = False

        logger.info("TrainingScheduler initialisÃ©")

    def start(self):
        """DÃ©marre le scheduler"""
        if self._is_running:
            logger.warning("Scheduler dÃ©jÃ  en cours d'exÃ©cution")
            return

        # Configuration depuis .env
        enabled = os.getenv('ML_WEEKLY_RETRAIN_ENABLED', 'true').lower() == 'true'

        if not enabled:
            logger.info("Re-entraÃ®nement hebdomadaire dÃ©sactivÃ© (ML_WEEKLY_RETRAIN_ENABLED=false)")
            return

        # Jour de la semaine (default: dimanche)
        day_of_week = os.getenv('ML_WEEKLY_RETRAIN_DAY', 'sunday').lower()

        # Heure (default: 2h du matin)
        try:
            hour = int(os.getenv('ML_WEEKLY_RETRAIN_HOUR', '2'))
        except ValueError:
            hour = 2
            logger.warning(f"ML_WEEKLY_RETRAIN_HOUR invalide, utilisation de 2h par dÃ©faut")

        # Mapper jour en format cron
        day_mapping = {
            'monday': 'mon',
            'tuesday': 'tue',
            'wednesday': 'wed',
            'thursday': 'thu',
            'friday': 'fri',
            'saturday': 'sat',
            'sunday': 'sun'
        }

        cron_day = day_mapping.get(day_of_week, 'sun')

        # Ajouter le job hebdomadaire
        self.scheduler.add_job(
            self.retrain_all_models,
            CronTrigger(day_of_week=cron_day, hour=hour, minute=0),
            id='weekly_retrain',
            name='Re-entraÃ®nement hebdomadaire des modÃ¨les ML',
            replace_existing=True
        )

        # DÃ©marrer le scheduler
        self.scheduler.start()
        self._is_running = True

        logger.info(f"âœ… Scheduler dÃ©marrÃ© - Re-entraÃ®nement chaque {day_of_week} Ã  {hour}h")

    def stop(self):
        """ArrÃªte le scheduler"""
        if not self._is_running:
            return

        self.scheduler.shutdown()
        self._is_running = False

        logger.info("ðŸ›‘ Scheduler arrÃªtÃ©")

    async def retrain_all_models(self):
        """
        Re-entraÃ®ne tous les modÃ¨les existants

        AppelÃ© automatiquement par le scheduler hebdomadaire
        """
        logger.info("=" * 80)
        logger.info("ðŸ”„ DÃ‰BUT RE-ENTRAÃŽNEMENT HEBDOMADAIRE")
        logger.info("=" * 80)

        try:
            # Lister tous les modÃ¨les existants
            models = self.auto_trainer.list_all_models()

            if not models:
                logger.info("   Aucun modÃ¨le Ã  re-entraÃ®ner")
                return

            tickers = [model['ticker'] for model in models]

            logger.info(f"   ModÃ¨les trouvÃ©s: {len(tickers)}")
            logger.info(f"   Tickers: {', '.join(tickers)}")

            # Mode d'entraÃ®nement depuis .env
            mode = os.getenv('ML_AUTO_TRAIN_MODE', 'xgboost')

            # Nombre max d'entraÃ®nements simultanÃ©s
            try:
                max_concurrent = int(os.getenv('ML_MAX_CONCURRENT_TRAINING', '2'))
            except ValueError:
                max_concurrent = 2

            logger.info(f"   Mode: {mode}")
            logger.info(f"   Concurrent: {max_concurrent}")

            # EntraÃ®ner en batch (force=True pour re-entraÃ®ner mÃªme si rÃ©cents)
            results = {}
            for ticker in tickers:
                success = await self.auto_trainer.force_train(ticker, mode=mode)
                results[ticker] = success

            # RÃ©sumÃ©
            success_count = sum(1 for v in results.values() if v)
            failure_count = len(results) - success_count

            logger.info("=" * 80)
            logger.info(f"âœ… RE-ENTRAÃŽNEMENT TERMINÃ‰")
            logger.info(f"   SuccÃ¨s: {success_count}/{len(results)}")
            logger.info(f"   Ã‰checs: {failure_count}/{len(results)}")

            if failure_count > 0:
                failed = [t for t, s in results.items() if not s]
                logger.warning(f"   Ã‰checs: {', '.join(failed)}")

            logger.info("=" * 80)

        except Exception as e:
            logger.error(f"âŒ Erreur lors du re-entraÃ®nement hebdomadaire: {e}", exc_info=True)

    async def pretrain_top_stocks(self):
        """
        PrÃ©-entraÃ®ne les top stocks au dÃ©marrage

        AppelÃ© manuellement au dÃ©marrage de l'application
        """
        # VÃ©rifier si activÃ©
        enabled = os.getenv('ML_PRETRAIN_ON_STARTUP', 'true').lower() == 'true'

        if not enabled:
            logger.info("PrÃ©-entraÃ®nement dÃ©sactivÃ© (ML_PRETRAIN_ON_STARTUP=false)")
            return

        # Liste des tickers Ã  prÃ©-entraÃ®ner
        tickers_env = os.getenv('ML_PRETRAIN_TICKERS', 'AAPL,MSFT,GOOGL,TSLA,AMZN,NVDA,META,NFLX')
        tickers = [t.strip() for t in tickers_env.split(',') if t.strip()]

        if not tickers:
            logger.info("Aucun ticker Ã  prÃ©-entraÃ®ner")
            return

        logger.info("=" * 80)
        logger.info("ðŸš€ PRÃ‰-ENTRAÃŽNEMENT DES TOP STOCKS")
        logger.info("=" * 80)
        logger.info(f"   Tickers: {', '.join(tickers)}")

        # Configuration
        mode = os.getenv('ML_AUTO_TRAIN_MODE', 'xgboost')
        max_age_days = int(os.getenv('ML_MODEL_MAX_AGE_DAYS', '7'))
        max_concurrent = int(os.getenv('ML_MAX_CONCURRENT_TRAINING', '2'))

        logger.info(f"   Mode: {mode}")
        logger.info(f"   Max age: {max_age_days} jours")
        logger.info(f"   Concurrent: {max_concurrent}")

        try:
            # EntraÃ®ner en batch
            results = await self.auto_trainer.batch_train(
                tickers=tickers,
                max_age_days=max_age_days,
                mode=mode,
                max_concurrent=max_concurrent
            )

            # RÃ©sumÃ©
            success_count = sum(1 for v in results.values() if v)
            skipped_count = 0  # ModÃ¨les dÃ©jÃ  Ã  jour
            failure_count = len(results) - success_count

            logger.info("=" * 80)
            logger.info(f"âœ… PRÃ‰-ENTRAÃŽNEMENT TERMINÃ‰")
            logger.info(f"   SuccÃ¨s: {success_count}/{len(results)}")
            logger.info(f"   Ã‰checs: {failure_count}/{len(results)}")

            if failure_count > 0:
                failed = [t for t, s in results.items() if not s]
                logger.warning(f"   Ã‰checs: {', '.join(failed)}")

            logger.info("=" * 80)

        except Exception as e:
            logger.error(f"âŒ Erreur lors du prÃ©-entraÃ®nement: {e}", exc_info=True)

    def get_next_run_time(self):
        """Retourne la date/heure du prochain re-entraÃ®nement"""
        if not self._is_running:
            return None

        job = self.scheduler.get_job('weekly_retrain')
        if job:
            return job.next_run_time

        return None


# ============================================================================
# SINGLETON
# ============================================================================

_training_scheduler_instance: Optional[TrainingScheduler] = None

def get_training_scheduler() -> TrainingScheduler:
    """Retourne l'instance singleton de TrainingScheduler"""
    global _training_scheduler_instance
    if _training_scheduler_instance is None:
        _training_scheduler_instance = TrainingScheduler()
    return _training_scheduler_instance
