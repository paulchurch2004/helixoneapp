# ü§ñ Machine Learning for Algorithmic Trading
## Guide Complet / Complete Guide

**Auteur Original / Original Author**: Stefan Jansen  
**Documentation**: HelixOne Complete Reference  
**Version**: 2.0 - Second Edition

---

# üìë TABLE DES MATI√àRES / TABLE OF CONTENTS

1. [Introduction](#1-introduction)
2. [Donn√©es de March√© et Fondamentales](#2-donn√©es-de-march√©-et-fondamentales)
3. [Donn√©es Alternatives](#3-donn√©es-alternatives)
4. [Recherche de Facteurs Alpha](#4-recherche-de-facteurs-alpha)
5. [√âvaluation de Strat√©gie](#5-√©valuation-de-strat√©gie)
6. [Processus Machine Learning](#6-processus-machine-learning)
7. [Mod√®les Lin√©aires](#7-mod√®les-lin√©aires)
8. [Workflow ML4T Complet](#8-workflow-ml4t-complet)
9. [Mod√®les de S√©ries Temporelles](#9-mod√®les-de-s√©ries-temporelles)
10. [Machine Learning Bay√©sien](#10-machine-learning-bay√©sien)
11. [Arbres de D√©cision et For√™ts Al√©atoires](#11-arbres-de-d√©cision-et-for√™ts-al√©atoires)
12. [Gradient Boosting Machines](#12-gradient-boosting-machines)
13. [Apprentissage Non Supervis√©](#13-apprentissage-non-supervis√©)
14. [Traitement du Langage Naturel (NLP)](#14-traitement-du-langage-naturel-nlp)
15. [Mod√©lisation de Sujets (Topic Modeling)](#15-mod√©lisation-de-sujets-topic-modeling)
16. [Embeddings de Mots](#16-embeddings-de-mots)
17. [Deep Learning - R√©seaux Feedforward](#17-deep-learning---r√©seaux-feedforward)
18. [R√©seaux de Neurones Convolutionnels (CNN)](#18-r√©seaux-de-neurones-convolutionnels-cnn)
19. [R√©seaux de Neurones R√©currents (RNN)](#19-r√©seaux-de-neurones-r√©currents-rnn)
20. [Autoencodeurs](#20-autoencodeurs)
21. [R√©seaux Adverses G√©n√©ratifs (GAN)](#21-r√©seaux-adverses-g√©n√©ratifs-gan)
22. [Apprentissage par Renforcement](#22-apprentissage-par-renforcement)
23. [Prochaines √âtapes](#23-prochaines-√©tapes)
24. [Biblioth√®que de Facteurs Alpha](#24-biblioth√®que-de-facteurs-alpha)

---

# 1. INTRODUCTION

## 1.1 Qu'est-ce que le ML4T (Machine Learning for Trading)?

Le **ML4T** (Machine Learning for Trading - Apprentissage Automatique pour le Trading) est l'application des techniques de Machine Learning (ML) et Deep Learning (DL) pour:

1. **G√©n√©rer des signaux de trading** (alpha factors - facteurs alpha)
2. **Optimiser les portefeuilles** (portfolio optimization - optimisation de portefeuille)
3. **Ex√©cuter des ordres** (order execution - ex√©cution d'ordres)
4. **G√©rer les risques** (risk management - gestion des risques)

## 1.2 Architecture du Workflow ML4T

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        WORKFLOW ML4T                                 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ
‚îÇ  ‚îÇ  DATA    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ FEATURES ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  MODEL   ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ BACKTEST ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ SOURCES  ‚îÇ    ‚îÇENGINEERING‚îÇ   ‚îÇ TRAINING ‚îÇ    ‚îÇ          ‚îÇ      ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ
‚îÇ       ‚îÇ              ‚îÇ               ‚îÇ               ‚îÇ              ‚îÇ
‚îÇ       ‚ñº              ‚ñº               ‚ñº               ‚ñº              ‚îÇ
‚îÇ  - Market Data   - Technical    - Linear Models  - Zipline         ‚îÇ
‚îÇ  - Fundamental     Indicators   - Tree-based     - Backtrader      ‚îÇ
‚îÇ  - Alternative   - Alpha        - Deep Learning  - PyFolio         ‚îÇ
‚îÇ  - SEC Filings     Factors      - Ensemble       - Alphalens       ‚îÇ
‚îÇ                                                                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## 1.3 D√©pendances Python Principales

```python
# === CORE DATA SCIENCE ===
import numpy as np                    # Calcul num√©rique (numerical computing)
import pandas as pd                   # Manipulation de donn√©es (data manipulation)
from scipy import stats               # Statistiques (statistics)

# === MACHINE LEARNING ===
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, Lasso
from sklearn.model_selection import cross_val_score, TimeSeriesSplit
from sklearn.preprocessing import StandardScaler
import lightgbm as lgb               # LightGBM (Light Gradient Boosting Machine)
import catboost as cb                # CatBoost (Categorical Boosting)
import xgboost as xgb                # XGBoost (eXtreme Gradient Boosting)

# === DEEP LEARNING ===
import tensorflow as tf              # TensorFlow (DL framework by Google)
from tensorflow import keras         # Keras (High-level DL API)
import torch                         # PyTorch (DL framework by Meta)
import torch.nn as nn                # Neural Network modules

# === FINANCE & TRADING ===
import yfinance as yf                # Yahoo Finance API
import pandas_datareader as web      # Financial data reader
from zipline.api import order_target_percent, record, symbol
from alphalens import utils, performance, plotting
import pyfolio as pf                 # Portfolio analysis
import talib                         # TA-Lib (Technical Analysis Library)

# === NLP (Natural Language Processing) ===
import spacy                         # spaCy NLP library
from gensim.models import Word2Vec, Doc2Vec
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer

# === VISUALIZATION ===
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
```

## 1.4 Configuration de l'Environnement

```bash
# Cr√©er l'environnement conda
conda create -n ml4t python=3.8

# Activer l'environnement
conda activate ml4t

# Installer les d√©pendances principales
conda install -c conda-forge \
    numpy pandas scipy scikit-learn \
    matplotlib seaborn plotly \
    jupyter jupyterlab \
    statsmodels arch \
    lightgbm catboost xgboost

# Installer les packages financiers
pip install yfinance alphalens-reloaded pyfolio-reloaded zipline-reloaded

# Installer TA-Lib (n√©cessite compilation)
conda install -c conda-forge ta-lib

# Installer les packages NLP
pip install spacy gensim textblob
python -m spacy download en_core_web_sm

# Installer TensorFlow et PyTorch
pip install tensorflow torch torchvision
```

---

# 2. DONN√âES DE MARCH√â ET FONDAMENTALES
## Market and Fundamental Data

## 2.1 Sources de Donn√©es

| Source | Type | Fr√©quence | Acc√®s |
|--------|------|-----------|-------|
| **Yahoo Finance** | Prix OHLCV (Open-High-Low-Close-Volume) | Journalier | Gratuit |
| **Quandl** | Multi-sources | Variable | Freemium |
| **NASDAQ ITCH** | Order book (carnet d'ordres) | Tick | Payant |
| **SEC EDGAR** | Filings (d√©clarations) | √âv√©nementiel | Gratuit |
| **AlgoSeek** | Intraday | Minute | Payant |

## 2.2 T√©l√©chargement avec yfinance

```python
"""
yfinance - T√©l√©chargement de donn√©es Yahoo Finance
==================================================
yfinance permet de t√©l√©charger gratuitement les donn√©es de prix historiques
depuis Yahoo Finance.

Exemple: T√©l√©charger les donn√©es AAPL (Apple Inc.)
"""
import yfinance as yf
import pandas as pd

# === M√©thode 1: Ticker unique ===
ticker = yf.Ticker("AAPL")

# Obtenir les donn√©es historiques
# period: 1d, 5d, 1mo, 3mo, 6mo, 1y, 2y, 5y, 10y, ytd, max
# interval: 1m, 2m, 5m, 15m, 30m, 60m, 90m, 1h, 1d, 5d, 1wk, 1mo, 3mo
hist = ticker.history(period="1y", interval="1d")
print(hist.head())
"""
                  Open        High         Low       Close    Volume  Dividends  Stock Splits
Date                                                                                          
2023-01-03  130.279999  130.899994  124.169998  125.070000  112117500        0.0           0.0
2023-01-04  126.889999  128.660004  125.080002  126.360001   89113600        0.0           0.0
"""

# Informations sur l'entreprise
info = ticker.info
print(f"Entreprise: {info['longName']}")
print(f"Secteur: {info['sector']}")
print(f"Market Cap: ${info['marketCap']:,.0f}")

# === M√©thode 2: T√©l√©chargement multiple ===
tickers = ["AAPL", "GOOGL", "MSFT", "AMZN", "META"]

# T√©l√©charger toutes les donn√©es en une fois
data = yf.download(
    tickers=tickers,
    start="2020-01-01",
    end="2024-01-01",
    interval="1d",
    group_by="ticker",    # Grouper par ticker
    auto_adjust=True,     # Ajuster pour dividendes/splits
    threads=True          # T√©l√©chargement parall√®le
)

# Acc√©der aux prix de cl√¥ture ajust√©s
close_prices = data.xs('Close', axis=1, level=1)
print(close_prices.head())

# === M√©thode 3: Donn√©es intraday ===
intraday = yf.download(
    tickers="SPY",
    period="5d",
    interval="5m"    # Barres de 5 minutes
)
print(f"Nombre de barres: {len(intraday)}")
```

## 2.3 T√©l√©chargement avec pandas-datareader

```python
"""
pandas-datareader - Acc√®s √† multiples sources de donn√©es
=========================================================
Permet d'acc√©der √†: FRED, Fama-French, World Bank, OECD, etc.
"""
import pandas_datareader as web
from datetime import datetime

# === Donn√©es Fama-French (Facteurs de risque) ===
# Les facteurs Fama-French sont utilis√©s pour expliquer les rendements
# Mkt-RF: Rendement du march√© moins le taux sans risque
# SMB: Small Minus Big (petites caps vs grandes caps)
# HML: High Minus Low (value vs growth)
# RMW: Robust Minus Weak (profitabilit√©)
# CMA: Conservative Minus Aggressive (investissement)

ff_factors = web.DataReader(
    'F-F_Research_Data_5_Factors_2x3',
    'famafrench',
    start='2010-01-01'
)[0]

# Convertir en pourcentages d√©cimaux
ff_factors = ff_factors / 100
print(ff_factors.head())
"""
              Mkt-RF     SMB     HML     RMW     CMA      RF
Date                                                        
2010-01   -0.0327 -0.0081  0.0058  0.0040 -0.0065  0.0000
2010-02    0.0309  0.0089 -0.0057  0.0126  0.0085  0.0000
"""

# === Donn√©es FRED (Federal Reserve Economic Data) ===
# Taux d'int√©r√™t, inflation, PIB, etc.
fred_data = web.DataReader(
    ['GS10', 'TB3MS', 'CPIAUCSL'],  # 10Y Treasury, 3M T-Bill, CPI
    'fred',
    start='2010-01-01'
)
print(fred_data.head())

# === Donn√©es de la Banque Mondiale ===
from pandas_datareader import wb

gdp_data = wb.download(
    indicator='NY.GDP.MKTP.CD',  # GDP (current US$)
    country=['US', 'CN', 'JP', 'DE', 'FR'],
    start=2010,
    end=2023
)
print(gdp_data.head())
```

## 2.4 Parsing NASDAQ ITCH Order Flow

```python
"""
NASDAQ ITCH Protocol Parser
===========================
Le protocole ITCH (Integrated Trading and Clearing) est le format de donn√©es
brutes du NASDAQ. Il contient TOUS les messages du march√©:
- Add Order: Nouvel ordre ajout√© au carnet
- Order Executed: Ordre ex√©cut√©
- Order Cancel: Ordre annul√©
- Trade: Transaction effectu√©e

Ce parsing est essentiel pour:
1. Reconstruire le carnet d'ordres (order book)
2. Analyser le flux d'ordres (order flow)
3. D√©tecter les patterns de trading haute fr√©quence (HFT)
"""
from pathlib import Path
from collections import namedtuple, Counter
from datetime import datetime, timedelta
import pandas as pd
import numpy as np
from struct import unpack

# === D√©finition des types de messages ITCH ===
# Chaque message a un format binaire sp√©cifique

# Format des messages (nom: (format_struct, champs))
message_formats = {
    'S': ('4sHHI', ['event_code']),                    # System Event
    'R': ('4sHHI', ['stock', 'market_category']),     # Stock Directory
    'H': ('4sHHI', ['stock', 'trading_state']),       # Trading Action
    'Y': ('4sHHI', ['stock', 'reg_sho']),             # Reg SHO
    'L': ('4sHHI', ['mpid', 'stock']),                # Market Participant
    'A': ('4sHH6sIQI', ['ref', 'side', 'shares', 'stock', 'price']),  # Add Order
    'F': ('4sHH6sIQI4s', ['ref', 'side', 'shares', 'stock', 'price', 'mpid']),  # Add Order MPID
    'E': ('4sHHQI', ['ref', 'shares', 'match']),      # Order Executed
    'C': ('4sHHQIQ', ['ref', 'shares', 'match', 'printable', 'price']),  # Order Executed Price
    'X': ('4sHHQI', ['ref', 'shares']),               # Order Cancel
    'D': ('4sHHQ', ['ref']),                          # Order Delete
    'U': ('4sHHQQI', ['ref', 'new_ref', 'shares', 'price']),  # Order Replace
    'P': ('4sHHQI6sQQ', ['ref', 'side', 'shares', 'stock', 'price', 'match']),  # Trade
    'Q': ('4sHHQ6sQQ', ['shares', 'stock', 'price', 'match', 'cross']),  # Cross Trade
    'B': ('4sHHQ', ['match']),                        # Broken Trade
    'I': ('4sHH6sIIIQ', ['paired', 'imbalance', 'direction', 'stock', 'far', 'near', 'current']),  # NOII
}

def parse_itch_message(message_type, data):
    """
    Parse un message ITCH binaire.
    
    Args:
        message_type: Type de message (char)
        data: Donn√©es binaires
    
    Returns:
        dict: Message pars√© avec les champs
    """
    if message_type not in message_formats:
        return None
    
    fmt, fields = message_formats[message_type]
    
    try:
        # Unpack les donn√©es binaires selon le format
        values = unpack('>' + fmt, data[:len(fmt)])
        
        # Cr√©er le dictionnaire de r√©sultat
        result = {'message_type': message_type}
        for field, value in zip(fields, values[1:]):  # Skip timestamp
            if isinstance(value, bytes):
                value = value.decode('ascii').strip()
            result[field] = value
        
        return result
    except Exception as e:
        return None


def read_itch_file(filepath, max_messages=None):
    """
    Lit un fichier ITCH et extrait les messages.
    
    Args:
        filepath: Chemin vers le fichier ITCH
        max_messages: Nombre maximum de messages √† lire (None = tous)
    
    Returns:
        list: Liste des messages pars√©s
    """
    messages = []
    
    with open(filepath, 'rb') as f:
        while True:
            # Lire la taille du message (2 bytes, big-endian)
            size_data = f.read(2)
            if len(size_data) < 2:
                break
            
            message_size = unpack('>H', size_data)[0]
            
            # Lire le message
            message_data = f.read(message_size)
            if len(message_data) < message_size:
                break
            
            # Parser le message
            message_type = chr(message_data[0])
            parsed = parse_itch_message(message_type, message_data)
            
            if parsed:
                messages.append(parsed)
            
            if max_messages and len(messages) >= max_messages:
                break
    
    return messages


# === Exemple d'utilisation ===
# messages = read_itch_file('data/01302019.NASDAQ_ITCH50', max_messages=100000)
# df = pd.DataFrame(messages)
# print(df['message_type'].value_counts())
```

## 2.5 Reconstruction du Carnet d'Ordres (Order Book)

```python
"""
Order Book Reconstruction
=========================
Le carnet d'ordres (order book ou LOB - Limit Order Book) repr√©sente
l'√©tat du march√© √† tout instant:
- BID (achat): Ordres d'achat en attente
- ASK (vente): Ordres de vente en attente
- Spread: Diff√©rence entre meilleur ask et meilleur bid

Structure:
    ASK (sell orders)      Prix
    ----------------       -----
    100 @ $150.05         150.05  <- Best Ask
    200 @ $150.06         150.06
    150 @ $150.07         150.07
    
    --- SPREAD: $0.03 ---
    
    BID (buy orders)       Prix
    ----------------       -----
    180 @ $150.02         150.02  <- Best Bid
    250 @ $150.01         150.01
    300 @ $150.00         150.00
"""
import pandas as pd
import numpy as np
from collections import defaultdict

class OrderBook:
    """
    Impl√©mentation d'un carnet d'ordres.
    
    Maintient l'√©tat du march√© et permet:
    - Ajouter/supprimer des ordres
    - Ex√©cuter des ordres
    - Calculer les m√©triques de microstructure
    """
    
    def __init__(self, ticker):
        self.ticker = ticker
        self.bids = {}  # {order_ref: {'price': p, 'shares': s, 'timestamp': t}}
        self.asks = {}
        self.trades = []
        
    def add_order(self, order_ref, side, price, shares, timestamp):
        """
        Ajoute un ordre au carnet.
        
        Args:
            order_ref: R√©f√©rence unique de l'ordre
            side: 'B' (buy/bid) ou 'S' (sell/ask)
            price: Prix en cents (ex: 15002 = $150.02)
            shares: Nombre d'actions
            timestamp: Horodatage
        """
        order = {
            'price': price,
            'shares': shares,
            'timestamp': timestamp
        }
        
        if side == 'B':
            self.bids[order_ref] = order
        else:
            self.asks[order_ref] = order
    
    def cancel_order(self, order_ref, shares_to_cancel):
        """
        Annule partiellement ou totalement un ordre.
        
        Args:
            order_ref: R√©f√©rence de l'ordre
            shares_to_cancel: Nombre d'actions √† annuler
        """
        for book in [self.bids, self.asks]:
            if order_ref in book:
                book[order_ref]['shares'] -= shares_to_cancel
                if book[order_ref]['shares'] <= 0:
                    del book[order_ref]
                return
    
    def delete_order(self, order_ref):
        """Supprime compl√®tement un ordre."""
        for book in [self.bids, self.asks]:
            if order_ref in book:
                del book[order_ref]
                return
    
    def execute_order(self, order_ref, shares_executed, price, timestamp):
        """
        Ex√©cute un ordre (trade).
        
        Args:
            order_ref: R√©f√©rence de l'ordre ex√©cut√©
            shares_executed: Nombre d'actions ex√©cut√©es
            price: Prix d'ex√©cution
            timestamp: Horodatage
        """
        # Enregistrer le trade
        self.trades.append({
            'order_ref': order_ref,
            'shares': shares_executed,
            'price': price,
            'timestamp': timestamp
        })
        
        # Mettre √† jour l'ordre
        for book in [self.bids, self.asks]:
            if order_ref in book:
                book[order_ref]['shares'] -= shares_executed
                if book[order_ref]['shares'] <= 0:
                    del book[order_ref]
                return
    
    def get_best_bid(self):
        """Retourne le meilleur bid (plus haut prix d'achat)."""
        if not self.bids:
            return None, 0
        best = max(self.bids.values(), key=lambda x: x['price'])
        total_shares = sum(o['shares'] for o in self.bids.values() 
                         if o['price'] == best['price'])
        return best['price'], total_shares
    
    def get_best_ask(self):
        """Retourne le meilleur ask (plus bas prix de vente)."""
        if not self.asks:
            return None, 0
        best = min(self.asks.values(), key=lambda x: x['price'])
        total_shares = sum(o['shares'] for o in self.asks.values() 
                         if o['price'] == best['price'])
        return best['price'], total_shares
    
    def get_spread(self):
        """
        Calcule le spread (√©cart bid-ask).
        
        Returns:
            float: Spread en cents, ou None si non disponible
        """
        bid_price, _ = self.get_best_bid()
        ask_price, _ = self.get_best_ask()
        
        if bid_price is None or ask_price is None:
            return None
        
        return ask_price - bid_price
    
    def get_midprice(self):
        """
        Calcule le prix m√©dian (midprice).
        
        Le midprice est souvent utilis√© comme estimation du "vrai" prix.
        
        Returns:
            float: (best_bid + best_ask) / 2
        """
        bid_price, _ = self.get_best_bid()
        ask_price, _ = self.get_best_ask()
        
        if bid_price is None or ask_price is None:
            return None
        
        return (bid_price + ask_price) / 2
    
    def get_depth(self, levels=5):
        """
        Retourne la profondeur du carnet sur N niveaux.
        
        Args:
            levels: Nombre de niveaux de prix √† retourner
        
        Returns:
            dict: {'bids': [...], 'asks': [...]}
        """
        # Agr√©ger par niveau de prix
        bid_levels = defaultdict(int)
        ask_levels = defaultdict(int)
        
        for order in self.bids.values():
            bid_levels[order['price']] += order['shares']
        
        for order in self.asks.values():
            ask_levels[order['price']] += order['shares']
        
        # Trier et prendre les N meilleurs niveaux
        sorted_bids = sorted(bid_levels.items(), key=lambda x: -x[0])[:levels]
        sorted_asks = sorted(ask_levels.items(), key=lambda x: x[0])[:levels]
        
        return {
            'bids': [{'price': p, 'shares': s} for p, s in sorted_bids],
            'asks': [{'price': p, 'shares': s} for p, s in sorted_asks]
        }
    
    def get_order_imbalance(self):
        """
        Calcule le d√©s√©quilibre d'ordres (order imbalance).
        
        L'imbalance est un indicateur de pression acheteuse/vendeuse:
        - Positif: Plus d'ordres d'achat (bullish)
        - N√©gatif: Plus d'ordres de vente (bearish)
        
        Returns:
            float: (bid_volume - ask_volume) / (bid_volume + ask_volume)
        """
        bid_volume = sum(o['shares'] for o in self.bids.values())
        ask_volume = sum(o['shares'] for o in self.asks.values())
        
        total = bid_volume + ask_volume
        if total == 0:
            return 0
        
        return (bid_volume - ask_volume) / total


# === Exemple d'utilisation ===
book = OrderBook('AAPL')

# Simuler quelques ordres
book.add_order('O001', 'B', 15002, 100, '09:30:00')  # Buy 100 @ $150.02
book.add_order('O002', 'B', 15001, 200, '09:30:01')  # Buy 200 @ $150.01
book.add_order('O003', 'S', 15005, 150, '09:30:02')  # Sell 150 @ $150.05
book.add_order('O004', 'S', 15006, 100, '09:30:03')  # Sell 100 @ $150.06

print(f"Best Bid: ${book.get_best_bid()[0]/100:.2f}")
print(f"Best Ask: ${book.get_best_ask()[0]/100:.2f}")
print(f"Spread: ${book.get_spread()/100:.4f}")
print(f"Midprice: ${book.get_midprice()/100:.2f}")
print(f"Order Imbalance: {book.get_order_imbalance():.2%}")
print(f"\nDepth:\n{book.get_depth(3)}")
```

## 2.6 SEC EDGAR - Parsing XBRL

```python
"""
SEC EDGAR XBRL Parser
=====================
EDGAR (Electronic Data Gathering, Analysis, and Retrieval) est le syst√®me
de la SEC pour collecter les d√©clarations des entreprises cot√©es.

XBRL (eXtensible Business Reporting Language) est le format standard
pour les donn√©es financi√®res structur√©es.

Types de filings courants:
- 10-K: Rapport annuel
- 10-Q: Rapport trimestriel  
- 8-K: √âv√©nements importants
- 13-F: Holdings des gestionnaires de fonds
"""
import requests
import pandas as pd
from bs4 import BeautifulSoup
import re
from datetime import datetime

class SECEdgarClient:
    """
    Client pour acc√©der aux donn√©es SEC EDGAR.
    """
    
    BASE_URL = "https://www.sec.gov"
    SEARCH_URL = "https://efts.sec.gov/LATEST/search-index"
    
    def __init__(self, user_agent):
        """
        Initialise le client.
        
        Args:
            user_agent: Votre email (requis par la SEC)
        """
        self.headers = {
            'User-Agent': user_agent,
            'Accept-Encoding': 'gzip, deflate'
        }
    
    def get_company_filings(self, cik, filing_type='10-K', count=10):
        """
        R√©cup√®re les filings d'une entreprise.
        
        Args:
            cik: Central Index Key (identifiant SEC)
            filing_type: Type de filing (10-K, 10-Q, 8-K, etc.)
            count: Nombre de filings √† r√©cup√©rer
        
        Returns:
            list: Liste des filings avec m√©tadonn√©es
        """
        # Formater le CIK (10 digits avec leading zeros)
        cik = str(cik).zfill(10)
        
        # URL de l'API EDGAR
        url = f"{self.BASE_URL}/cgi-bin/browse-edgar"
        params = {
            'action': 'getcompany',
            'CIK': cik,
            'type': filing_type,
            'dateb': '',
            'owner': 'include',
            'count': count,
            'output': 'atom'
        }
        
        response = requests.get(url, params=params, headers=self.headers)
        
        if response.status_code != 200:
            raise Exception(f"Error fetching filings: {response.status_code}")
        
        # Parser le XML Atom
        soup = BeautifulSoup(response.content, 'xml')
        entries = soup.find_all('entry')
        
        filings = []
        for entry in entries:
            filing = {
                'title': entry.find('title').text if entry.find('title') else None,
                'link': entry.find('link')['href'] if entry.find('link') else None,
                'filing_date': entry.find('filing-date').text if entry.find('filing-date') else None,
                'accession_number': entry.find('accession-number').text if entry.find('accession-number') else None,
            }
            filings.append(filing)
        
        return filings
    
    def get_filing_documents(self, accession_number, cik):
        """
        R√©cup√®re la liste des documents d'un filing.
        
        Args:
            accession_number: Num√©ro d'accession du filing
            cik: CIK de l'entreprise
        
        Returns:
            list: Documents du filing
        """
        cik = str(cik).zfill(10)
        accession_formatted = accession_number.replace('-', '')
        
        url = f"{self.BASE_URL}/Archives/edgar/data/{cik}/{accession_formatted}/index.json"
        
        response = requests.get(url, headers=self.headers)
        
        if response.status_code != 200:
            return []
        
        data = response.json()
        return data.get('directory', {}).get('item', [])
    
    def parse_xbrl_facts(self, url):
        """
        Parse les faits XBRL d'un document.
        
        Args:
            url: URL du document XBRL
        
        Returns:
            dict: Faits financiers extraits
        """
        response = requests.get(url, headers=self.headers)
        soup = BeautifulSoup(response.content, 'lxml')
        
        facts = {}
        
        # √âl√©ments financiers courants
        xbrl_elements = [
            'us-gaap:Assets',
            'us-gaap:Liabilities',
            'us-gaap:StockholdersEquity',
            'us-gaap:Revenues',
            'us-gaap:NetIncomeLoss',
            'us-gaap:EarningsPerShareBasic',
            'us-gaap:EarningsPerShareDiluted',
            'us-gaap:CashAndCashEquivalentsAtCarryingValue',
            'us-gaap:LongTermDebt',
            'us-gaap:CommonStockSharesOutstanding',
        ]
        
        for element_name in xbrl_elements:
            # Chercher l'√©l√©ment (avec ou sans namespace)
            element = soup.find(element_name.lower().replace(':', '_'))
            if element is None:
                element = soup.find(element_name.split(':')[1].lower())
            
            if element:
                try:
                    value = float(element.text.replace(',', ''))
                    facts[element_name] = value
                except ValueError:
                    facts[element_name] = element.text
        
        return facts


def get_financial_ratios(facts):
    """
    Calcule les ratios financiers √† partir des faits XBRL.
    
    Args:
        facts: Dictionnaire des faits financiers
    
    Returns:
        dict: Ratios calcul√©s
    """
    ratios = {}
    
    # Current Ratio (liquidit√©)
    assets = facts.get('us-gaap:Assets', 0)
    liabilities = facts.get('us-gaap:Liabilities', 0)
    
    if liabilities > 0:
        ratios['debt_to_assets'] = liabilities / assets
    
    # Return on Equity (ROE)
    net_income = facts.get('us-gaap:NetIncomeLoss', 0)
    equity = facts.get('us-gaap:StockholdersEquity', 0)
    
    if equity > 0:
        ratios['roe'] = net_income / equity
    
    # Profit Margin
    revenue = facts.get('us-gaap:Revenues', 0)
    if revenue > 0:
        ratios['profit_margin'] = net_income / revenue
    
    return ratios


# === Exemple d'utilisation ===
# client = SECEdgarClient("votre.email@exemple.com")
# 
# # Apple Inc. CIK: 320193
# filings = client.get_company_filings('320193', '10-K', count=5)
# print(f"Derniers 10-K d'Apple: {len(filings)}")
# 
# for f in filings:
#     print(f"  {f['filing_date']}: {f['title']}")
```

## 2.7 Stockage des Donn√©es avec HDF5

```python
"""
HDF5 Storage for Financial Data
================================
HDF5 (Hierarchical Data Format) est id√©al pour stocker de grandes
quantit√©s de donn√©es financi√®res car il offre:
- Compression efficace
- Acc√®s rapide par chunks
- Structure hi√©rarchique (comme un syst√®me de fichiers)
- Support natif par pandas

Structure recommand√©e:
/prices
    /daily          - Prix OHLCV journaliers
    /minute         - Donn√©es minute
/fundamentals
    /quarterly      - Donn√©es trimestrielles
    /annual         - Donn√©es annuelles
/factors
    /fama_french    - Facteurs FF
    /custom         - Vos propres facteurs
"""
import pandas as pd
import numpy as np
from pathlib import Path

# === Cr√©ation et √©criture ===
DATA_STORE = 'data/assets.h5'

# Cr√©er le fichier HDF5 et stocker des donn√©es
with pd.HDFStore(DATA_STORE, mode='w') as store:
    
    # Exemple: stocker des prix
    prices = pd.DataFrame({
        'AAPL': np.random.randn(1000).cumsum() + 150,
        'GOOGL': np.random.randn(1000).cumsum() + 2800,
        'MSFT': np.random.randn(1000).cumsum() + 330,
    }, index=pd.date_range('2020-01-01', periods=1000, freq='D'))
    
    store.put('prices/daily', prices)
    
    # Stocker avec compression
    store.put('prices/compressed', prices, 
              complevel=9,           # Niveau de compression (0-9)
              complib='blosc')       # Algorithme de compression
    
    # Stocker avec format 'table' (permet les requ√™tes)
    store.put('prices/queryable', prices, format='table')

# === Lecture ===
with pd.HDFStore(DATA_STORE, mode='r') as store:
    # Lire toutes les donn√©es
    prices = store['prices/daily']
    print(f"Shape: {prices.shape}")
    
    # Lister les cl√©s
    print(f"Keys: {store.keys()}")
    
    # Requ√™te sur donn√©es 'table'
    subset = store.select('prices/queryable', 
                         where='index >= "2020-06-01" and index < "2020-07-01"')
    print(f"Juin 2020: {len(subset)} jours")

# === Multi-Index Storage ===
# Pour stocker des donn√©es avec MultiIndex (ticker, date)
def create_multiindex_data():
    """Cr√©e des donn√©es avec MultiIndex pour stockage efficace."""
    tickers = ['AAPL', 'GOOGL', 'MSFT', 'AMZN']
    dates = pd.date_range('2020-01-01', '2023-12-31', freq='D')
    
    # Cr√©er MultiIndex
    idx = pd.MultiIndex.from_product([dates, tickers], names=['date', 'ticker'])
    
    # Cr√©er DataFrame
    n = len(idx)
    data = pd.DataFrame({
        'open': np.random.randn(n) * 10 + 100,
        'high': np.random.randn(n) * 10 + 105,
        'low': np.random.randn(n) * 10 + 95,
        'close': np.random.randn(n) * 10 + 100,
        'volume': np.random.randint(1000000, 10000000, n)
    }, index=idx)
    
    return data

# Stocker et charger des donn√©es MultiIndex
# data = create_multiindex_data()
# data.to_hdf(DATA_STORE, 'prices/multiindex', format='table')

# Charger avec s√©lection
# with pd.HDFStore(DATA_STORE) as store:
#     aapl = store.select('prices/multiindex', 
#                         where="ticker == 'AAPL'")

# === Benchmark des formats de stockage ===
def benchmark_storage_formats(df, filename_base='benchmark'):
    """
    Compare les performances de diff√©rents formats de stockage.
    
    Args:
        df: DataFrame √† stocker
        filename_base: Pr√©fixe des fichiers
    
    Returns:
        dict: R√©sultats du benchmark
    """
    import time
    import os
    
    results = {}
    
    # CSV
    start = time.time()
    df.to_csv(f'{filename_base}.csv')
    csv_write = time.time() - start
    
    start = time.time()
    _ = pd.read_csv(f'{filename_base}.csv', index_col=0, parse_dates=True)
    csv_read = time.time() - start
    csv_size = os.path.getsize(f'{filename_base}.csv')
    
    results['csv'] = {
        'write_time': csv_write,
        'read_time': csv_read,
        'file_size': csv_size
    }
    
    # Parquet
    start = time.time()
    df.to_parquet(f'{filename_base}.parquet')
    parquet_write = time.time() - start
    
    start = time.time()
    _ = pd.read_parquet(f'{filename_base}.parquet')
    parquet_read = time.time() - start
    parquet_size = os.path.getsize(f'{filename_base}.parquet')
    
    results['parquet'] = {
        'write_time': parquet_write,
        'read_time': parquet_read,
        'file_size': parquet_size
    }
    
    # HDF5
    start = time.time()
    df.to_hdf(f'{filename_base}.h5', 'data', mode='w', complevel=9)
    hdf5_write = time.time() - start
    
    start = time.time()
    _ = pd.read_hdf(f'{filename_base}.h5', 'data')
    hdf5_read = time.time() - start
    hdf5_size = os.path.getsize(f'{filename_base}.h5')
    
    results['hdf5'] = {
        'write_time': hdf5_write,
        'read_time': hdf5_read,
        'file_size': hdf5_size
    }
    
    # Afficher les r√©sultats
    print("\n" + "="*60)
    print("BENCHMARK STORAGE FORMATS")
    print("="*60)
    print(f"{'Format':<10} {'Write (s)':<12} {'Read (s)':<12} {'Size (MB)':<12}")
    print("-"*60)
    for fmt, res in results.items():
        print(f"{fmt:<10} {res['write_time']:<12.3f} {res['read_time']:<12.3f} "
              f"{res['file_size']/1e6:<12.2f}")
    
    return results
```

---

# 3. DONN√âES ALTERNATIVES
## Alternative Data

## 3.1 Web Scraping avec Scrapy

```python
"""
Web Scraping pour Donn√©es Alternatives
======================================
Les donn√©es alternatives (alternative data) incluent:
- Donn√©es de r√©servation (OpenTable, Booking)
- Sentiment social media (Twitter, Reddit)
- Trafic web (SimilarWeb)
- Donn√©es satellites (parkings, agriculture)
- Earnings calls transcripts

IMPORTANT: Toujours respecter:
1. robots.txt du site
2. Conditions d'utilisation
3. Rate limiting (d√©lai entre requ√™tes)
"""

# === Scrapy Spider pour OpenTable ===
# Fichier: opentable/spiders/table_spider.py

"""
Spider Scrapy pour OpenTable
Ce spider collecte les donn√©es de r√©servation des restaurants.
"""
import scrapy
from scrapy.loader import ItemLoader
from ..items import RestaurantItem

class OpenTableSpider(scrapy.Spider):
    """
    Spider pour scraper les donn√©es OpenTable.
    
    Usage:
        scrapy crawl opentable -o restaurants.json
    """
    name = 'opentable'
    allowed_domains = ['opentable.com']
    
    # URL de d√©part (page de r√©sultats)
    start_urls = [
        'https://www.opentable.com/new-york-restaurant-listings'
    ]
    
    # Configuration du spider
    custom_settings = {
        'DOWNLOAD_DELAY': 2,              # 2 secondes entre requ√™tes
        'RANDOMIZE_DOWNLOAD_DELAY': True,  # Randomiser le d√©lai
        'CONCURRENT_REQUESTS': 1,          # Une requ√™te √† la fois
        'ROBOTSTXT_OBEY': True,           # Respecter robots.txt
    }
    
    def parse(self, response):
        """
        Parse la page de liste des restaurants.
        
        Args:
            response: R√©ponse HTTP
        
        Yields:
            dict: Donn√©es du restaurant ou Request pour la page suivante
        """
        # Extraire les cartes de restaurants
        restaurant_cards = response.css('div.restaurant-card')
        
        for card in restaurant_cards:
            # Extraire les informations de base
            yield {
                'name': card.css('h2.restaurant-name::text').get(),
                'cuisine': card.css('span.cuisine-type::text').get(),
                'price_range': card.css('span.price-range::text').get(),
                'rating': card.css('span.rating-score::text').get(),
                'reviews_count': card.css('span.review-count::text').get(),
                'neighborhood': card.css('span.neighborhood::text').get(),
                'available_slots': card.css('span.time-slot::text').getall(),
            }
        
        # Pagination - suivre la page suivante
        next_page = response.css('a.pagination-next::attr(href)').get()
        if next_page:
            yield response.follow(next_page, callback=self.parse)


# === Alternative: Selenium pour sites JavaScript ===
"""
Selenium pour sites dynamiques
Certains sites chargent le contenu via JavaScript,
ce qui n√©cessite un navigateur headless.
"""
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
import time

def scrape_with_selenium(url, wait_element_id):
    """
    Scrape une page avec Selenium.
    
    Args:
        url: URL √† scraper
        wait_element_id: ID de l'√©l√©ment √† attendre avant de scraper
    
    Returns:
        str: HTML de la page
    """
    # Configuration Chrome headless
    options = Options()
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--disable-gpu')
    options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
                        'AppleWebKit/537.36 (KHTML, like Gecko) '
                        'Chrome/91.0.4472.124 Safari/537.36')
    
    driver = webdriver.Chrome(options=options)
    
    try:
        driver.get(url)
        
        # Attendre que l'√©l√©ment soit charg√© (max 10 secondes)
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.ID, wait_element_id))
        )
        
        # Pause suppl√©mentaire pour le JavaScript
        time.sleep(2)
        
        return driver.page_source
    
    finally:
        driver.quit()


# === Scraping des Earnings Calls (SeekingAlpha) ===
"""
ATTENTION: SeekingAlpha bloque maintenant le scraping avec CAPTCHA.
Ce code est fourni √† titre √©ducatif uniquement.
Utilisez des APIs officielles ou des fournisseurs de donn√©es.
"""
import requests
from bs4 import BeautifulSoup

def get_earnings_call_transcript(ticker, api_key=None):
    """
    R√©cup√®re le transcript d'un earnings call.
    
    Pour une utilisation en production, utilisez:
    - API SeekingAlpha (payante)
    - Refinitiv/Reuters
    - Bloomberg
    - Quandl
    
    Args:
        ticker: Symbol du ticker
        api_key: Cl√© API (si disponible)
    
    Returns:
        str: Transcript de l'earnings call
    """
    # Exemple avec API fictive
    if api_key:
        url = f"https://api.provider.com/transcripts/{ticker}"
        headers = {'Authorization': f'Bearer {api_key}'}
        response = requests.get(url, headers=headers)
        return response.json().get('transcript', '')
    
    # Sans API - m√©thode manuelle non recommand√©e
    print("ATTENTION: Le scraping de SeekingAlpha n'est plus possible.")
    print("Utilisez une API officielle ou un fournisseur de donn√©es.")
    return None
```

---

# 4. RECHERCHE DE FACTEURS ALPHA
## Alpha Factor Research

## 4.1 Qu'est-ce qu'un Alpha Factor?

Un **alpha factor** (facteur alpha) est un signal pr√©dictif qui aide √† pr√©voir les rendements futurs d'un actif. L'alpha repr√©sente le rendement exc√©dentaire par rapport √† un benchmark.

**Types de facteurs:**
- **Value (Valeur)**: P/E, P/B, Dividend Yield
- **Momentum**: Rendements pass√©s, RSI (Relative Strength Index)
- **Quality (Qualit√©)**: ROE (Return on Equity), dette/capitaux propres
- **Volatility (Volatilit√©)**: Volatilit√© historique, beta
- **Size (Taille)**: Market cap, volume

## 4.2 Feature Engineering pour le Trading

```python
"""
Feature Engineering pour Alpha Factors
======================================
Le feature engineering est l'√©tape la plus importante du ML4T.
Un bon feature engineering peut transformer une strat√©gie m√©diocre
en une strat√©gie rentable.

Cat√©gories de features:
1. Rendements (Returns) - Diff√©rentes p√©riodes
2. Volatilit√© (Volatility) - Risque
3. Momentum - Force de la tendance
4. Mean Reversion - Retour √† la moyenne
5. Volume - Liquidit√© et activit√©
6. Fondamentaux - Ratios financiers
"""
import pandas as pd
import numpy as np
import talib
from scipy import stats

def compute_returns(prices, periods=[1, 5, 10, 21, 63, 126, 252]):
    """
    Calcule les rendements sur diff√©rentes p√©riodes.
    
    Args:
        prices: Series ou DataFrame de prix
        periods: Liste des p√©riodes en jours
    
    Returns:
        DataFrame: Rendements pour chaque p√©riode
    
    Exemple:
        >>> prices = pd.Series([100, 101, 102, 103, 104])
        >>> returns = compute_returns(prices, [1, 2])
        >>> print(returns)
           return_1d  return_2d
        0        NaN        NaN
        1      0.010        NaN
        2      0.010      0.020
        3      0.010      0.020
        4      0.010      0.020
    """
    returns = pd.DataFrame(index=prices.index)
    
    for period in periods:
        col_name = f'return_{period}d'
        returns[col_name] = prices.pct_change(period)
    
    return returns


def compute_volatility(prices, windows=[5, 10, 21, 63]):
    """
    Calcule la volatilit√© (√©cart-type des rendements) sur diff√©rentes fen√™tres.
    
    Args:
        prices: Series de prix
        windows: Liste des tailles de fen√™tre
    
    Returns:
        DataFrame: Volatilit√© pour chaque fen√™tre
    
    Note:
        La volatilit√© est annualis√©e en multipliant par sqrt(252).
    """
    returns = prices.pct_change()
    vol = pd.DataFrame(index=prices.index)
    
    for window in windows:
        col_name = f'vol_{window}d'
        vol[col_name] = returns.rolling(window).std() * np.sqrt(252)
    
    return vol


def compute_momentum_indicators(prices, high=None, low=None, volume=None):
    """
    Calcule les indicateurs de momentum avec TA-Lib.
    
    Args:
        prices: Series de prix de cl√¥ture
        high: Series de prix hauts (optionnel)
        low: Series de prix bas (optionnel)
        volume: Series de volume (optionnel)
    
    Returns:
        DataFrame: Indicateurs de momentum
    
    Indicateurs inclus:
        - RSI (Relative Strength Index): Mesure la vitesse des changements de prix
          * > 70: Surachet√© (overbought)
          * < 30: Survendu (oversold)
        
        - MACD (Moving Average Convergence Divergence): Diff√©rence entre EMA rapide et lente
          * Signal > 0: Momentum haussier
          * Signal < 0: Momentum baissier
        
        - Stochastic: Position du prix dans la range r√©cente
          * > 80: Surachet√©
          * < 20: Survendu
        
        - ADX (Average Directional Index): Force de la tendance
          * > 25: Tendance forte
          * < 20: Pas de tendance claire
    """
    close = prices.values
    indicators = pd.DataFrame(index=prices.index)
    
    # RSI - Relative Strength Index
    # Mesure la magnitude des gains r√©cents vs pertes
    indicators['rsi_14'] = talib.RSI(close, timeperiod=14)
    indicators['rsi_7'] = talib.RSI(close, timeperiod=7)
    
    # MACD - Moving Average Convergence Divergence
    macd, macd_signal, macd_hist = talib.MACD(close, 
                                               fastperiod=12, 
                                               slowperiod=26, 
                                               signalperiod=9)
    indicators['macd'] = macd
    indicators['macd_signal'] = macd_signal
    indicators['macd_hist'] = macd_hist
    
    # Williams %R - Similaire au stochastique
    if high is not None and low is not None:
        indicators['willr'] = talib.WILLR(high.values, low.values, close, timeperiod=14)
        
        # Stochastic
        slowk, slowd = talib.STOCH(high.values, low.values, close,
                                    fastk_period=14, slowk_period=3, slowd_period=3)
        indicators['stoch_k'] = slowk
        indicators['stoch_d'] = slowd
        
        # ADX - Average Directional Index
        indicators['adx'] = talib.ADX(high.values, low.values, close, timeperiod=14)
        
        # CCI - Commodity Channel Index
        indicators['cci'] = talib.CCI(high.values, low.values, close, timeperiod=14)
        
        # ATR - Average True Range (volatilit√©)
        indicators['atr'] = talib.ATR(high.values, low.values, close, timeperiod=14)
    
    # ROC - Rate of Change
    indicators['roc_10'] = talib.ROC(close, timeperiod=10)
    indicators['roc_20'] = talib.ROC(close, timeperiod=20)
    
    # MOM - Momentum
    indicators['mom_10'] = talib.MOM(close, timeperiod=10)
    
    # OBV - On Balance Volume
    if volume is not None:
        indicators['obv'] = talib.OBV(close, volume.values)
    
    return indicators


def compute_mean_reversion_indicators(prices):
    """
    Calcule les indicateurs de mean reversion (retour √† la moyenne).
    
    Args:
        prices: Series de prix
    
    Returns:
        DataFrame: Indicateurs de mean reversion
    
    Indicateurs inclus:
        - Bollinger Bands: Bandes de volatilit√© autour de la moyenne mobile
          * Prix > Upper Band: Surachet√©
          * Prix < Lower Band: Survendu
        
        - Z-Score: Nombre d'√©carts-types par rapport √† la moyenne
          * |Z| > 2: Signal potentiel de retour √† la moyenne
        
        - Distance to MA: √âcart par rapport √† la moyenne mobile
    """
    close = prices.values
    indicators = pd.DataFrame(index=prices.index)
    
    # Bollinger Bands
    for window in [20, 50]:
        upper, middle, lower = talib.BBANDS(close, 
                                             timeperiod=window,
                                             nbdevup=2, 
                                             nbdevdn=2)
        indicators[f'bb_upper_{window}'] = upper
        indicators[f'bb_middle_{window}'] = middle
        indicators[f'bb_lower_{window}'] = lower
        
        # Position dans les bandes (0 = lower, 1 = upper)
        indicators[f'bb_position_{window}'] = (close - lower) / (upper - lower)
    
    # Z-Score
    for window in [10, 20, 50]:
        rolling_mean = prices.rolling(window).mean()
        rolling_std = prices.rolling(window).std()
        indicators[f'zscore_{window}'] = (prices - rolling_mean) / rolling_std
    
    # Distance to Moving Average (en %)
    for window in [10, 20, 50, 200]:
        ma = prices.rolling(window).mean()
        indicators[f'dist_ma_{window}'] = (prices - ma) / ma * 100
    
    # Percentile Rank
    for window in [20, 63, 252]:
        indicators[f'pct_rank_{window}'] = prices.rolling(window).apply(
            lambda x: stats.percentileofscore(x, x.iloc[-1]) / 100
        )
    
    return indicators


def compute_volume_indicators(prices, volume, windows=[5, 10, 20]):
    """
    Calcule les indicateurs bas√©s sur le volume.
    
    Args:
        prices: Series de prix
        volume: Series de volume
        windows: Fen√™tres pour les moyennes mobiles
    
    Returns:
        DataFrame: Indicateurs de volume
    
    Indicateurs inclus:
        - Volume Ratio: Volume actuel / Moyenne mobile
          * > 2: Volume anormalement √©lev√© (possible signal)
        
        - VWAP (Volume Weighted Average Price): Prix moyen pond√©r√© par volume
        
        - Money Flow: Flux mon√©taire entrant/sortant
    """
    indicators = pd.DataFrame(index=prices.index)
    
    # Volume relatif
    for window in windows:
        vol_ma = volume.rolling(window).mean()
        indicators[f'vol_ratio_{window}'] = volume / vol_ma
    
    # Volume trend
    indicators['vol_trend_5'] = volume.rolling(5).mean() / volume.rolling(20).mean()
    
    # Price-Volume Correlation
    indicators['pv_corr_20'] = prices.rolling(20).corr(volume)
    
    # Dollar Volume (proxy de liquidit√©)
    dollar_vol = prices * volume
    indicators['dollar_vol_ma_20'] = dollar_vol.rolling(20).mean()
    
    # Volume Spike (z-score du volume)
    vol_mean = volume.rolling(20).mean()
    vol_std = volume.rolling(20).std()
    indicators['vol_zscore'] = (volume - vol_mean) / vol_std
    
    return indicators


class AlphaFactorPipeline:
    """
    Pipeline complet de cr√©ation de facteurs alpha.
    
    Cette classe encapsule tout le processus de feature engineering
    pour le trading quantitatif.
    
    Usage:
        pipeline = AlphaFactorPipeline()
        features = pipeline.fit_transform(data)
    """
    
    def __init__(self, include_volume=True, include_fundamentals=False):
        """
        Initialise le pipeline.
        
        Args:
            include_volume: Inclure les indicateurs de volume
            include_fundamentals: Inclure les ratios fondamentaux
        """
        self.include_volume = include_volume
        self.include_fundamentals = include_fundamentals
    
    def fit_transform(self, data):
        """
        G√©n√®re tous les alpha factors.
        
        Args:
            data: DataFrame avec colonnes 'open', 'high', 'low', 'close', 'volume'
        
        Returns:
            DataFrame: Tous les facteurs calcul√©s
        """
        features = pd.DataFrame(index=data.index)
        
        close = data['close']
        
        # 1. Rendements
        returns = compute_returns(close)
        features = features.join(returns)
        
        # 2. Volatilit√©
        vol = compute_volatility(close)
        features = features.join(vol)
        
        # 3. Momentum
        momentum = compute_momentum_indicators(
            close,
            high=data.get('high'),
            low=data.get('low'),
            volume=data.get('volume')
        )
        features = features.join(momentum)
        
        # 4. Mean Reversion
        mean_rev = compute_mean_reversion_indicators(close)
        features = features.join(mean_rev)
        
        # 5. Volume
        if self.include_volume and 'volume' in data.columns:
            vol_ind = compute_volume_indicators(close, data['volume'])
            features = features.join(vol_ind)
        
        # 6. Nettoyer les NaN
        features = features.replace([np.inf, -np.inf], np.nan)
        
        return features


# === Exemple d'utilisation ===
if __name__ == "__main__":
    # Simuler des donn√©es
    np.random.seed(42)
    n = 500
    
    data = pd.DataFrame({
        'open': np.random.randn(n).cumsum() + 100,
        'high': np.random.randn(n).cumsum() + 102,
        'low': np.random.randn(n).cumsum() + 98,
        'close': np.random.randn(n).cumsum() + 100,
        'volume': np.random.randint(1000000, 10000000, n)
    }, index=pd.date_range('2020-01-01', periods=n, freq='D'))
    
    # Cr√©er le pipeline
    pipeline = AlphaFactorPipeline()
    features = pipeline.fit_transform(data)
    
    print(f"Nombre de features: {features.shape[1]}")
    print(f"\nFeatures disponibles:")
    print(features.columns.tolist())
```

## 4.3 D√©bruitage avec Filtre de Kalman et Wavelets

```python
"""
D√©bruitage des S√©ries Temporelles Financi√®res
=============================================
Les prix financiers sont bruit√©s. Le d√©bruitage permet d'extraire
le signal sous-jacent pour de meilleures pr√©dictions.

M√©thodes:
1. Filtre de Kalman (Kalman Filter)
   - Mod√®le d'√©tat-espace
   - Estimation optimale en temps r√©el
   - Adaptatif aux changements de r√©gime

2. Transform√©e en Ondelettes (Wavelets)
   - D√©composition multi-√©chelle
   - Conservation des discontinuit√©s
   - Flexible pour diff√©rents types de signaux
"""
import numpy as np
import pandas as pd
from pykalman import KalmanFilter
import pywt

# === FILTRE DE KALMAN ===
class KalmanSmoother:
    """
    Lissage de s√©ries temporelles avec filtre de Kalman.
    
    Le filtre de Kalman mod√©lise le prix comme un processus d'√©tat cach√©
    observ√© avec du bruit. Il estime r√©cursivement l'√©tat r√©el.
    
    Mod√®le:
        x(t) = A * x(t-1) + w(t)    # √âquation d'√©tat
        y(t) = H * x(t) + v(t)      # √âquation d'observation
    
    o√π:
        x(t): √âtat cach√© (vrai prix)
        y(t): Observation (prix bruit√©)
        w(t): Bruit de processus
        v(t): Bruit d'observation
    """
    
    def __init__(self, observation_covariance=1, transition_covariance=0.01):
        """
        Initialise le filtre de Kalman.
        
        Args:
            observation_covariance: Variance du bruit d'observation (plus grand = plus de lissage)
            transition_covariance: Variance du bruit de processus (plus grand = plus r√©actif)
        """
        self.observation_covariance = observation_covariance
        self.transition_covariance = transition_covariance
        self.kf = None
    
    def fit(self, observations):
        """
        Ajuste le filtre de Kalman aux donn√©es.
        
        Args:
            observations: Array de prix observ√©s
        
        Returns:
            self
        """
        n_timesteps = len(observations)
        
        # D√©finir le filtre de Kalman
        self.kf = KalmanFilter(
            transition_matrices=[1],                          # A: marche al√©atoire
            observation_matrices=[1],                         # H: observation directe
            initial_state_mean=observations[0],               # √âtat initial
            initial_state_covariance=1,                       # Incertitude initiale
            observation_covariance=self.observation_covariance,
            transition_covariance=self.transition_covariance
        )
        
        return self
    
    def smooth(self, observations):
        """
        Applique le lissage de Kalman.
        
        Args:
            observations: Array de prix observ√©s
        
        Returns:
            tuple: (√©tat_liss√©, covariance)
        """
        if self.kf is None:
            self.fit(observations)
        
        # Lissage (utilise toutes les observations)
        state_means, state_covariances = self.kf.smooth(observations)
        
        return state_means.flatten(), state_covariances.flatten()
    
    def filter(self, observations):
        """
        Applique le filtrage de Kalman (temps r√©el).
        
        Contrairement au lissage, le filtrage n'utilise que les observations
        pass√©es et pr√©sentes (pas de look-ahead).
        
        Args:
            observations: Array de prix observ√©s
        
        Returns:
            tuple: (√©tat_filtr√©, covariance)
        """
        if self.kf is None:
            self.fit(observations)
        
        # Filtrage (temps r√©el)
        state_means, state_covariances = self.kf.filter(observations)
        
        return state_means.flatten(), state_covariances.flatten()


# === ONDELETTES (WAVELETS) ===
class WaveletDenoiser:
    """
    D√©bruitage par ondelettes.
    
    Les ondelettes d√©composent le signal en composantes de diff√©rentes
    fr√©quences, permettant de filtrer le bruit haute fr√©quence tout en
    pr√©servant les discontinuit√©s (changements abrupts).
    
    Ondelettes courantes:
        - 'db4': Daubechies 4 (bon compromis)
        - 'sym8': Symlet 8 (sym√©trique)
        - 'coif5': Coiflet 5 (moments nuls)
        - 'haar': Haar (simple, discontinuit√©s)
    """
    
    def __init__(self, wavelet='db4', level=None, threshold_mode='soft'):
        """
        Initialise le d√©bruiteur par ondelettes.
        
        Args:
            wavelet: Type d'ondelette ('db4', 'sym8', 'coif5', 'haar')
            level: Niveau de d√©composition (None = maximum)
            threshold_mode: 'soft' (doux) ou 'hard' (dur)
        """
        self.wavelet = wavelet
        self.level = level
        self.threshold_mode = threshold_mode
    
    def denoise(self, signal, threshold_method='universal'):
        """
        D√©bruite un signal avec les ondelettes.
        
        Args:
            signal: Array du signal √† d√©bruiter
            threshold_method: 'universal' (Donoho) ou 'bayesian'
        
        Returns:
            array: Signal d√©bruit√©
        """
        signal = np.array(signal)
        
        # D√©terminer le niveau de d√©composition
        if self.level is None:
            self.level = pywt.dwt_max_level(len(signal), self.wavelet)
        
        # D√©composition en ondelettes
        coeffs = pywt.wavedec(signal, self.wavelet, level=self.level)
        
        # Calculer le seuil
        if threshold_method == 'universal':
            # Seuil universel de Donoho-Johnstone
            # œÉ * sqrt(2 * log(n))
            sigma = self._estimate_noise(coeffs[-1])
            threshold = sigma * np.sqrt(2 * np.log(len(signal)))
        else:
            # Seuil adaptatif par niveau
            threshold = None
        
        # Appliquer le seuillage aux coefficients de d√©tail
        denoised_coeffs = [coeffs[0]]  # Garder les coefficients d'approximation
        
        for i, coeff in enumerate(coeffs[1:]):
            if threshold_method == 'universal':
                thresh = threshold
            else:
                # BayesShrink
                sigma = self._estimate_noise(coeff)
                sigma_signal = np.sqrt(max(np.var(coeff) - sigma**2, 0))
                thresh = sigma**2 / sigma_signal if sigma_signal > 0 else np.max(np.abs(coeff))
            
            # Appliquer le seuil
            denoised_coeff = pywt.threshold(coeff, thresh, mode=self.threshold_mode)
            denoised_coeffs.append(denoised_coeff)
        
        # Reconstruction
        denoised_signal = pywt.waverec(denoised_coeffs, self.wavelet)
        
        # Ajuster la longueur (peut diff√©rer l√©g√®rement)
        return denoised_signal[:len(signal)]
    
    def _estimate_noise(self, detail_coeffs):
        """
        Estime le niveau de bruit √† partir des coefficients de d√©tail.
        
        Utilise la MAD (Median Absolute Deviation) qui est robuste aux outliers.
        
        Args:
            detail_coeffs: Coefficients de d√©tail du niveau le plus fin
        
        Returns:
            float: Estimation de sigma
        """
        # MAD / 0.6745 est un estimateur robuste de sigma
        return np.median(np.abs(detail_coeffs)) / 0.6745
    
    def decompose(self, signal, return_details=True):
        """
        D√©compose le signal en composantes de diff√©rentes √©chelles.
        
        Args:
            signal: Signal √† d√©composer
            return_details: Si True, retourne aussi les d√©tails
        
        Returns:
            dict: Composantes √† diff√©rentes √©chelles
        """
        signal = np.array(signal)
        
        if self.level is None:
            self.level = pywt.dwt_max_level(len(signal), self.wavelet)
        
        # D√©composition
        coeffs = pywt.wavedec(signal, self.wavelet, level=self.level)
        
        # Reconstruction par niveau
        result = {}
        
        # Approximation (tendance basse fr√©quence)
        approx_coeffs = [coeffs[0]] + [np.zeros_like(c) for c in coeffs[1:]]
        result['approximation'] = pywt.waverec(approx_coeffs, self.wavelet)[:len(signal)]
        
        if return_details:
            # D√©tails √† chaque niveau
            for i in range(1, len(coeffs)):
                detail_coeffs = [np.zeros_like(coeffs[0])]
                for j in range(1, len(coeffs)):
                    if j == i:
                        detail_coeffs.append(coeffs[j])
                    else:
                        detail_coeffs.append(np.zeros_like(coeffs[j]))
                
                result[f'detail_level_{i}'] = pywt.waverec(detail_coeffs, self.wavelet)[:len(signal)]
        
        return result


# === Comparaison des m√©thodes ===
def compare_denoising_methods(prices, plot=True):
    """
    Compare les m√©thodes de d√©bruitage.
    
    Args:
        prices: Series de prix
        plot: Si True, affiche un graphique
    
    Returns:
        DataFrame: Prix d√©bruit√©s par chaque m√©thode
    """
    prices_array = prices.values
    
    results = pd.DataFrame(index=prices.index)
    results['original'] = prices_array
    
    # Filtre de Kalman
    kalman = KalmanSmoother(observation_covariance=0.1, transition_covariance=0.01)
    results['kalman_smooth'], _ = kalman.smooth(prices_array)
    results['kalman_filter'], _ = kalman.filter(prices_array)
    
    # Ondelettes
    wavelet = WaveletDenoiser(wavelet='db4')
    results['wavelet_db4'] = wavelet.denoise(prices_array)
    
    wavelet_sym = WaveletDenoiser(wavelet='sym8')
    results['wavelet_sym8'] = wavelet_sym.denoise(prices_array)
    
    # Moving Average (baseline)
    results['ma_20'] = prices.rolling(20).mean()
    
    # EMA (Exponential Moving Average)
    results['ema_20'] = prices.ewm(span=20).mean()
    
    if plot:
        import matplotlib.pyplot as plt
        
        fig, axes = plt.subplots(2, 1, figsize=(14, 10))
        
        # Plot 1: Toutes les m√©thodes
        ax1 = axes[0]
        ax1.plot(results['original'], alpha=0.5, label='Original', linewidth=0.5)
        ax1.plot(results['kalman_smooth'], label='Kalman Smooth', linewidth=1.5)
        ax1.plot(results['wavelet_db4'], label='Wavelet DB4', linewidth=1.5)
        ax1.plot(results['ma_20'], label='MA 20', linestyle='--')
        ax1.legend()
        ax1.set_title('Comparison of Denoising Methods')
        
        # Plot 2: Zoom sur une p√©riode
        zoom_start = len(results) // 2
        zoom_end = zoom_start + 100
        ax2 = axes[1]
        ax2.plot(results['original'].iloc[zoom_start:zoom_end], 
                alpha=0.5, label='Original', linewidth=0.5)
        ax2.plot(results['kalman_smooth'].iloc[zoom_start:zoom_end], 
                label='Kalman', linewidth=2)
        ax2.plot(results['wavelet_db4'].iloc[zoom_start:zoom_end], 
                label='Wavelet', linewidth=2)
        ax2.legend()
        ax2.set_title('Zoomed View')
        
        plt.tight_layout()
        plt.show()
    
    return results


# === Exemple d'utilisation ===
if __name__ == "__main__":
    # Cr√©er un signal synth√©tique bruit√©
    np.random.seed(42)
    n = 500
    
    # Signal vrai: tendance + oscillation
    t = np.linspace(0, 10, n)
    true_signal = 100 + 0.5 * t + 5 * np.sin(t)
    
    # Ajouter du bruit
    noise = np.random.randn(n) * 2
    noisy_signal = true_signal + noise
    
    prices = pd.Series(noisy_signal, 
                       index=pd.date_range('2020-01-01', periods=n, freq='D'))
    
    # Comparer les m√©thodes
    results = compare_denoising_methods(prices, plot=False)
    
    # Calculer les erreurs
    print("\nMean Squared Error vs True Signal:")
    print("-" * 40)
    for col in results.columns:
        if col != 'original':
            mse = np.nanmean((results[col].values - true_signal)**2)
            print(f"{col:<20}: {mse:.4f}")
```

---

# 5. √âVALUATION DE STRAT√âGIE
## Strategy Evaluation

## 5.1 M√©triques de Performance

```python
"""
M√©triques de Performance pour Strat√©gies de Trading
===================================================
Ces m√©triques permettent d'√©valuer objectivement une strat√©gie:

1. Rendement (Return): Performance absolue
2. Risque (Risk): Volatilit√©, drawdown
3. Ratio risque/rendement: Sharpe, Sortino, Calmar
4. Stabilit√©: Consistance des rendements
"""
import numpy as np
import pandas as pd
from scipy import stats

def calculate_returns(prices):
    """
    Calcule les rendements √† partir des prix.
    
    Args:
        prices: Series ou array de prix
    
    Returns:
        Series: Rendements journaliers
    """
    return pd.Series(prices).pct_change().dropna()


def annualized_return(returns, periods_per_year=252):
    """
    Calcule le rendement annualis√©.
    
    Formule: (1 + rendement_moyen)^252 - 1
    
    Args:
        returns: Series de rendements journaliers
        periods_per_year: Nombre de p√©riodes par an (252 jours de trading)
    
    Returns:
        float: Rendement annualis√©
    
    Exemple:
        >>> returns = pd.Series([0.01, 0.02, -0.01, 0.015])
        >>> ann_ret = annualized_return(returns)
        >>> print(f"Rendement annualis√©: {ann_ret:.2%}")
    """
    total_return = (1 + returns).prod()
    n_periods = len(returns)
    return total_return ** (periods_per_year / n_periods) - 1


def annualized_volatility(returns, periods_per_year=252):
    """
    Calcule la volatilit√© annualis√©e.
    
    Formule: œÉ_journalier * ‚àö252
    
    Args:
        returns: Series de rendements journaliers
        periods_per_year: Nombre de p√©riodes par an
    
    Returns:
        float: Volatilit√© annualis√©e
    """
    return returns.std() * np.sqrt(periods_per_year)


def sharpe_ratio(returns, risk_free_rate=0.02, periods_per_year=252):
    """
    Calcule le ratio de Sharpe.
    
    Le Sharpe mesure le rendement exc√©dentaire par unit√© de risque:
    Sharpe = (Rendement - Taux sans risque) / Volatilit√©
    
    Interpr√©tation:
        < 1.0  : Sous-performance
        1.0-2.0: Acceptable
        2.0-3.0: Tr√®s bon
        > 3.0  : Excellent (ou suspect!)
    
    Args:
        returns: Series de rendements
        risk_free_rate: Taux sans risque annuel
        periods_per_year: Nombre de p√©riodes par an
    
    Returns:
        float: Ratio de Sharpe
    """
    # Convertir le taux sans risque en journalier
    rf_per_period = (1 + risk_free_rate) ** (1 / periods_per_year) - 1
    
    # Rendement exc√©dentaire
    excess_returns = returns - rf_per_period
    
    # Sharpe annualis√©
    return np.sqrt(periods_per_year) * excess_returns.mean() / excess_returns.std()


def sortino_ratio(returns, risk_free_rate=0.02, periods_per_year=252):
    """
    Calcule le ratio de Sortino.
    
    Similaire au Sharpe mais ne p√©nalise que la volatilit√© n√©gative
    (downside risk), pas la volatilit√© positive.
    
    Sortino = (Rendement - Taux sans risque) / Downside Deviation
    
    Args:
        returns: Series de rendements
        risk_free_rate: Taux sans risque annuel
        periods_per_year: Nombre de p√©riodes par an
    
    Returns:
        float: Ratio de Sortino
    """
    rf_per_period = (1 + risk_free_rate) ** (1 / periods_per_year) - 1
    excess_returns = returns - rf_per_period
    
    # Downside deviation: √©cart-type des rendements n√©gatifs seulement
    downside_returns = excess_returns[excess_returns < 0]
    downside_std = downside_returns.std()
    
    if downside_std == 0:
        return np.inf
    
    return np.sqrt(periods_per_year) * excess_returns.mean() / downside_std


def max_drawdown(prices):
    """
    Calcule le maximum drawdown (perte maximale depuis un pic).
    
    Le drawdown mesure la perte maximale qu'un investisseur aurait
    subie en entrant au pire moment.
    
    Args:
        prices: Series de prix ou valeur du portefeuille
    
    Returns:
        float: Maximum drawdown (n√©gatif)
    
    Exemple:
        >>> prices = pd.Series([100, 110, 90, 95, 120, 100])
        >>> mdd = max_drawdown(prices)
        >>> print(f"Max Drawdown: {mdd:.2%}")  # -18.18%
    """
    prices = pd.Series(prices)
    
    # Pic cumulatif
    cumulative_max = prices.cummax()
    
    # Drawdown √† chaque point
    drawdown = (prices - cumulative_max) / cumulative_max
    
    return drawdown.min()


def calmar_ratio(returns, prices=None, periods_per_year=252):
    """
    Calcule le ratio de Calmar.
    
    Calmar = Rendement annualis√© / |Max Drawdown|
    
    Plus le Calmar est √©lev√©, meilleur est le rendement par rapport
    au risque de perte maximale.
    
    Args:
        returns: Series de rendements
        prices: Series de prix (optionnel, calcul√© si non fourni)
        periods_per_year: Nombre de p√©riodes par an
    
    Returns:
        float: Ratio de Calmar
    """
    ann_ret = annualized_return(returns, periods_per_year)
    
    if prices is None:
        prices = (1 + returns).cumprod()
    
    mdd = abs(max_drawdown(prices))
    
    if mdd == 0:
        return np.inf
    
    return ann_ret / mdd


def information_ratio(returns, benchmark_returns):
    """
    Calcule le ratio d'information.
    
    IR = Alpha / Tracking Error
    
    Mesure la performance ajust√©e au risque par rapport √† un benchmark.
    
    Args:
        returns: Series de rendements de la strat√©gie
        benchmark_returns: Series de rendements du benchmark
    
    Returns:
        float: Ratio d'information
    """
    # Rendement actif (diff√©rence avec le benchmark)
    active_returns = returns - benchmark_returns
    
    # Tracking error (volatilit√© du rendement actif)
    tracking_error = active_returns.std() * np.sqrt(252)
    
    if tracking_error == 0:
        return np.inf
    
    return (active_returns.mean() * 252) / tracking_error


def win_rate(returns):
    """
    Calcule le taux de gains.
    
    Args:
        returns: Series de rendements
    
    Returns:
        float: Pourcentage de p√©riodes positives
    """
    return (returns > 0).mean()


def profit_factor(returns):
    """
    Calcule le profit factor.
    
    Profit Factor = Somme des gains / |Somme des pertes|
    
    Args:
        returns: Series de rendements
    
    Returns:
        float: Profit factor (> 1 = profitable)
    """
    gains = returns[returns > 0].sum()
    losses = abs(returns[returns < 0].sum())
    
    if losses == 0:
        return np.inf
    
    return gains / losses


def calculate_all_metrics(returns, prices=None, benchmark_returns=None, 
                          risk_free_rate=0.02):
    """
    Calcule toutes les m√©triques de performance.
    
    Args:
        returns: Series de rendements
        prices: Series de prix (optionnel)
        benchmark_returns: Series de rendements du benchmark (optionnel)
        risk_free_rate: Taux sans risque annuel
    
    Returns:
        dict: Toutes les m√©triques
    """
    if prices is None:
        prices = (1 + returns).cumprod() * 100  # Partir de 100
    
    metrics = {
        # Rendement
        'total_return': (prices.iloc[-1] / prices.iloc[0]) - 1,
        'annualized_return': annualized_return(returns),
        'cagr': (prices.iloc[-1] / prices.iloc[0]) ** (252 / len(returns)) - 1,
        
        # Risque
        'annualized_volatility': annualized_volatility(returns),
        'max_drawdown': max_drawdown(prices),
        'var_95': returns.quantile(0.05),  # Value at Risk 95%
        'cvar_95': returns[returns <= returns.quantile(0.05)].mean(),  # Conditional VaR
        
        # Ratios
        'sharpe_ratio': sharpe_ratio(returns, risk_free_rate),
        'sortino_ratio': sortino_ratio(returns, risk_free_rate),
        'calmar_ratio': calmar_ratio(returns, prices),
        
        # Trading
        'win_rate': win_rate(returns),
        'profit_factor': profit_factor(returns),
        'avg_win': returns[returns > 0].mean() if (returns > 0).any() else 0,
        'avg_loss': returns[returns < 0].mean() if (returns < 0).any() else 0,
        
        # Distribution
        'skewness': returns.skew(),
        'kurtosis': returns.kurtosis(),
    }
    
    if benchmark_returns is not None:
        metrics['information_ratio'] = information_ratio(returns, benchmark_returns)
        metrics['beta'] = returns.cov(benchmark_returns) / benchmark_returns.var()
        metrics['alpha'] = metrics['annualized_return'] - metrics['beta'] * annualized_return(benchmark_returns)
    
    return metrics


# === Affichage des m√©triques ===
def print_performance_report(metrics):
    """
    Affiche un rapport de performance format√©.
    
    Args:
        metrics: dict retourn√© par calculate_all_metrics
    """
    print("\n" + "="*60)
    print("RAPPORT DE PERFORMANCE / PERFORMANCE REPORT")
    print("="*60)
    
    print("\nüìà RENDEMENT / RETURN")
    print("-"*40)
    print(f"  Rendement total      : {metrics['total_return']:>10.2%}")
    print(f"  Rendement annualis√©  : {metrics['annualized_return']:>10.2%}")
    print(f"  CAGR                 : {metrics['cagr']:>10.2%}")
    
    print("\nüìâ RISQUE / RISK")
    print("-"*40)
    print(f"  Volatilit√© annualis√©e: {metrics['annualized_volatility']:>10.2%}")
    print(f"  Max Drawdown         : {metrics['max_drawdown']:>10.2%}")
    print(f"  VaR 95%              : {metrics['var_95']:>10.2%}")
    print(f"  CVaR 95%             : {metrics['cvar_95']:>10.2%}")
    
    print("\nüìä RATIOS")
    print("-"*40)
    print(f"  Sharpe Ratio         : {metrics['sharpe_ratio']:>10.2f}")
    print(f"  Sortino Ratio        : {metrics['sortino_ratio']:>10.2f}")
    print(f"  Calmar Ratio         : {metrics['calmar_ratio']:>10.2f}")
    
    print("\nüéØ TRADING")
    print("-"*40)
    print(f"  Win Rate             : {metrics['win_rate']:>10.2%}")
    print(f"  Profit Factor        : {metrics['profit_factor']:>10.2f}")
    print(f"  Gain moyen           : {metrics['avg_win']:>10.2%}")
    print(f"  Perte moyenne        : {metrics['avg_loss']:>10.2%}")
    
    if 'information_ratio' in metrics:
        print("\nüìé VS BENCHMARK")
        print("-"*40)
        print(f"  Information Ratio    : {metrics['information_ratio']:>10.2f}")
        print(f"  Beta                 : {metrics['beta']:>10.2f}")
        print(f"  Alpha                : {metrics['alpha']:>10.2%}")
    
    print("\n" + "="*60)
```

## 5.2 Backtesting Vectoris√©

```python
"""
Backtesting Vectoris√©
=====================
Le backtesting vectoris√© utilise les op√©rations pandas/numpy pour
simuler une strat√©gie sur donn√©es historiques.

Avantages:
- Tr√®s rapide (pas de boucles)
- Facile √† impl√©menter
- Bon pour le prototypage

Inconv√©nients:
- Pas de gestion des ordres complexes
- Pas de slippage r√©aliste
- Look-ahead bias potentiel
"""
import numpy as np
import pandas as pd

class VectorizedBacktest:
    """
    Backtester vectoris√© simple.
    
    Cette classe permet de tester rapidement des strat√©gies bas√©es
    sur des signaux.
    """
    
    def __init__(self, prices, signals, initial_capital=100000, 
                 transaction_cost=0.001):
        """
        Initialise le backtester.
        
        Args:
            prices: Series de prix
            signals: Series de signaux (-1, 0, 1) pour short/flat/long
            initial_capital: Capital initial
            transaction_cost: Co√ªt de transaction (0.1% = 0.001)
        """
        self.prices = prices
        self.signals = signals
        self.initial_capital = initial_capital
        self.transaction_cost = transaction_cost
        
        # R√©sultats
        self.positions = None
        self.portfolio_value = None
        self.returns = None
    
    def run(self):
        """
        Ex√©cute le backtest.
        
        Returns:
            DataFrame: R√©sultats du backtest
        """
        # Aligner les donn√©es
        prices = self.prices.copy()
        signals = self.signals.reindex(prices.index).fillna(0)
        
        # Positions: signal d√©cal√© d'un jour (on trade le lendemain du signal)
        self.positions = signals.shift(1).fillna(0)
        
        # Rendements du march√©
        market_returns = prices.pct_change()
        
        # Rendements de la strat√©gie (position * rendement march√©)
        strategy_returns = self.positions * market_returns
        
        # Co√ªts de transaction
        # On paie quand la position change
        position_changes = self.positions.diff().abs()
        costs = position_changes * self.transaction_cost
        
        # Rendements nets
        self.returns = strategy_returns - costs
        
        # Valeur du portefeuille
        self.portfolio_value = self.initial_capital * (1 + self.returns).cumprod()
        
        # Cr√©er le DataFrame de r√©sultats
        results = pd.DataFrame({
            'price': prices,
            'signal': signals,
            'position': self.positions,
            'market_return': market_returns,
            'strategy_return': strategy_returns,
            'costs': costs,
            'net_return': self.returns,
            'portfolio_value': self.portfolio_value
        })
        
        return results
    
    def get_metrics(self):
        """
        Calcule les m√©triques de performance.
        
        Returns:
            dict: M√©triques de performance
        """
        if self.returns is None:
            self.run()
        
        return calculate_all_metrics(self.returns, self.portfolio_value)
    
    def plot_results(self, benchmark_prices=None):
        """
        Affiche les r√©sultats du backtest.
        
        Args:
            benchmark_prices: Series de prix du benchmark (optionnel)
        """
        import matplotlib.pyplot as plt
        
        fig, axes = plt.subplots(3, 1, figsize=(14, 12))
        
        # 1. Valeur du portefeuille
        ax1 = axes[0]
        ax1.plot(self.portfolio_value, label='Strategy', linewidth=2)
        
        if benchmark_prices is not None:
            benchmark_value = self.initial_capital * (benchmark_prices / benchmark_prices.iloc[0])
            ax1.plot(benchmark_value, label='Benchmark', linewidth=2, alpha=0.7)
        
        ax1.set_title('Portfolio Value')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. Drawdown
        ax2 = axes[1]
        cummax = self.portfolio_value.cummax()
        drawdown = (self.portfolio_value - cummax) / cummax
        ax2.fill_between(drawdown.index, drawdown, 0, alpha=0.5, color='red')
        ax2.set_title('Drawdown')
        ax2.grid(True, alpha=0.3)
        
        # 3. Positions
        ax3 = axes[2]
        ax3.plot(self.positions, label='Position', drawstyle='steps-post')
        ax3.axhline(0, color='black', linestyle='--', alpha=0.3)
        ax3.set_title('Positions')
        ax3.set_ylim(-1.5, 1.5)
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()


# === Strat√©gies de base ===
def moving_average_crossover(prices, fast_window=10, slow_window=50):
    """
    Strat√©gie de croisement de moyennes mobiles.
    
    Signal LONG quand MA rapide > MA lente (tendance haussi√®re)
    Signal SHORT quand MA rapide < MA lente (tendance baissi√®re)
    
    Args:
        prices: Series de prix
        fast_window: Fen√™tre de la MA rapide
        slow_window: Fen√™tre de la MA lente
    
    Returns:
        Series: Signaux (-1, 0, 1)
    """
    fast_ma = prices.rolling(fast_window).mean()
    slow_ma = prices.rolling(slow_window).mean()
    
    signals = pd.Series(0, index=prices.index)
    signals[fast_ma > slow_ma] = 1    # Long
    signals[fast_ma < slow_ma] = -1   # Short
    
    return signals


def mean_reversion(prices, window=20, threshold=2):
    """
    Strat√©gie de retour √† la moyenne.
    
    SHORT quand le prix est trop au-dessus de la moyenne (surachat)
    LONG quand le prix est trop en-dessous de la moyenne (survente)
    
    Args:
        prices: Series de prix
        window: Fen√™tre pour la moyenne et √©cart-type
        threshold: Nombre d'√©carts-types pour d√©clencher un signal
    
    Returns:
        Series: Signaux (-1, 0, 1)
    """
    rolling_mean = prices.rolling(window).mean()
    rolling_std = prices.rolling(window).std()
    
    z_score = (prices - rolling_mean) / rolling_std
    
    signals = pd.Series(0, index=prices.index)
    signals[z_score > threshold] = -1   # Short (surachat)
    signals[z_score < -threshold] = 1   # Long (survente)
    
    return signals


def momentum_strategy(prices, lookback=20, top_pct=0.2):
    """
    Strat√©gie momentum.
    
    LONG sur les actifs avec les meilleurs rendements pass√©s.
    
    Args:
        prices: Series de prix
        lookback: P√©riode de calcul du momentum
        top_pct: Pourcentage des meilleurs (ex: 0.2 = top 20%)
    
    Returns:
        Series: Signaux (0, 1)
    """
    returns = prices.pct_change(lookback)
    
    signals = pd.Series(0, index=prices.index)
    signals[returns > returns.quantile(1 - top_pct)] = 1
    
    return signals


# === Exemple complet ===
def run_backtest_example():
    """Exemple complet de backtest."""
    import yfinance as yf
    
    # T√©l√©charger les donn√©es
    ticker = "SPY"
    data = yf.download(ticker, start="2018-01-01", end="2023-12-31")
    prices = data['Adj Close']
    
    # Cr√©er les signaux
    signals = moving_average_crossover(prices, fast_window=20, slow_window=50)
    
    # Ex√©cuter le backtest
    bt = VectorizedBacktest(prices, signals, transaction_cost=0.001)
    results = bt.run()
    
    # Afficher les m√©triques
    metrics = bt.get_metrics()
    print_performance_report(metrics)
    
    # Afficher les graphiques
    bt.plot_results(benchmark_prices=prices)
    
    return results


# Pour ex√©cuter:
# results = run_backtest_example()
```

## 5.3 Optimisation de Portefeuille Mean-Variance

```python
"""
Optimisation de Portefeuille Mean-Variance (Markowitz)
======================================================
La th√©orie moderne du portefeuille (MPT - Modern Portfolio Theory) de
Harry Markowitz (1952) cherche √† maximiser le rendement pour un niveau
de risque donn√©, ou minimiser le risque pour un rendement cible.

Concepts cl√©s:
- Fronti√®re efficiente: Ensemble des portefeuilles optimaux
- Portefeuille tangent: Meilleur ratio Sharpe
- Diversification: R√©duction du risque non-syst√©matique
"""
import numpy as np
import pandas as pd
from scipy.optimize import minimize
import matplotlib.pyplot as plt

class MeanVarianceOptimizer:
    """
    Optimiseur Mean-Variance (Markowitz).
    
    Trouve les poids optimaux pour un portefeuille d'actifs.
    """
    
    def __init__(self, returns, risk_free_rate=0.02):
        """
        Initialise l'optimiseur.
        
        Args:
            returns: DataFrame de rendements (colonnes = actifs)
            risk_free_rate: Taux sans risque annuel
        """
        self.returns = returns
        self.risk_free_rate = risk_free_rate
        
        # Calculer les statistiques
        self.mean_returns = returns.mean() * 252  # Annualis√©
        self.cov_matrix = returns.cov() * 252     # Annualis√©
        self.n_assets = len(returns.columns)
        self.asset_names = returns.columns.tolist()
    
    def portfolio_return(self, weights):
        """
        Calcule le rendement attendu du portefeuille.
        
        Args:
            weights: Array de poids
        
        Returns:
            float: Rendement annualis√©
        """
        return np.dot(weights, self.mean_returns)
    
    def portfolio_volatility(self, weights):
        """
        Calcule la volatilit√© du portefeuille.
        
        Args:
            weights: Array de poids
        
        Returns:
            float: Volatilit√© annualis√©e
        """
        return np.sqrt(np.dot(weights.T, np.dot(self.cov_matrix, weights)))
    
    def portfolio_sharpe(self, weights):
        """
        Calcule le ratio de Sharpe du portefeuille.
        
        Args:
            weights: Array de poids
        
        Returns:
            float: Ratio de Sharpe
        """
        ret = self.portfolio_return(weights)
        vol = self.portfolio_volatility(weights)
        return (ret - self.risk_free_rate) / vol
    
    def optimize_sharpe(self, allow_short=False):
        """
        Trouve le portefeuille avec le meilleur ratio de Sharpe.
        
        Args:
            allow_short: Autoriser les positions short
        
        Returns:
            dict: R√©sultat de l'optimisation
        """
        # Fonction objectif: minimiser -Sharpe (pour maximiser Sharpe)
        def neg_sharpe(weights):
            return -self.portfolio_sharpe(weights)
        
        # Contraintes
        constraints = [
            {'type': 'eq', 'fun': lambda x: np.sum(x) - 1}  # Somme = 1
        ]
        
        # Bornes
        if allow_short:
            bounds = tuple((-1, 1) for _ in range(self.n_assets))
        else:
            bounds = tuple((0, 1) for _ in range(self.n_assets))
        
        # Point de d√©part: √©quipond√©r√©
        init_weights = np.ones(self.n_assets) / self.n_assets
        
        # Optimisation
        result = minimize(
            neg_sharpe,
            init_weights,
            method='SLSQP',
            bounds=bounds,
            constraints=constraints
        )
        
        optimal_weights = result.x
        
        return {
            'weights': dict(zip(self.asset_names, optimal_weights)),
            'return': self.portfolio_return(optimal_weights),
            'volatility': self.portfolio_volatility(optimal_weights),
            'sharpe': self.portfolio_sharpe(optimal_weights)
        }
    
    def optimize_min_volatility(self, target_return=None):
        """
        Trouve le portefeuille de variance minimale.
        
        Args:
            target_return: Rendement cible (optionnel)
        
        Returns:
            dict: R√©sultat de l'optimisation
        """
        # Fonction objectif: minimiser la volatilit√©
        def volatility(weights):
            return self.portfolio_volatility(weights)
        
        # Contraintes
        constraints = [
            {'type': 'eq', 'fun': lambda x: np.sum(x) - 1}
        ]
        
        if target_return is not None:
            constraints.append({
                'type': 'eq', 
                'fun': lambda x: self.portfolio_return(x) - target_return
            })
        
        # Bornes (long only)
        bounds = tuple((0, 1) for _ in range(self.n_assets))
        
        # Point de d√©part
        init_weights = np.ones(self.n_assets) / self.n_assets
        
        # Optimisation
        result = minimize(
            volatility,
            init_weights,
            method='SLSQP',
            bounds=bounds,
            constraints=constraints
        )
        
        optimal_weights = result.x
        
        return {
            'weights': dict(zip(self.asset_names, optimal_weights)),
            'return': self.portfolio_return(optimal_weights),
            'volatility': self.portfolio_volatility(optimal_weights),
            'sharpe': self.portfolio_sharpe(optimal_weights)
        }
    
    def efficient_frontier(self, n_points=50):
        """
        Calcule la fronti√®re efficiente.
        
        Args:
            n_points: Nombre de points sur la fronti√®re
        
        Returns:
            DataFrame: Points de la fronti√®re efficiente
        """
        # Range de rendements cibles
        min_ret = self.mean_returns.min()
        max_ret = self.mean_returns.max()
        target_returns = np.linspace(min_ret, max_ret, n_points)
        
        frontier = []
        
        for target in target_returns:
            try:
                result = self.optimize_min_volatility(target_return=target)
                frontier.append({
                    'target_return': target,
                    'return': result['return'],
                    'volatility': result['volatility'],
                    'sharpe': result['sharpe']
                })
            except:
                continue
        
        return pd.DataFrame(frontier)
    
    def plot_efficient_frontier(self, n_points=50, show_assets=True):
        """
        Affiche la fronti√®re efficiente.
        
        Args:
            n_points: Nombre de points sur la fronti√®re
            show_assets: Afficher les actifs individuels
        """
        # Calculer la fronti√®re
        frontier = self.efficient_frontier(n_points)
        
        # Portefeuille optimal (max Sharpe)
        optimal = self.optimize_sharpe()
        
        # Portefeuille min variance
        min_vol = self.optimize_min_volatility()
        
        # Plot
        fig, ax = plt.subplots(figsize=(12, 8))
        
        # Fronti√®re efficiente
        ax.plot(frontier['volatility'], frontier['return'], 
                'b-', linewidth=2, label='Efficient Frontier')
        
        # Portefeuille optimal
        ax.scatter(optimal['volatility'], optimal['return'], 
                  marker='*', s=300, c='red', label='Max Sharpe')
        
        # Portefeuille min variance
        ax.scatter(min_vol['volatility'], min_vol['return'], 
                  marker='o', s=200, c='green', label='Min Volatility')
        
        # Actifs individuels
        if show_assets:
            for i, asset in enumerate(self.asset_names):
                ret = self.mean_returns.iloc[i]
                vol = np.sqrt(self.cov_matrix.iloc[i, i])
                ax.scatter(vol, ret, s=100, alpha=0.7)
                ax.annotate(asset, (vol, ret), fontsize=10)
        
        # Capital Market Line (CML)
        max_sharpe_ret = optimal['return']
        max_sharpe_vol = optimal['volatility']
        cml_x = np.linspace(0, max_sharpe_vol * 1.5, 100)
        cml_y = self.risk_free_rate + (max_sharpe_ret - self.risk_free_rate) / max_sharpe_vol * cml_x
        ax.plot(cml_x, cml_y, 'r--', label='Capital Market Line')
        
        ax.set_xlabel('Volatility (Annualized)')
        ax.set_ylabel('Expected Return (Annualized)')
        ax.set_title('Efficient Frontier')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
        
        return fig


# === Kelly Criterion ===
def kelly_criterion(win_prob, win_loss_ratio):
    """
    Calcule la fraction Kelly optimale.
    
    La formule Kelly d√©termine la fraction optimale du capital √† risquer
    pour maximiser la croissance √† long terme.
    
    f* = (p * b - q) / b
    
    o√π:
        p = probabilit√© de gain
        q = probabilit√© de perte (1 - p)
        b = ratio gain/perte
    
    Args:
        win_prob: Probabilit√© de gain (ex: 0.55)
        win_loss_ratio: Ratio gain moyen / perte moyenne (ex: 1.5)
    
    Returns:
        float: Fraction Kelly (ex: 0.2 = 20% du capital)
    """
    q = 1 - win_prob
    kelly = (win_prob * win_loss_ratio - q) / win_loss_ratio
    return max(0, kelly)  # Ne pas retourner de valeur n√©gative


def half_kelly(win_prob, win_loss_ratio):
    """
    Calcule le demi-Kelly (plus conservateur).
    
    En pratique, on utilise souvent une fraction du Kelly (1/2, 1/4)
    car la formule suppose des param√®tres parfaitement connus.
    
    Args:
        win_prob: Probabilit√© de gain
        win_loss_ratio: Ratio gain/perte
    
    Returns:
        float: Demi-fraction Kelly
    """
    return kelly_criterion(win_prob, win_loss_ratio) / 2


# === Exemple ===
def run_portfolio_optimization_example():
    """Exemple d'optimisation de portefeuille."""
    import yfinance as yf
    
    # T√©l√©charger les donn√©es
    tickers = ['AAPL', 'GOOGL', 'MSFT', 'AMZN', 'JPM', 'JNJ', 'XOM', 'PG']
    data = yf.download(tickers, start='2019-01-01', end='2023-12-31')['Adj Close']
    
    # Calculer les rendements
    returns = data.pct_change().dropna()
    
    # Cr√©er l'optimiseur
    optimizer = MeanVarianceOptimizer(returns)
    
    # Optimiser
    print("="*60)
    print("PORTEFEUILLE OPTIMAL (MAX SHARPE)")
    print("="*60)
    optimal = optimizer.optimize_sharpe()
    print(f"\nRendement: {optimal['return']:.2%}")
    print(f"Volatilit√©: {optimal['volatility']:.2%}")
    print(f"Sharpe Ratio: {optimal['sharpe']:.2f}")
    print("\nPoids:")
    for asset, weight in sorted(optimal['weights'].items(), key=lambda x: -x[1]):
        if abs(weight) > 0.01:
            print(f"  {asset}: {weight:.2%}")
    
    print("\n" + "="*60)
    print("PORTEFEUILLE MIN VARIANCE")
    print("="*60)
    min_vol = optimizer.optimize_min_volatility()
    print(f"\nRendement: {min_vol['return']:.2%}")
    print(f"Volatilit√©: {min_vol['volatility']:.2%}")
    print(f"Sharpe Ratio: {min_vol['sharpe']:.2f}")
    
    # Afficher la fronti√®re efficiente
    optimizer.plot_efficient_frontier()
    
    return optimizer


# Pour ex√©cuter:
# optimizer = run_portfolio_optimization_example()
```

---

# 6. PROCESSUS MACHINE LEARNING
## ML Workflow

## 6.1 Cross-Validation pour S√©ries Temporelles

```python
"""
Cross-Validation pour S√©ries Temporelles Financi√®res
====================================================
La cross-validation standard (K-Fold) ne fonctionne PAS pour les s√©ries
temporelles car elle cr√©e un look-ahead bias (utiliser des donn√©es futures
pour pr√©dire le pass√©).

Solutions:
1. TimeSeriesSplit: Validation glissante
2. Walk-Forward: R√©-entra√Ænement p√©riodique
3. Purged K-Fold: Avec gap entre train et test
"""
import numpy as np
import pandas as pd
from sklearn.model_selection import BaseCrossValidator

class TimeSeriesSplitCustom(BaseCrossValidator):
    """
    Split temporel personnalis√© avec gap et embargo.
    
    Le gap emp√™che le look-ahead bias en cr√©ant un tampon entre
    les donn√©es d'entra√Ænement et de test.
    
    Exemple visuel:
    
    |------- Train -------|  Gap  |--- Test ---|
    |=====================|       |============|
    t0                   t1       t2           t3
    """
    
    def __init__(self, n_splits=5, train_period_length=252, 
                 test_period_length=63, gap=5):
        """
        Initialise le splitter.
        
        Args:
            n_splits: Nombre de splits
            train_period_length: Taille de la p√©riode d'entra√Ænement (en jours)
            test_period_length: Taille de la p√©riode de test
            gap: Nombre de jours entre train et test (pour √©viter look-ahead)
        """
        self.n_splits = n_splits
        self.train_length = train_period_length
        self.test_length = test_period_length
        self.gap = gap
    
    def split(self, X, y=None, groups=None):
        """
        G√©n√®re les indices train/test.
        
        Args:
            X: Features
            y: Target (optionnel)
            groups: Groupes (optionnel)
        
        Yields:
            tuple: (train_indices, test_indices)
        """
        n_samples = len(X)
        
        # Calculer la taille totale n√©cessaire par split
        total_per_split = self.train_length + self.gap + self.test_length
        
        # Point de d√©part pour le premier split
        # On part de la fin et on recule
        for i in range(self.n_splits):
            # Fin du test = n_samples - i * test_length
            test_end = n_samples - i * self.test_length
            test_start = test_end - self.test_length
            
            # Gap
            train_end = test_start - self.gap
            train_start = train_end - self.train_length
            
            if train_start < 0:
                break
            
            train_idx = np.arange(train_start, train_end)
            test_idx = np.arange(test_start, test_end)
            
            yield train_idx, test_idx
    
    def get_n_splits(self, X=None, y=None, groups=None):
        return self.n_splits


class WalkForwardValidator:
    """
    Walk-Forward Validation avec r√©-entra√Ænement.
    
    Le mod√®le est r√©-entra√Æn√© √† chaque √©tape avec les nouvelles donn√©es,
    simulant une utilisation en temps r√©el.
    
    Sch√©ma:
    Step 1: [====Train====]  [Test]
    Step 2:     [====Train====]  [Test]
    Step 3:         [====Train====]  [Test]
    """
    
    def __init__(self, train_window=252, test_window=21, 
                 step_size=21, expanding=False):
        """
        Initialise le validateur.
        
        Args:
            train_window: Taille de la fen√™tre d'entra√Ænement
            test_window: Taille de la fen√™tre de test
            step_size: Pas entre chaque split
            expanding: Si True, la fen√™tre d'entra√Ænement grandit
        """
        self.train_window = train_window
        self.test_window = test_window
        self.step_size = step_size
        self.expanding = expanding
    
    def split(self, X):
        """
        G√©n√®re les splits.
        
        Args:
            X: Features
        
        Yields:
            tuple: (train_indices, test_indices)
        """
        n_samples = len(X)
        
        # Premier point de d√©part
        train_start = 0
        train_end = self.train_window
        
        while train_end + self.test_window <= n_samples:
            test_start = train_end
            test_end = test_start + self.test_window
            
            train_idx = np.arange(train_start, train_end)
            test_idx = np.arange(test_start, test_end)
            
            yield train_idx, test_idx
            
            # Avancer
            if not self.expanding:
                train_start += self.step_size
            train_end += self.step_size


class MultipleTimeSeriesCV:
    """
    Cross-validation pour donn√©es multi-actifs (panel data).
    
    G√®re correctement les donn√©es avec MultiIndex (date, ticker).
    √âvite le look-ahead bias en purgeant les observations qui chevauchent.
    """
    
    def __init__(self, n_splits=3, train_period_length=126,
                 test_period_length=21, lookahead=None,
                 date_idx='date', shuffle=False):
        """
        Initialise le validateur.
        
        Args:
            n_splits: Nombre de splits
            train_period_length: Taille de la p√©riode d'entra√Ænement
            test_period_length: Taille de la p√©riode de test
            lookahead: Horizon de pr√©diction (pour purging)
            date_idx: Nom de l'index de date
            shuffle: M√©langer les donn√©es d'entra√Ænement
        """
        self.n_splits = n_splits
        self.lookahead = lookahead
        self.test_length = test_period_length
        self.train_length = train_period_length
        self.shuffle = shuffle
        self.date_idx = date_idx
    
    def split(self, X, y=None, groups=None):
        """
        G√©n√®re les splits pour donn√©es panel.
        
        Args:
            X: DataFrame avec MultiIndex (date, ticker)
        
        Yields:
            tuple: (train_indices, test_indices)
        """
        unique_dates = X.index.get_level_values(self.date_idx).unique()
        days = sorted(unique_dates, reverse=True)
        
        split_idx = []
        for i in range(self.n_splits):
            test_end_idx = i * self.test_length
            test_start_idx = test_end_idx + self.test_length
            train_end_idx = test_start_idx + (self.lookahead or 0) - 1
            train_start_idx = train_end_idx + self.train_length + (self.lookahead or 0) - 1
            split_idx.append([train_start_idx, train_end_idx,
                             test_start_idx, test_end_idx])
        
        dates = X.reset_index()[[self.date_idx]]
        
        for train_start, train_end, test_start, test_end in split_idx:
            if train_start >= len(days):
                continue
                
            train_idx = dates[(dates[self.date_idx] > days[train_start])
                             & (dates[self.date_idx] <= days[train_end])].index
            test_idx = dates[(dates[self.date_idx] > days[test_start])
                            & (dates[self.date_idx] <= days[test_end])].index
            
            if self.shuffle:
                np.random.shuffle(train_idx.to_numpy())
            
            yield train_idx.to_numpy(), test_idx.to_numpy()
    
    def get_n_splits(self, X=None, y=None, groups=None):
        return self.n_splits


# === Exemple d'utilisation ===
def demonstrate_cv():
    """D√©montre les diff√©rentes m√©thodes de cross-validation."""
    
    # Cr√©er des donn√©es
    n_samples = 500
    X = np.random.randn(n_samples, 10)
    y = np.random.randn(n_samples)
    
    print("TimeSeriesSplit Custom")
    print("-" * 40)
    cv = TimeSeriesSplitCustom(n_splits=3, train_period_length=200, 
                                test_period_length=50, gap=5)
    
    for i, (train_idx, test_idx) in enumerate(cv.split(X)):
        print(f"Split {i+1}:")
        print(f"  Train: {train_idx[0]} - {train_idx[-1]} ({len(train_idx)} samples)")
        print(f"  Test:  {test_idx[0]} - {test_idx[-1]} ({len(test_idx)} samples)")
    
    print("\nWalk-Forward Validation")
    print("-" * 40)
    wf = WalkForwardValidator(train_window=200, test_window=20, step_size=20)
    
    splits = list(wf.split(X))
    print(f"Nombre de splits: {len(splits)}")
    print(f"Premier split - Train: {splits[0][0][0]}-{splits[0][0][-1]}, "
          f"Test: {splits[0][1][0]}-{splits[0][1][-1]}")
    print(f"Dernier split - Train: {splits[-1][0][0]}-{splits[-1][0][-1]}, "
          f"Test: {splits[-1][1][0]}-{splits[-1][1][-1]}")


# demonstrate_cv()
```

## 6.2 Information Mutuelle pour S√©lection de Features

```python
"""
Information Mutuelle pour S√©lection de Features
===============================================
L'information mutuelle (MI) mesure la d√©pendance entre deux variables.
Contrairement √† la corr√©lation, elle capture les relations non-lin√©aires.

MI(X, Y) = 0: X et Y sont ind√©pendants
MI(X, Y) > 0: X contient de l'information sur Y
"""
import numpy as np
import pandas as pd
from sklearn.feature_selection import mutual_info_regression, mutual_info_classif
from sklearn.preprocessing import KBinsDiscretizer

def calculate_mutual_information(X, y, task='regression', n_neighbors=3):
    """
    Calcule l'information mutuelle entre features et target.
    
    Args:
        X: DataFrame de features
        y: Series cible
        task: 'regression' ou 'classification'
        n_neighbors: Nombre de voisins pour l'estimation
    
    Returns:
        Series: MI pour chaque feature, tri√©e d√©croissante
    """
    if task == 'regression':
        mi_scores = mutual_info_regression(X, y, n_neighbors=n_neighbors, random_state=42)
    else:
        mi_scores = mutual_info_classif(X, y, n_neighbors=n_neighbors, random_state=42)
    
    mi_series = pd.Series(mi_scores, index=X.columns)
    return mi_series.sort_values(ascending=False)


def select_features_by_mi(X, y, n_features=10, task='regression'):
    """
    S√©lectionne les meilleures features par information mutuelle.
    
    Args:
        X: DataFrame de features
        y: Series cible
        n_features: Nombre de features √† s√©lectionner
        task: 'regression' ou 'classification'
    
    Returns:
        list: Noms des features s√©lectionn√©es
    """
    mi_scores = calculate_mutual_information(X, y, task)
    return mi_scores.head(n_features).index.tolist()


def plot_mutual_information(mi_scores, top_n=20):
    """
    Affiche les scores d'information mutuelle.
    
    Args:
        mi_scores: Series de MI scores
        top_n: Nombre de features √† afficher
    """
    import matplotlib.pyplot as plt
    
    top_scores = mi_scores.head(top_n)
    
    fig, ax = plt.subplots(figsize=(10, 8))
    top_scores.plot(kind='barh', ax=ax)
    ax.set_xlabel('Mutual Information')
    ax.set_title(f'Top {top_n} Features by Mutual Information')
    ax.invert_yaxis()
    plt.tight_layout()
    plt.show()


# === Exemple ===
def demonstrate_mi():
    """D√©montre la s√©lection de features par MI."""
    
    # Cr√©er des donn√©es synth√©tiques
    np.random.seed(42)
    n = 1000
    
    # Features avec diff√©rentes relations avec la target
    X = pd.DataFrame({
        'linear': np.random.randn(n),           # Relation lin√©aire
        'quadratic': np.random.randn(n),        # Relation quadratique
        'sine': np.random.randn(n),             # Relation sinuso√Ødale
        'noise1': np.random.randn(n),           # Bruit
        'noise2': np.random.randn(n),           # Bruit
    })
    
    # Target avec relations non-lin√©aires
    y = (2 * X['linear'] + 
         X['quadratic']**2 + 
         np.sin(X['sine'] * 3) + 
         np.random.randn(n) * 0.5)
    
    # Calculer MI
    mi_scores = calculate_mutual_information(X, y)
    
    print("Information Mutuelle:")
    print("-" * 30)
    for feature, score in mi_scores.items():
        print(f"  {feature}: {score:.4f}")
    
    return mi_scores


# mi_scores = demonstrate_mi()
```

---

# 7. MOD√àLES LIN√âAIRES
## Linear Models

## 7.1 R√©gression Lin√©aire pour Pr√©diction de Rendements

```python
"""
R√©gression Lin√©aire pour Finance
================================
La r√©gression lin√©aire est le point de d√©part pour la pr√©diction
de rendements. Malgr√© sa simplicit√©, elle reste tr√®s utilis√©e car:
- Interpr√©table
- Rapide
- Base pour les mod√®les plus complexes (Ridge, Lasso)
"""
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_squared_error
import statsmodels.api as sm

class LinearReturnPredictor:
    """
    Pr√©dicteur de rendements bas√© sur r√©gression lin√©aire.
    """
    
    def __init__(self, regularization='none', alpha=1.0, l1_ratio=0.5):
        """
        Initialise le pr√©dicteur.
        
        Args:
            regularization: 'none', 'ridge' (L2), 'lasso' (L1), 'elasticnet'
            alpha: Force de la r√©gularisation
            l1_ratio: Ratio L1 pour ElasticNet (0=Ridge, 1=Lasso)
        """
        self.regularization = regularization
        self.alpha = alpha
        self.l1_ratio = l1_ratio
        
        # Cr√©er le mod√®le
        if regularization == 'ridge':
            self.model = Ridge(alpha=alpha)
        elif regularization == 'lasso':
            self.model = Lasso(alpha=alpha)
        elif regularization == 'elasticnet':
            self.model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio)
        else:
            self.model = LinearRegression()
        
        self.scaler = StandardScaler()
        self.feature_names = None
    
    def fit(self, X, y):
        """
        Entra√Æne le mod√®le.
        
        Args:
            X: Features (DataFrame ou array)
            y: Target (rendements)
        
        Returns:
            self
        """
        if isinstance(X, pd.DataFrame):
            self.feature_names = X.columns.tolist()
        
        # Standardiser les features
        X_scaled = self.scaler.fit_transform(X)
        
        # Entra√Æner
        self.model.fit(X_scaled, y)
        
        return self
    
    def predict(self, X):
        """
        Pr√©dit les rendements.
        
        Args:
            X: Features
        
        Returns:
            array: Pr√©dictions
        """
        X_scaled = self.scaler.transform(X)
        return self.model.predict(X_scaled)
    
    def get_coefficients(self):
        """
        Retourne les coefficients du mod√®le.
        
        Returns:
            Series: Coefficients par feature
        """
        coef = self.model.coef_
        
        if self.feature_names:
            return pd.Series(coef, index=self.feature_names).sort_values(
                key=abs, ascending=False
            )
        return pd.Series(coef)
    
    def evaluate(self, X, y):
        """
        √âvalue le mod√®le.
        
        Args:
            X: Features
            y: Vraies valeurs
        
        Returns:
            dict: M√©triques d'√©valuation
        """
        predictions = self.predict(X)
        
        return {
            'r2': r2_score(y, predictions),
            'mse': mean_squared_error(y, predictions),
            'rmse': np.sqrt(mean_squared_error(y, predictions)),
            'mae': np.mean(np.abs(y - predictions)),
            'ic': np.corrcoef(y, predictions)[0, 1],  # Information Coefficient
            'ic_rank': pd.Series(y).corr(pd.Series(predictions), method='spearman')
        }


def fama_macbeth_regression(data, factor_columns, return_column='forward_return'):
    """
    R√©gression Fama-MacBeth pour panel data.
    
    La r√©gression Fama-MacBeth est standard en finance pour estimer
    les primes de risque des facteurs. Elle se fait en deux √©tapes:
    
    1. Cross-sectional: Pour chaque date, r√©gresser les rendements
       sur les facteurs pour obtenir les primes de risque
    2. Time-series: Calculer la moyenne et l'√©cart-type des primes
    
    Args:
        data: DataFrame avec MultiIndex (date, ticker)
        factor_columns: Liste des colonnes de facteurs
        return_column: Nom de la colonne de rendement
    
    Returns:
        DataFrame: Primes de risque avec t-stats
    """
    # Grouper par date
    dates = data.index.get_level_values('date').unique()
    
    # Stocker les primes de risque
    risk_premia = []
    
    for date in dates:
        # Donn√©es de cette date
        cross_section = data.loc[date]
        
        if len(cross_section) < len(factor_columns) + 5:  # Minimum d'observations
            continue
        
        # R√©gression cross-sectionnelle
        X = cross_section[factor_columns]
        y = cross_section[return_column]
        
        # Ajouter constante
        X_const = sm.add_constant(X)
        
        try:
            model = sm.OLS(y, X_const).fit()
            risk_premia.append(model.params)
        except:
            continue
    
    # Convertir en DataFrame
    risk_premia_df = pd.DataFrame(risk_premia)
    
    # Statistiques
    results = pd.DataFrame({
        'mean': risk_premia_df.mean(),
        'std': risk_premia_df.std(),
        't_stat': risk_premia_df.mean() / (risk_premia_df.std() / np.sqrt(len(risk_premia_df))),
        'p_value': 2 * (1 - stats.t.cdf(
            abs(risk_premia_df.mean() / (risk_premia_df.std() / np.sqrt(len(risk_premia_df)))),
            df=len(risk_premia_df) - 1
        ))
    })
    
    return results


# === Exemple ===
def demonstrate_linear_models():
    """D√©montre les mod√®les lin√©aires."""
    
    # Cr√©er des donn√©es
    np.random.seed(42)
    n = 1000
    
    # Features
    X = pd.DataFrame({
        'momentum': np.random.randn(n),
        'value': np.random.randn(n),
        'size': np.random.randn(n),
        'volatility': np.random.randn(n),
        'quality': np.random.randn(n),
    })
    
    # Target: combinaison lin√©aire + bruit
    y = (0.05 * X['momentum'] + 
         0.03 * X['value'] + 
         -0.02 * X['size'] + 
         0.01 * X['quality'] +
         np.random.randn(n) * 0.1)
    
    # Split train/test
    train_size = int(0.8 * n)
    X_train, X_test = X[:train_size], X[train_size:]
    y_train, y_test = y[:train_size], y[train_size:]
    
    # Comparer les mod√®les
    models = {
        'OLS': LinearReturnPredictor(regularization='none'),
        'Ridge': LinearReturnPredictor(regularization='ridge', alpha=1.0),
        'Lasso': LinearReturnPredictor(regularization='lasso', alpha=0.01),
        'ElasticNet': LinearReturnPredictor(regularization='elasticnet', alpha=0.1),
    }
    
    print("Comparaison des mod√®les lin√©aires")
    print("=" * 60)
    
    for name, model in models.items():
        model.fit(X_train, y_train)
        metrics = model.evaluate(X_test, y_test)
        
        print(f"\n{name}:")
        print(f"  R¬≤: {metrics['r2']:.4f}")
        print(f"  RMSE: {metrics['rmse']:.4f}")
        print(f"  IC: {metrics['ic']:.4f}")
        print(f"  IC (rank): {metrics['ic_rank']:.4f}")
        
        print(f"  Coefficients:")
        for feat, coef in model.get_coefficients().items():
            print(f"    {feat}: {coef:.4f}")


# demonstrate_linear_models()
```

## 7.2 R√©gression Logistique pour Classification

```python
"""
R√©gression Logistique pour Classification de Mouvements de Prix
===============================================================
La r√©gression logistique pr√©dit la probabilit√© d'un √©v√©nement binaire:
- Le prix va-t-il monter ou descendre?
- Y aura-t-il un mouvement significatif?
"""
import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (accuracy_score, precision_score, recall_score,
                            f1_score, roc_auc_score, confusion_matrix,
                            classification_report)

class PriceMovementClassifier:
    """
    Classifieur de mouvements de prix.
    """
    
    def __init__(self, threshold=0, regularization='l2', C=1.0):
        """
        Initialise le classifieur.
        
        Args:
            threshold: Seuil pour d√©finir up/down (0 = simple sign)
            regularization: 'l1', 'l2', ou 'elasticnet'
            C: Inverse de la force de r√©gularisation
        """
        self.threshold = threshold
        
        self.model = LogisticRegression(
            penalty=regularization,
            C=C,
            solver='saga' if regularization == 'elasticnet' else 'lbfgs',
            max_iter=1000,
            random_state=42
        )
        
        self.scaler = StandardScaler()
        self.feature_names = None
    
    def _create_labels(self, returns):
        """
        Cr√©e les labels binaires √† partir des rendements.
        
        Args:
            returns: Series de rendements
        
        Returns:
            array: Labels (1 = up, 0 = down)
        """
        return (returns > self.threshold).astype(int)
    
    def fit(self, X, returns):
        """
        Entra√Æne le classifieur.
        
        Args:
            X: Features
            returns: Rendements (seront convertis en labels)
        """
        if isinstance(X, pd.DataFrame):
            self.feature_names = X.columns.tolist()
        
        y = self._create_labels(returns)
        X_scaled = self.scaler.fit_transform(X)
        
        self.model.fit(X_scaled, y)
        
        return self
    
    def predict(self, X):
        """
        Pr√©dit les labels.
        
        Args:
            X: Features
        
        Returns:
            array: Labels pr√©dits (0 ou 1)
        """
        X_scaled = self.scaler.transform(X)
        return self.model.predict(X_scaled)
    
    def predict_proba(self, X):
        """
        Pr√©dit les probabilit√©s.
        
        Args:
            X: Features
        
        Returns:
            array: Probabilit√©s de chaque classe
        """
        X_scaled = self.scaler.transform(X)
        return self.model.predict_proba(X_scaled)
    
    def evaluate(self, X, returns):
        """
        √âvalue le classifieur.
        
        Args:
            X: Features
            returns: Vrais rendements
        
        Returns:
            dict: M√©triques d'√©valuation
        """
        y_true = self._create_labels(returns)
        y_pred = self.predict(X)
        y_proba = self.predict_proba(X)[:, 1]
        
        return {
            'accuracy': accuracy_score(y_true, y_pred),
            'precision': precision_score(y_true, y_pred),
            'recall': recall_score(y_true, y_pred),
            'f1': f1_score(y_true, y_pred),
            'roc_auc': roc_auc_score(y_true, y_proba),
            'confusion_matrix': confusion_matrix(y_true, y_pred)
        }
    
    def get_feature_importance(self):
        """
        Retourne l'importance des features.
        
        Returns:
            Series: Importance (coefficients) par feature
        """
        importance = self.model.coef_[0]
        
        if self.feature_names:
            return pd.Series(importance, index=self.feature_names).sort_values(
                key=abs, ascending=False
            )
        return pd.Series(importance)


# === Exemple ===
def demonstrate_logistic_regression():
    """D√©montre la r√©gression logistique."""
    
    np.random.seed(42)
    n = 1000
    
    # Features
    X = pd.DataFrame({
        'momentum': np.random.randn(n),
        'rsi': np.random.randn(n),
        'macd': np.random.randn(n),
        'volume_ratio': np.random.randn(n),
    })
    
    # Rendements (l√©g√®rement pr√©visibles)
    returns = (0.3 * X['momentum'] + 
               0.2 * X['rsi'] + 
               0.1 * X['macd'] +
               np.random.randn(n) * 0.5)
    
    # Split
    train_size = int(0.8 * n)
    X_train, X_test = X[:train_size], X[train_size:]
    ret_train, ret_test = returns[:train_size], returns[train_size:]
    
    # Entra√Æner
    clf = PriceMovementClassifier(threshold=0, C=0.1)
    clf.fit(X_train, ret_train)
    
    # √âvaluer
    metrics = clf.evaluate(X_test, ret_test)
    
    print("R√©sultats du classifieur")
    print("=" * 40)
    print(f"Accuracy: {metrics['accuracy']:.2%}")
    print(f"Precision: {metrics['precision']:.2%}")
    print(f"Recall: {metrics['recall']:.2%}")
    print(f"F1 Score: {metrics['f1']:.2%}")
    print(f"ROC AUC: {metrics['roc_auc']:.4f}")
    
    print("\nImportance des features:")
    for feat, imp in clf.get_feature_importance().items():
        print(f"  {feat}: {imp:.4f}")


# demonstrate_logistic_regression()
```

---

# 8. WORKFLOW ML4T COMPLET
## Complete ML4T Workflow

## 8.1 Deflated Sharpe Ratio

```python
"""
Deflated Sharpe Ratio
=====================
Le Deflated Sharpe Ratio (DSR) corrige le Sharpe Ratio pour tenir compte
du multiple testing (test de nombreuses strat√©gies).

Quand on teste N strat√©gies, la meilleure aura un Sharpe √©lev√© par chance
m√™me si aucune n'est vraiment profitable.

Le DSR estime la probabilit√© que le Sharpe observ√© soit d√ª au hasard.
"""
import numpy as np
from scipy import stats

def deflated_sharpe_ratio(sharpe_observed, n_trials, variance_sharpe=1,
                          skewness_returns=0, kurtosis_returns=3,
                          n_observations=252):
    """
    Calcule le Deflated Sharpe Ratio.
    
    Args:
        sharpe_observed: Sharpe ratio observ√© de la meilleure strat√©gie
        n_trials: Nombre de strat√©gies test√©es
        variance_sharpe: Variance des Sharpe ratios (g√©n√©ralement 1)
        skewness_returns: Skewness des rendements (0 = normal)
        kurtosis_returns: Kurtosis des rendements (3 = normal)
        n_observations: Nombre d'observations
    
    Returns:
        float: Probabilit√© que le Sharpe soit d√ª au skill (pas au hasard)
    """
    # Sharpe ratio attendu de la meilleure strat√©gie sous l'hypoth√®se nulle
    # (toutes les strat√©gies ont un vrai Sharpe de 0)
    expected_max_sharpe = (
        (1 - np.euler_gamma) * stats.norm.ppf(1 - 1/n_trials) +
        np.euler_gamma * stats.norm.ppf(1 - 1/(n_trials * np.e))
    ) * np.sqrt(variance_sharpe)
    
    # Variance du Sharpe ratio estim√©
    var_sharpe = (
        (1 + 0.25 * sharpe_observed**2 * (kurtosis_returns - 1) -
         sharpe_observed * skewness_returns) / n_observations
    )
    
    # Test statistique
    z_stat = (sharpe_observed - expected_max_sharpe) / np.sqrt(var_sharpe)
    
    # Probabilit√© (p-value one-sided)
    prob_skill = stats.norm.cdf(z_stat)
    
    return prob_skill


def minimum_track_record_length(sharpe_target, sharpe_benchmark=0,
                                skewness=0, kurtosis=3, alpha=0.05):
    """
    Calcule la dur√©e minimale de track record n√©cessaire.
    
    Combien d'observations faut-il pour √™tre confiant que le Sharpe
    observ√© n'est pas d√ª au hasard?
    
    Args:
        sharpe_target: Sharpe ratio cible
        sharpe_benchmark: Sharpe ratio du benchmark (g√©n√©ralement 0)
        skewness: Skewness des rendements
        kurtosis: Kurtosis des rendements
        alpha: Niveau de significativit√©
    
    Returns:
        int: Nombre minimum d'observations requises
    """
    z_alpha = stats.norm.ppf(1 - alpha)
    
    # Formule de Bailey et Lopez de Prado
    min_length = (
        (z_alpha / (sharpe_target - sharpe_benchmark))**2 *
        (1 + 0.25 * sharpe_target**2 * (kurtosis - 1) - sharpe_target * skewness)
    )
    
    return int(np.ceil(min_length))


# === Exemple ===
def demonstrate_dsr():
    """D√©montre le Deflated Sharpe Ratio."""
    
    print("Deflated Sharpe Ratio")
    print("=" * 50)
    
    # Sc√©nario: On a test√© 100 strat√©gies
    # La meilleure a un Sharpe de 2.0
    sharpe = 2.0
    n_trials = 100
    
    dsr = deflated_sharpe_ratio(sharpe, n_trials)
    
    print(f"\nSharpe observ√©: {sharpe}")
    print(f"Nombre de strat√©gies test√©es: {n_trials}")
    print(f"Probabilit√© de skill (DSR): {dsr:.2%}")
    
    if dsr > 0.95:
        print("‚Üí Forte probabilit√© que ce soit du skill")
    elif dsr > 0.50:
        print("‚Üí R√©sultat incertain, prudence recommand√©e")
    else:
        print("‚Üí Probablement d√ª au hasard (data mining)")
    
    # Minimum track record
    print("\n" + "-" * 50)
    print("Track record minimum pour diff√©rents Sharpe cibles:")
    
    for target_sharpe in [0.5, 1.0, 1.5, 2.0, 2.5]:
        min_obs = minimum_track_record_length(target_sharpe)
        years = min_obs / 252
        print(f"  Sharpe {target_sharpe}: {min_obs} jours ({years:.1f} ann√©es)")


# demonstrate_dsr()
```

## 8.2 Backtrader - Framework de Backtesting

```python
"""
Backtrader - Framework de Backtesting Professionnel
===================================================
Backtrader est un framework Python complet pour le backtesting
et le trading algorithmique.

Avantages:
- Event-driven (simulation r√©aliste)
- Support multi-timeframe
- Gestion des ordres complexes
- Int√©gration avec brokers (IB, Oanda)
"""
import backtrader as bt
import pandas as pd
import numpy as np

class MovingAverageCrossStrategy(bt.Strategy):
    """
    Strat√©gie de croisement de moyennes mobiles.
    
    Long quand MA rapide > MA lente
    Flat quand MA rapide < MA lente
    """
    
    params = (
        ('fast_period', 10),
        ('slow_period', 30),
        ('printlog', True),
    )
    
    def __init__(self):
        """Initialise les indicateurs."""
        # Moyennes mobiles
        self.fast_ma = bt.indicators.SMA(
            self.data.close, period=self.params.fast_period
        )
        self.slow_ma = bt.indicators.SMA(
            self.data.close, period=self.params.slow_period
        )
        
        # Signal de crossover
        self.crossover = bt.indicators.CrossOver(self.fast_ma, self.slow_ma)
        
        # Pour le logging
        self.order = None
    
    def next(self):
        """
        Appel√© √† chaque nouvelle barre.
        
        Logique de trading:
        - Si pas en position et crossover up ‚Üí acheter
        - Si en position et crossover down ‚Üí vendre
        """
        # V√©rifier si un ordre est en attente
        if self.order:
            return
        
        # V√©rifier si on est en position
        if not self.position:
            # Pas en position
            if self.crossover > 0:  # MA rapide croise au-dessus
                self.order = self.buy()
                self.log(f'BUY CREATE, {self.data.close[0]:.2f}')
        else:
            # En position
            if self.crossover < 0:  # MA rapide croise en-dessous
                self.order = self.sell()
                self.log(f'SELL CREATE, {self.data.close[0]:.2f}')
    
    def notify_order(self, order):
        """Appel√© quand un ordre change de statut."""
        if order.status in [order.Submitted, order.Accepted]:
            return
        
        if order.status in [order.Completed]:
            if order.isbuy():
                self.log(f'BUY EXECUTED, Price: {order.executed.price:.2f}')
            else:
                self.log(f'SELL EXECUTED, Price: {order.executed.price:.2f}')
        
        elif order.status in [order.Canceled, order.Margin, order.Rejected]:
            self.log('Order Canceled/Margin/Rejected')
        
        self.order = None
    
    def log(self, txt, dt=None):
        """Logging helper."""
        if self.params.printlog:
            dt = dt or self.datas[0].datetime.date(0)
            print(f'{dt.isoformat()} {txt}')


class MomentumStrategy(bt.Strategy):
    """
    Strat√©gie momentum bas√©e sur le RSI (Relative Strength Index).
    
    - Ach√®te quand RSI sort de la zone de survente (< 30)
    - Vend quand RSI entre dans la zone de surachat (> 70)
    """
    
    params = (
        ('rsi_period', 14),
        ('oversold', 30),
        ('overbought', 70),
        ('stake', 100),  # Nombre d'actions par trade
    )
    
    def __init__(self):
        """Initialise les indicateurs."""
        self.rsi = bt.indicators.RSI(
            self.data.close, period=self.params.rsi_period
        )
        self.order = None
    
    def next(self):
        """Logique de trading."""
        if self.order:
            return
        
        if not self.position:
            # Pas en position - chercher signal d'achat
            if self.rsi < self.params.oversold:
                self.order = self.buy(size=self.params.stake)
        else:
            # En position - chercher signal de vente
            if self.rsi > self.params.overbought:
                self.order = self.sell(size=self.params.stake)
    
    def notify_order(self, order):
        if order.status in [order.Completed]:
            self.order = None


def run_backtrader_backtest(data, strategy_class, strategy_params=None,
                            initial_cash=100000, commission=0.001):
    """
    Ex√©cute un backtest avec Backtrader.
    
    Args:
        data: DataFrame avec OHLCV (index = dates)
        strategy_class: Classe de strat√©gie Backtrader
        strategy_params: Param√®tres de la strat√©gie
        initial_cash: Capital initial
        commission: Commission par transaction
    
    Returns:
        dict: R√©sultats du backtest
    """
    # Cr√©er l'instance Cerebro
    cerebro = bt.Cerebro()
    
    # Ajouter la strat√©gie
    if strategy_params:
        cerebro.addstrategy(strategy_class, **strategy_params)
    else:
        cerebro.addstrategy(strategy_class)
    
    # Convertir les donn√©es en format Backtrader
    data_feed = bt.feeds.PandasData(
        dataname=data,
        datetime=None,  # Utiliser l'index
        open='Open',
        high='High',
        low='Low',
        close='Close',
        volume='Volume',
        openinterest=-1
    )
    
    cerebro.adddata(data_feed)
    
    # Configuration
    cerebro.broker.setcash(initial_cash)
    cerebro.broker.setcommission(commission=commission)
    
    # Ajouter les analyseurs
    cerebro.addanalyzer(bt.analyzers.SharpeRatio, _name='sharpe')
    cerebro.addanalyzer(bt.analyzers.DrawDown, _name='drawdown')
    cerebro.addanalyzer(bt.analyzers.Returns, _name='returns')
    cerebro.addanalyzer(bt.analyzers.TradeAnalyzer, _name='trades')
    
    # Ex√©cuter
    print(f'Starting Portfolio Value: ${initial_cash:,.2f}')
    results = cerebro.run()
    strat = results[0]
    
    final_value = cerebro.broker.getvalue()
    print(f'Final Portfolio Value: ${final_value:,.2f}')
    
    # Extraire les r√©sultats
    sharpe = strat.analyzers.sharpe.get_analysis()
    drawdown = strat.analyzers.drawdown.get_analysis()
    returns = strat.analyzers.returns.get_analysis()
    trades = strat.analyzers.trades.get_analysis()
    
    results_dict = {
        'initial_value': initial_cash,
        'final_value': final_value,
        'total_return': (final_value - initial_cash) / initial_cash,
        'sharpe_ratio': sharpe.get('sharperatio', None),
        'max_drawdown': drawdown.get('max', {}).get('drawdown', None),
        'total_trades': trades.get('total', {}).get('total', 0),
    }
    
    return results_dict, cerebro


# === Exemple d'utilisation ===
def backtrader_example():
    """Exemple de backtest avec Backtrader."""
    import yfinance as yf
    
    # T√©l√©charger les donn√©es
    data = yf.download('AAPL', start='2020-01-01', end='2023-12-31')
    data.columns = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']
    
    # Ex√©cuter le backtest
    results, cerebro = run_backtrader_backtest(
        data,
        MovingAverageCrossStrategy,
        strategy_params={'fast_period': 10, 'slow_period': 30, 'printlog': False}
    )
    
    print("\nR√©sultats du backtest:")
    print("-" * 40)
    print(f"Rendement total: {results['total_return']:.2%}")
    print(f"Sharpe Ratio: {results['sharpe_ratio']:.2f}" if results['sharpe_ratio'] else "N/A")
    print(f"Max Drawdown: {results['max_drawdown']:.2%}" if results['max_drawdown'] else "N/A")
    print(f"Nombre de trades: {results['total_trades']}")
    
    # Afficher le graphique
    cerebro.plot(style='candlestick')
    
    return results


# Pour ex√©cuter:
# results = backtrader_example()
```

---

# 9. MOD√àLES DE S√âRIES TEMPORELLES
## Time Series Models

## 9.1 Stationnarit√© et Tests

```python
"""
Stationnarit√© des S√©ries Temporelles
====================================
Une s√©rie temporelle est stationnaire si ses propri√©t√©s statistiques
(moyenne, variance, autocorr√©lation) ne changent pas dans le temps.

C'est important car la plupart des mod√®les (ARIMA, etc.) supposent
la stationnarit√©.

Types de non-stationnarit√©:
1. Tendance (trend): La moyenne change
2. Saisonnalit√©: Patterns p√©riodiques
3. H√©t√©rosc√©dasticit√©: La variance change (GARCH)
"""
import numpy as np
import pandas as pd
from statsmodels.tsa.stattools import adfuller, kpss, acf, pacf
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
import matplotlib.pyplot as plt

def adf_test(series, significance_level=0.05, verbose=True):
    """
    Test de Dickey-Fuller Augment√© (ADF) pour la stationnarit√©.
    
    H0: La s√©rie a une racine unitaire (non-stationnaire)
    H1: La s√©rie est stationnaire
    
    Args:
        series: Series temporelle
        significance_level: Niveau de significativit√©
        verbose: Afficher les r√©sultats
    
    Returns:
        dict: R√©sultats du test
    """
    result = adfuller(series.dropna(), autolag='AIC')
    
    output = {
        'test_statistic': result[0],
        'p_value': result[1],
        'lags_used': result[2],
        'n_observations': result[3],
        'critical_values': result[4],
        'is_stationary': result[1] < significance_level
    }
    
    if verbose:
        print("Test ADF (Augmented Dickey-Fuller)")
        print("=" * 50)
        print(f"Test Statistic: {output['test_statistic']:.4f}")
        print(f"p-value: {output['p_value']:.4f}")
        print(f"Lags Used: {output['lags_used']}")
        print(f"Number of Observations: {output['n_observations']}")
        print("Critical Values:")
        for key, value in output['critical_values'].items():
            print(f"  {key}: {value:.4f}")
        
        if output['is_stationary']:
            print(f"\n‚úì La s√©rie est STATIONNAIRE (p-value < {significance_level})")
        else:
            print(f"\n‚úó La s√©rie est NON-STATIONNAIRE (p-value >= {significance_level})")
    
    return output


def kpss_test(series, regression='c', significance_level=0.05, verbose=True):
    """
    Test KPSS pour la stationnarit√©.
    
    Contrairement √† ADF, KPSS teste:
    H0: La s√©rie est stationnaire
    H1: La s√©rie a une racine unitaire
    
    Args:
        series: Series temporelle
        regression: 'c' (constant) ou 'ct' (constant + trend)
        significance_level: Niveau de significativit√©
        verbose: Afficher les r√©sultats
    
    Returns:
        dict: R√©sultats du test
    """
    result = kpss(series.dropna(), regression=regression)
    
    output = {
        'test_statistic': result[0],
        'p_value': result[1],
        'lags_used': result[2],
        'critical_values': result[3],
        'is_stationary': result[1] > significance_level
    }
    
    if verbose:
        print("\nTest KPSS")
        print("=" * 50)
        print(f"Test Statistic: {output['test_statistic']:.4f}")
        print(f"p-value: {output['p_value']:.4f}")
        print(f"Lags Used: {output['lags_used']}")
        print("Critical Values:")
        for key, value in output['critical_values'].items():
            print(f"  {key}: {value:.4f}")
        
        if output['is_stationary']:
            print(f"\n‚úì La s√©rie est STATIONNAIRE (p-value > {significance_level})")
        else:
            print(f"\n‚úó La s√©rie est NON-STATIONNAIRE (p-value <= {significance_level})")
    
    return output


def make_stationary(series, method='diff', order=1):
    """
    Transforme une s√©rie en s√©rie stationnaire.
    
    Args:
        series: Series non-stationnaire
        method: 'diff' (diff√©renciation), 'log_diff' (rendements log),
                'pct_change' (rendements)
        order: Ordre de diff√©renciation
    
    Returns:
        Series: S√©rie transform√©e
    """
    if method == 'diff':
        return series.diff(order).dropna()
    elif method == 'log_diff':
        return np.log(series).diff(order).dropna()
    elif method == 'pct_change':
        return series.pct_change(order).dropna()
    else:
        raise ValueError(f"M√©thode inconnue: {method}")


def plot_stationarity_diagnostics(series, lags=40, title=''):
    """
    Affiche les diagnostics de stationnarit√©.
    
    Args:
        series: Series temporelle
        lags: Nombre de lags pour ACF/PACF
        title: Titre du graphique
    """
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    # 1. S√©rie originale
    axes[0, 0].plot(series)
    axes[0, 0].set_title(f'{title} - Time Series')
    
    # 2. Distribution
    series.hist(ax=axes[0, 1], bins=50, density=True)
    axes[0, 1].set_title('Distribution')
    
    # 3. ACF (AutoCorrelation Function)
    plot_acf(series.dropna(), ax=axes[1, 0], lags=lags)
    axes[1, 0].set_title('Autocorrelation (ACF)')
    
    # 4. PACF (Partial ACF)
    plot_pacf(series.dropna(), ax=axes[1, 1], lags=lags)
    axes[1, 1].set_title('Partial Autocorrelation (PACF)')
    
    plt.tight_layout()
    plt.show()


# === Exemple ===
def demonstrate_stationarity():
    """D√©montre les tests de stationnarit√©."""
    import yfinance as yf
    
    # T√©l√©charger des donn√©es
    data = yf.download('SPY', start='2020-01-01', end='2023-12-31')
    prices = data['Adj Close']
    returns = prices.pct_change().dropna()
    
    print("PRIX (non-stationnaire)")
    print("=" * 60)
    adf_test(prices)
    kpss_test(prices)
    
    print("\n\nRENDEMENTS (stationnaire)")
    print("=" * 60)
    adf_test(returns)
    kpss_test(returns)


# demonstrate_stationarity()
```

## 9.2 Mod√®les ARIMA

```python
"""
Mod√®les ARIMA (AutoRegressive Integrated Moving Average)
========================================================
ARIMA(p, d, q) combine trois composantes:
- AR(p): AutoR√©gressive - utilise les p observations pass√©es
- I(d): Int√©gration - d diff√©renciations pour rendre stationnaire
- MA(q): Moving Average - utilise les q erreurs pass√©es

Formule:
y'(t) = c + œÜ‚ÇÅy'(t-1) + ... + œÜ‚Çöy'(t-p) + Œ∏‚ÇÅŒµ(t-1) + ... + Œ∏qŒµ(t-q) + Œµ(t)

o√π y'(t) est la s√©rie diff√©renci√©e d fois.
"""
import numpy as np
import pandas as pd
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.statespace.sarimax import SARIMAX
from statsmodels.tsa.stattools import acf, pacf
import warnings
warnings.filterwarnings('ignore')

def select_arima_order(series, max_p=5, max_d=2, max_q=5, criterion='aic'):
    """
    S√©lectionne automatiquement les ordres ARIMA optimaux.
    
    Args:
        series: Series temporelle
        max_p: Maximum pour p
        max_d: Maximum pour d
        max_q: Maximum pour q
        criterion: 'aic' ou 'bic'
    
    Returns:
        tuple: (p, d, q) optimaux
    """
    best_score = np.inf
    best_order = (0, 0, 0)
    
    for p in range(max_p + 1):
        for d in range(max_d + 1):
            for q in range(max_q + 1):
                try:
                    model = ARIMA(series, order=(p, d, q))
                    results = model.fit()
                    
                    score = results.aic if criterion == 'aic' else results.bic
                    
                    if score < best_score:
                        best_score = score
                        best_order = (p, d, q)
                except:
                    continue
    
    return best_order


class ARIMAForecaster:
    """
    Forecaster bas√© sur ARIMA.
    """
    
    def __init__(self, order=None, auto_select=True, seasonal_order=None):
        """
        Initialise le forecaster.
        
        Args:
            order: (p, d, q) ou None pour auto-s√©lection
            auto_select: S√©lectionner automatiquement l'ordre
            seasonal_order: (P, D, Q, S) pour SARIMA
        """
        self.order = order
        self.auto_select = auto_select
        self.seasonal_order = seasonal_order
        self.model = None
        self.results = None
    
    def fit(self, series):
        """
        Entra√Æne le mod√®le.
        
        Args:
            series: Series temporelle
        
        Returns:
            self
        """
        if self.auto_select and self.order is None:
            self.order = select_arima_order(series)
            print(f"Ordre s√©lectionn√©: ARIMA{self.order}")
        
        if self.seasonal_order:
            self.model = SARIMAX(
                series, 
                order=self.order, 
                seasonal_order=self.seasonal_order
            )
        else:
            self.model = ARIMA(series, order=self.order)
        
        self.results = self.model.fit()
        
        return self
    
    def predict(self, steps=1):
        """
        Pr√©dit les valeurs futures.
        
        Args:
            steps: Nombre de pas √† pr√©dire
        
        Returns:
            Series: Pr√©dictions
        """
        forecast = self.results.forecast(steps=steps)
        return forecast
    
    def get_summary(self):
        """
        Retourne le r√©sum√© du mod√®le.
        
        Returns:
            str: R√©sum√© statistique
        """
        return self.results.summary()
    
    def diagnostic_plots(self):
        """Affiche les diagnostics du mod√®le."""
        self.results.plot_diagnostics(figsize=(14, 10))
        plt.tight_layout()
        plt.show()


# === Exemple ===
def demonstrate_arima():
    """D√©montre les mod√®les ARIMA."""
    import yfinance as yf
    
    # Donn√©es
    data = yf.download('SPY', start='2020-01-01', end='2023-12-31')
    returns = data['Adj Close'].pct_change().dropna() * 100  # En pourcentage
    
    # Split train/test
    train = returns[:-30]
    test = returns[-30:]
    
    # Cr√©er et entra√Æner le mod√®le
    forecaster = ARIMAForecaster(auto_select=True)
    forecaster.fit(train)
    
    print(forecaster.get_summary())
    
    # Pr√©dictions
    predictions = forecaster.predict(steps=30)
    
    # √âvaluer
    mae = np.mean(np.abs(test.values - predictions.values))
    rmse = np.sqrt(np.mean((test.values - predictions.values)**2))
    
    print(f"\nM√©triques de pr√©diction:")
    print(f"  MAE: {mae:.4f}")
    print(f"  RMSE: {rmse:.4f}")
    
    return forecaster


# forecaster = demonstrate_arima()
```

## 9.3 Mod√®les GARCH pour la Volatilit√©

```python
"""
Mod√®les GARCH (Generalized AutoRegressive Conditional Heteroskedasticity)
=========================================================================
Les mod√®les GARCH mod√©lisent la volatilit√© qui varie dans le temps.

GARCH(p, q):
œÉ¬≤(t) = œâ + Œ£Œ±·µ¢Œµ¬≤(t-i) + Œ£Œ≤‚±ºœÉ¬≤(t-j)

o√π:
- œÉ¬≤(t): Variance conditionnelle
- Œµ(t): R√©sidus
- Œ±: Impact des chocs pass√©s (ARCH)
- Œ≤: Persistance de la volatilit√© (GARCH)

Applications:
- Pr√©vision de volatilit√©
- Value at Risk (VaR)
- Option pricing
"""
import numpy as np
import pandas as pd
from arch import arch_model
import matplotlib.pyplot as plt

class GARCHVolatilityModel:
    """
    Mod√®le GARCH pour la pr√©vision de volatilit√©.
    """
    
    def __init__(self, p=1, q=1, vol='Garch', dist='normal'):
        """
        Initialise le mod√®le.
        
        Args:
            p: Ordre GARCH (persistance)
            q: Ordre ARCH (chocs)
            vol: Type de volatilit√© ('Garch', 'EGarch', 'GJR-GARCH')
            dist: Distribution des erreurs ('normal', 't', 'skewt')
        """
        self.p = p
        self.q = q
        self.vol = vol
        self.dist = dist
        self.model = None
        self.results = None
    
    def fit(self, returns, rescale=True):
        """
        Entra√Æne le mod√®le.
        
        Args:
            returns: Series de rendements (en pourcentage recommand√©)
            rescale: Multiplier par 100 si n√©cessaire
        
        Returns:
            self
        """
        # Rescale si les rendements sont petits
        if rescale and returns.std() < 0.1:
            returns = returns * 100
        
        self.model = arch_model(
            returns,
            vol=self.vol,
            p=self.p,
            q=self.q,
            dist=self.dist
        )
        
        self.results = self.model.fit(disp='off')
        
        return self
    
    def forecast_volatility(self, horizon=1, method='analytic'):
        """
        Pr√©voit la volatilit√© future.
        
        Args:
            horizon: Nombre de p√©riodes
            method: 'analytic', 'simulation', ou 'bootstrap'
        
        Returns:
            DataFrame: Pr√©visions de volatilit√©
        """
        forecast = self.results.forecast(horizon=horizon, method=method)
        
        # Retourner l'√©cart-type (pas la variance)
        return np.sqrt(forecast.variance)
    
    def conditional_volatility(self):
        """
        Retourne la volatilit√© conditionnelle historique.
        
        Returns:
            Series: Volatilit√© conditionnelle
        """
        return np.sqrt(self.results.conditional_volatility)
    
    def get_summary(self):
        """Retourne le r√©sum√© du mod√®le."""
        return self.results.summary()
    
    def calculate_var(self, confidence_level=0.05, horizon=1):
        """
        Calcule la Value at Risk (VaR).
        
        Args:
            confidence_level: Niveau de confiance (0.05 = 95%)
            horizon: Horizon en jours
        
        Returns:
            float: VaR
        """
        from scipy import stats
        
        # Pr√©vision de volatilit√©
        vol_forecast = self.forecast_volatility(horizon=horizon)
        vol = vol_forecast.values[-1, 0]
        
        # VaR param√©trique
        z = stats.norm.ppf(confidence_level)
        var = z * vol * np.sqrt(horizon)
        
        return var
    
    def plot_volatility(self, figsize=(14, 8)):
        """Affiche la volatilit√© conditionnelle."""
        fig, axes = plt.subplots(2, 1, figsize=figsize)
        
        # 1. Rendements
        ax1 = axes[0]
        self.results.resid.plot(ax=ax1, alpha=0.7)
        ax1.set_title('Returns')
        
        # 2. Volatilit√© conditionnelle
        ax2 = axes[1]
        vol = self.conditional_volatility()
        vol.plot(ax=ax2, color='red')
        ax2.set_title('Conditional Volatility (GARCH)')
        ax2.fill_between(vol.index, 0, vol, alpha=0.3, color='red')
        
        plt.tight_layout()
        plt.show()


# === Exemple ===
def demonstrate_garch():
    """D√©montre les mod√®les GARCH."""
    import yfinance as yf
    
    # Donn√©es
    data = yf.download('SPY', start='2015-01-01', end='2023-12-31')
    returns = data['Adj Close'].pct_change().dropna() * 100
    
    # Cr√©er et entra√Æner le mod√®le
    garch = GARCHVolatilityModel(p=1, q=1, vol='Garch', dist='t')
    garch.fit(returns, rescale=False)
    
    print(garch.get_summary())
    
    # Pr√©vision
    vol_forecast = garch.forecast_volatility(horizon=5)
    print(f"\nPr√©vision de volatilit√© (5 jours):")
    print(vol_forecast)
    
    # VaR
    var_95 = garch.calculate_var(confidence_level=0.05)
    print(f"\nVaR 95% (1 jour): {var_95:.2f}%")
    
    # Plot
    garch.plot_volatility()
    
    return garch


# garch = demonstrate_garch()
```

## 9.4 Cointegration et Pairs Trading

```python
"""
Cointegration et Pairs Trading
==============================
Deux s√©ries sont coint√©gr√©es si leur combinaison lin√©aire est stationnaire,
m√™me si chaque s√©rie ne l'est pas individuellement.

Exemple: Prix de Coca-Cola et Pepsi
- Chaque prix peut √™tre non-stationnaire (tendance)
- Mais le spread (Coca - Œ≤*Pepsi) peut √™tre stationnaire

C'est la base du pairs trading:
1. Trouver des paires coint√©gr√©es
2. Calculer le spread
3. Trader quand le spread s'√©carte de sa moyenne
"""
import numpy as np
import pandas as pd
from statsmodels.tsa.stattools import coint, adfuller
from statsmodels.regression.linear_model import OLS
import statsmodels.api as sm

def test_cointegration(series1, series2, significance_level=0.05):
    """
    Teste la cointegration entre deux s√©ries.
    
    Utilise le test d'Engle-Granger:
    1. R√©gresse series1 sur series2
    2. Teste si les r√©sidus sont stationnaires
    
    Args:
        series1: Premi√®re s√©rie
        series2: Deuxi√®me s√©rie
        significance_level: Niveau de significativit√©
    
    Returns:
        dict: R√©sultats du test
    """
    # Test de cointegration
    score, pvalue, _ = coint(series1, series2)
    
    # R√©gression pour obtenir le coefficient
    series2_const = sm.add_constant(series2)
    model = OLS(series1, series2_const).fit()
    
    # Spread (r√©sidus)
    spread = series1 - model.params[1] * series2 - model.params[0]
    
    # Test ADF sur le spread
    adf_result = adfuller(spread)
    
    return {
        'coint_stat': score,
        'coint_pvalue': pvalue,
        'is_cointegrated': pvalue < significance_level,
        'hedge_ratio': model.params[1],
        'intercept': model.params[0],
        'spread': spread,
        'spread_mean': spread.mean(),
        'spread_std': spread.std(),
        'adf_stat': adf_result[0],
        'adf_pvalue': adf_result[1]
    }


def find_cointegrated_pairs(prices_df, significance_level=0.05):
    """
    Trouve toutes les paires coint√©gr√©es dans un DataFrame.
    
    Args:
        prices_df: DataFrame de prix (colonnes = actifs)
        significance_level: Niveau de significativit√©
    
    Returns:
        list: Liste des paires coint√©gr√©es avec leurs statistiques
    """
    n = len(prices_df.columns)
    tickers = prices_df.columns.tolist()
    pairs = []
    
    for i in range(n):
        for j in range(i+1, n):
            ticker1, ticker2 = tickers[i], tickers[j]
            
            result = test_cointegration(
                prices_df[ticker1], 
                prices_df[ticker2],
                significance_level
            )
            
            if result['is_cointegrated']:
                pairs.append({
                    'ticker1': ticker1,
                    'ticker2': ticker2,
                    'pvalue': result['coint_pvalue'],
                    'hedge_ratio': result['hedge_ratio']
                })
    
    return sorted(pairs, key=lambda x: x['pvalue'])


class PairsTradingStrategy:
    """
    Strat√©gie de pairs trading bas√©e sur la cointegration.
    """
    
    def __init__(self, entry_zscore=2.0, exit_zscore=0.5, 
                 lookback=252, hedge_ratio=None):
        """
        Initialise la strat√©gie.
        
        Args:
            entry_zscore: Z-score pour entrer en position
            exit_zscore: Z-score pour sortir
            lookback: P√©riode pour calculer la moyenne/std du spread
            hedge_ratio: Ratio de hedge fixe (None = calculer dynamiquement)
        """
        self.entry_zscore = entry_zscore
        self.exit_zscore = exit_zscore
        self.lookback = lookback
        self.hedge_ratio = hedge_ratio
    
    def calculate_spread(self, prices1, prices2):
        """
        Calcule le spread entre deux s√©ries.
        
        Args:
            prices1: Prix de l'actif 1
            prices2: Prix de l'actif 2
        
        Returns:
            Series: Spread
        """
        if self.hedge_ratio is None:
            # Calculer le hedge ratio par r√©gression rolling
            # Simplifi√©: ratio constant
            result = test_cointegration(prices1, prices2)
            hr = result['hedge_ratio']
        else:
            hr = self.hedge_ratio
        
        return prices1 - hr * prices2
    
    def calculate_zscore(self, spread):
        """
        Calcule le z-score du spread.
        
        Args:
            spread: Series du spread
        
        Returns:
            Series: Z-score
        """
        mean = spread.rolling(self.lookback).mean()
        std = spread.rolling(self.lookback).std()
        
        return (spread - mean) / std
    
    def generate_signals(self, prices1, prices2):
        """
        G√©n√®re les signaux de trading.
        
        Args:
            prices1: Prix de l'actif 1
            prices2: Prix de l'actif 2
        
        Returns:
            DataFrame: Signaux pour chaque actif
        """
        spread = self.calculate_spread(prices1, prices2)
        zscore = self.calculate_zscore(spread)
        
        signals = pd.DataFrame(index=prices1.index)
        signals['zscore'] = zscore
        signals['signal1'] = 0  # Signal pour actif 1
        signals['signal2'] = 0  # Signal pour actif 2
        
        # Entr√©e short spread (spread trop haut)
        signals.loc[zscore > self.entry_zscore, 'signal1'] = -1  # Short asset 1
        signals.loc[zscore > self.entry_zscore, 'signal2'] = 1   # Long asset 2
        
        # Entr√©e long spread (spread trop bas)
        signals.loc[zscore < -self.entry_zscore, 'signal1'] = 1  # Long asset 1
        signals.loc[zscore < -self.entry_zscore, 'signal2'] = -1 # Short asset 2
        
        # Sortie
        signals.loc[abs(zscore) < self.exit_zscore, 'signal1'] = 0
        signals.loc[abs(zscore) < self.exit_zscore, 'signal2'] = 0
        
        return signals
    
    def backtest(self, prices1, prices2):
        """
        Backteste la strat√©gie.
        
        Args:
            prices1: Prix de l'actif 1
            prices2: Prix de l'actif 2
        
        Returns:
            DataFrame: R√©sultats du backtest
        """
        signals = self.generate_signals(prices1, prices2)
        
        # Rendements
        returns1 = prices1.pct_change()
        returns2 = prices2.pct_change()
        
        # Rendements de la strat√©gie
        strategy_returns = (
            signals['signal1'].shift(1) * returns1 +
            signals['signal2'].shift(1) * returns2
        ) / 2  # Normalis√©
        
        # R√©sultats
        results = pd.DataFrame(index=prices1.index)
        results['spread'] = self.calculate_spread(prices1, prices2)
        results['zscore'] = signals['zscore']
        results['signal1'] = signals['signal1']
        results['signal2'] = signals['signal2']
        results['strategy_return'] = strategy_returns
        results['cumulative_return'] = (1 + strategy_returns).cumprod()
        
        return results


# === Exemple ===
def demonstrate_pairs_trading():
    """D√©montre le pairs trading."""
    import yfinance as yf
    
    # T√©l√©charger des donn√©es (secteur financier)
    tickers = ['JPM', 'BAC', 'C', 'WFC', 'GS']
    data = yf.download(tickers, start='2018-01-01', end='2023-12-31')['Adj Close']
    
    # Trouver les paires coint√©gr√©es
    print("Recherche de paires coint√©gr√©es...")
    pairs = find_cointegrated_pairs(data, significance_level=0.05)
    
    print(f"\nPaires trouv√©es: {len(pairs)}")
    for pair in pairs[:5]:
        print(f"  {pair['ticker1']}-{pair['ticker2']}: "
              f"p-value={pair['pvalue']:.4f}, "
              f"hedge_ratio={pair['hedge_ratio']:.2f}")
    
    if pairs:
        # Backtester la meilleure paire
        best = pairs[0]
        print(f"\nBacktest de {best['ticker1']}-{best['ticker2']}")
        
        strategy = PairsTradingStrategy(
            entry_zscore=2.0,
            exit_zscore=0.5,
            hedge_ratio=best['hedge_ratio']
        )
        
        results = strategy.backtest(
            data[best['ticker1']], 
            data[best['ticker2']]
        )
        
        # M√©triques
        total_return = results['cumulative_return'].iloc[-1] - 1
        sharpe = np.sqrt(252) * results['strategy_return'].mean() / results['strategy_return'].std()
        
        print(f"\nR√©sultats:")
        print(f"  Rendement total: {total_return:.2%}")
        print(f"  Sharpe Ratio: {sharpe:.2f}")
        
        return results, strategy
    
    return None, None


# results, strategy = demonstrate_pairs_trading()
```

                                        reference=train_data)
            valid_sets.append(val_data)
            valid_names.append('valid')
        
        callbacks = [
            lgb.early_stopping(stopping_rounds=early_stopping_rounds),
            lgb.log_evaluation(period=100)
        ]
        
        self.model = lgb.train(
            self.params,
            train_data,
            num_boost_round=num_boost_round,
            valid_sets=valid_sets,
            valid_names=valid_names,
            callbacks=callbacks
        )
        
        return self
    
    def predict(self, X):
        """Pr√©dit les valeurs."""
        preds = self.model.predict(X)
        
        if self.task == 'classification':
            return (preds > 0.5).astype(int)
        return preds
    
    def predict_proba(self, X):
        """Pr√©dit les probabilit√©s."""
        return self.model.predict(X)
    
    def get_feature_importance(self, importance_type='gain'):
        """
        Retourne l'importance des features.
        
        Args:
            importance_type: 'gain', 'split', ou 'shap'
        """
        importance = self.model.feature_importance(importance_type=importance_type)
        
        if self.feature_names:
            return pd.Series(importance, index=self.feature_names).sort_values(ascending=False)
        return pd.Series(importance)
    
    def cross_validate(self, X, y, n_splits=5):
        """
        Cross-validation temporelle.
        
        Args:
            X: Features
            y: Target
            n_splits: Nombre de splits
        
        Returns:
            dict: R√©sultats de CV
        """
        tscv = TimeSeriesSplit(n_splits=n_splits)
        
        scores = []
        for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):
            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
            
            self.fit(X_train, y_train, X_val, y_val,
                    num_boost_round=500, early_stopping_rounds=30)
            
            if self.task == 'classification':
                from sklearn.metrics import roc_auc_score
                preds = self.predict_proba(X_val)
                score = roc_auc_score(y_val, preds)
            else:
                from sklearn.metrics import r2_score
                preds = self.predict(X_val)
                score = r2_score(y_val, preds)
            
            scores.append(score)
            print(f"Fold {fold+1}: {score:.4f}")
        
        return {
            'scores': scores,
            'mean': np.mean(scores),
            'std': np.std(scores)
        }


# === CatBoost ===
"""
CatBoost pour le Trading
========================
CatBoost (Categorical Boosting) est optimis√© pour les features cat√©gorielles.

Avantages:
- Gestion native des cat√©gorielles (pas besoin d'encoding)
- Ordered Target Statistics (√©vite target leakage)
- Tr√®s performant out-of-the-box
"""
from catboost import CatBoostClassifier, CatBoostRegressor, Pool

class CatBoostTrader:
    """
    CatBoost pour pr√©diction de trading.
    """
    
    def __init__(self, task='classification', params=None, cat_features=None):
        """
        Initialise CatBoost.
        
        Args:
            task: 'classification' ou 'regression'
            params: Param√®tres CatBoost
            cat_features: Liste des features cat√©gorielles
        """
        self.task = task
        self.cat_features = cat_features or []
        
        default_params = {
            'iterations': 1000,
            'learning_rate': 0.05,
            'depth': 6,
            'l2_leaf_reg': 3,
            'min_child_samples': 100,
            'random_seed': 42,
            'verbose': 100,
        }
        
        self.params = {**default_params, **(params or {})}
        
        ModelClass = CatBoostClassifier if task == 'classification' else CatBoostRegressor
        self.model = ModelClass(**self.params)
    
    def fit(self, X_train, y_train, X_val=None, y_val=None, early_stopping_rounds=50):
        """Entra√Æne le mod√®le."""
        train_pool = Pool(X_train, y_train, cat_features=self.cat_features)
        
        eval_set = None
        if X_val is not None:
            eval_set = Pool(X_val, y_val, cat_features=self.cat_features)
        
        self.model.fit(
            train_pool,
            eval_set=eval_set,
            early_stopping_rounds=early_stopping_rounds,
            use_best_model=True
        )
        
        return self
    
    def predict(self, X):
        """Pr√©dit."""
        return self.model.predict(X)
    
    def predict_proba(self, X):
        """Pr√©dit les probabilit√©s."""
        if self.task == 'classification':
            return self.model.predict_proba(X)[:, 1]
        raise ValueError("predict_proba only for classification")
    
    def get_feature_importance(self):
        """Retourne l'importance des features."""
        importance = self.model.feature_importances_
        feature_names = self.model.feature_names_
        
        return pd.Series(importance, index=feature_names).sort_values(ascending=False)


# === SHAP pour Interpr√©tabilit√© ===
"""
SHAP (SHapley Additive exPlanations)
====================================
SHAP explique les pr√©dictions individuelles en attribuant
une contribution √† chaque feature.

Bas√© sur la th√©orie des jeux (valeurs de Shapley).
"""
import shap

def explain_with_shap(model, X, feature_names=None, plot=True):
    """
    Explique les pr√©dictions avec SHAP.
    
    Args:
        model: Mod√®le entra√Æn√© (LightGBM, XGBoost, CatBoost, etc.)
        X: Features
        feature_names: Noms des features
        plot: Afficher les graphiques
    
    Returns:
        shap_values: Valeurs SHAP
    """
    # Cr√©er l'explainer
    explainer = shap.TreeExplainer(model)
    
    # Calculer les valeurs SHAP
    shap_values = explainer.shap_values(X)
    
    if plot:
        # Summary plot
        plt.figure(figsize=(10, 8))
        shap.summary_plot(shap_values, X, feature_names=feature_names, show=False)
        plt.tight_layout()
        plt.show()
        
        # Feature importance
        plt.figure(figsize=(10, 6))
        shap.summary_plot(shap_values, X, feature_names=feature_names, 
                         plot_type="bar", show=False)
        plt.tight_layout()
        plt.show()
    
    return shap_values


def explain_single_prediction(model, X_single, X_background, feature_names=None):
    """
    Explique une pr√©diction individuelle.
    
    Args:
        model: Mod√®le entra√Æn√©
        X_single: Une seule observation √† expliquer
        X_background: Donn√©es de background pour SHAP
        feature_names: Noms des features
    """
    explainer = shap.TreeExplainer(model, X_background)
    shap_values = explainer.shap_values(X_single)
    
    # Waterfall plot
    shap.plots.waterfall(shap.Explanation(
        values=shap_values[0] if isinstance(shap_values, list) else shap_values,
        base_values=explainer.expected_value[0] if isinstance(explainer.expected_value, list) else explainer.expected_value,
        data=X_single.values[0] if hasattr(X_single, 'values') else X_single[0],
        feature_names=feature_names
    ))


# === Exemple complet GBM ===
def demonstrate_gbm_trading():
    """D√©montre LightGBM et CatBoost pour le trading."""
    np.random.seed(42)
    n = 10000
    
    # Cr√©er des donn√©es
    X = pd.DataFrame({
        'momentum_1m': np.random.randn(n),
        'momentum_3m': np.random.randn(n),
        'volatility': np.abs(np.random.randn(n)) + 0.1,
        'rsi': np.random.uniform(20, 80, n),
        'macd': np.random.randn(n),
        'volume_ratio': np.random.lognormal(0, 0.3, n),
        'sector': np.random.choice(['Tech', 'Finance', 'Health', 'Energy'], n),  # Cat√©gorielle
    })
    
    # Encoder la cat√©gorielle pour LightGBM
    X_lgb = X.copy()
    X_lgb['sector'] = X_lgb['sector'].astype('category').cat.codes
    
    # Target
    y = ((X['momentum_1m'] > 0.3) & (X['rsi'] < 65) | 
         (X['macd'] > 0.5)).astype(int)
    
    # Split temporel
    train_end = int(0.7 * n)
    val_end = int(0.85 * n)
    
    X_train, y_train = X_lgb[:train_end], y[:train_end]
    X_val, y_val = X_lgb[train_end:val_end], y[train_end:val_end]
    X_test, y_test = X_lgb[val_end:], y[val_end:]
    
    # LightGBM
    print("=" * 60)
    print("LightGBM")
    print("=" * 60)
    
    lgbm = LightGBMTrader(task='classification')
    lgbm.fit(X_train, y_train, X_val, y_val)
    
    from sklearn.metrics import accuracy_score, roc_auc_score
    
    y_pred_lgb = lgbm.predict(X_test)
    y_proba_lgb = lgbm.predict_proba(X_test)
    
    print(f"\nLightGBM Results:")
    print(f"  Accuracy: {accuracy_score(y_test, y_pred_lgb):.2%}")
    print(f"  ROC AUC: {roc_auc_score(y_test, y_proba_lgb):.4f}")
    
    print(f"\nFeature Importance (Gain):")
    for feat, imp in lgbm.get_feature_importance('gain').head(5).items():
        print(f"  {feat}: {imp:.0f}")
    
    # CatBoost
    print("\n" + "=" * 60)
    print("CatBoost")
    print("=" * 60)
    
    # CatBoost avec features originales (g√®re les cat√©gorielles)
    X_cat_train = X[:train_end]
    X_cat_val = X[train_end:val_end]
    X_cat_test = X[val_end:]
    
    catboost = CatBoostTrader(task='classification', cat_features=['sector'])
    catboost.fit(X_cat_train, y_train, X_cat_val, y_val)
    
    y_pred_cat = catboost.predict(X_cat_test)
    y_proba_cat = catboost.predict_proba(X_cat_test)
    
    print(f"\nCatBoost Results:")
    print(f"  Accuracy: {accuracy_score(y_test, y_pred_cat):.2%}")
    print(f"  ROC AUC: {roc_auc_score(y_test, y_proba_cat):.4f}")
    
    return lgbm, catboost


# lgbm, catboost = demonstrate_gbm_trading()
```

---

# 13. APPRENTISSAGE NON SUPERVIS√â
## Unsupervised Learning

## 13.1 PCA (Principal Component Analysis)

```python
"""
PCA pour la Finance
===================
PCA (Principal Component Analysis - Analyse en Composantes Principales)
r√©duit la dimensionnalit√© en trouvant les directions de variance maximale.

Applications en finance:
- Extraction de facteurs de risque
- Compression de donn√©es
- D√©tection d'anomalies
- Construction d'eigenportfolios
"""
import numpy as np
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

class FinancialPCA:
    """
    PCA pour donn√©es financi√®res.
    """
    
    def __init__(self, n_components=None, variance_threshold=0.95):
        """
        Initialise PCA.
        
        Args:
            n_components: Nombre de composantes (None = auto)
            variance_threshold: Seuil de variance expliqu√©e si n_components=None
        """
        self.n_components = n_components
        self.variance_threshold = variance_threshold
        self.scaler = StandardScaler()
        self.pca = None
        self.feature_names = None
    
    def fit(self, X):
        """
        Ajuste PCA aux donn√©es.
        
        Args:
            X: DataFrame ou array de features
        """
        if isinstance(X, pd.DataFrame):
            self.feature_names = X.columns.tolist()
        
        # Standardiser
        X_scaled = self.scaler.fit_transform(X)
        
        # Si n_components est None, trouver automatiquement
        if self.n_components is None:
            # D'abord faire PCA complet
            pca_full = PCA()
            pca_full.fit(X_scaled)
            
            # Trouver le nombre de composantes pour atteindre le seuil
            cumsum = np.cumsum(pca_full.explained_variance_ratio_)
            self.n_components = np.argmax(cumsum >= self.variance_threshold) + 1
            print(f"Composantes s√©lectionn√©es: {self.n_components} "
                  f"({cumsum[self.n_components-1]:.1%} de variance)")
        
        # PCA final
        self.pca = PCA(n_components=self.n_components)
        self.pca.fit(X_scaled)
        
        return self
    
    def transform(self, X):
        """Transforme les donn√©es en composantes principales."""
        X_scaled = self.scaler.transform(X)
        return self.pca.transform(X_scaled)
    
    def fit_transform(self, X):
        """Fit et transforme."""
        self.fit(X)
        return self.transform(X)
    
    def get_loadings(self):
        """
        Retourne les loadings (poids des features originales).
        
        Returns:
            DataFrame: Loadings par composante
        """
        loadings = pd.DataFrame(
            self.pca.components_.T,
            index=self.feature_names or range(len(self.pca.components_[0])),
            columns=[f'PC{i+1}' for i in range(self.n_components)]
        )
        return loadings
    
    def explained_variance(self):
        """Retourne la variance expliqu√©e par composante."""
        return pd.Series(
            self.pca.explained_variance_ratio_,
            index=[f'PC{i+1}' for i in range(self.n_components)]
        )
    
    def plot_explained_variance(self):
        """Affiche la variance expliqu√©e."""
        fig, axes = plt.subplots(1, 2, figsize=(14, 5))
        
        # Variance par composante
        ax1 = axes[0]
        var_ratio = self.pca.explained_variance_ratio_
        ax1.bar(range(1, len(var_ratio)+1), var_ratio, alpha=0.7)
        ax1.set_xlabel('Composante')
        ax1.set_ylabel('Variance Expliqu√©e')
        ax1.set_title('Variance par Composante')
        
        # Variance cumul√©e
        ax2 = axes[1]
        cumsum = np.cumsum(var_ratio)
        ax2.plot(range(1, len(cumsum)+1), cumsum, 'bo-')
        ax2.axhline(y=0.95, color='r', linestyle='--', label='95%')
        ax2.set_xlabel('Nombre de Composantes')
        ax2.set_ylabel('Variance Cumul√©e')
        ax2.set_title('Variance Cumul√©e')
        ax2.legend()
        
        plt.tight_layout()
        plt.show()


def extract_risk_factors(returns_df, n_factors=5):
    """
    Extrait les facteurs de risque √† partir des rendements.
    
    Args:
        returns_df: DataFrame de rendements (colonnes = actifs)
        n_factors: Nombre de facteurs √† extraire
    
    Returns:
        tuple: (facteurs, loadings, pca_object)
    """
    pca = FinancialPCA(n_components=n_factors)
    factors = pca.fit_transform(returns_df)
    
    factors_df = pd.DataFrame(
        factors,
        index=returns_df.index,
        columns=[f'Factor_{i+1}' for i in range(n_factors)]
    )
    
    loadings = pca.get_loadings()
    
    return factors_df, loadings, pca


def build_eigenportfolios(returns_df, n_portfolios=5):
    """
    Construit des eigenportfolios √† partir de PCA.
    
    Les eigenportfolios sont des portefeuilles dont les poids
    correspondent aux loadings des composantes principales.
    
    Args:
        returns_df: DataFrame de rendements
        n_portfolios: Nombre d'eigenportfolios
    
    Returns:
        DataFrame: Rendements des eigenportfolios
    """
    pca = FinancialPCA(n_components=n_portfolios)
    pca.fit(returns_df)
    
    # Les loadings sont les poids (normalis√©s)
    loadings = pca.get_loadings()
    
    # Calculer les rendements des eigenportfolios
    eigen_returns = pd.DataFrame(index=returns_df.index)
    
    for i in range(n_portfolios):
        weights = loadings[f'PC{i+1}'].values
        weights = weights / np.sum(np.abs(weights))  # Normaliser
        
        eigen_returns[f'EigenPF_{i+1}'] = returns_df.dot(weights)
    
    return eigen_returns, loadings


# === Exemple ===
def demonstrate_pca():
    """D√©montre PCA pour la finance."""
    import yfinance as yf
    
    # T√©l√©charger des donn√©es
    tickers = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'META', 
               'JPM', 'BAC', 'GS', 'XOM', 'CVX']
    data = yf.download(tickers, start='2020-01-01', end='2023-12-31')['Adj Close']
    returns = data.pct_change().dropna()
    
    print("PCA sur les rendements boursiers")
    print("=" * 50)
    
    # Extraire les facteurs
    factors, loadings, pca = extract_risk_factors(returns, n_factors=3)
    
    print(f"\nVariance expliqu√©e:")
    for comp, var in pca.explained_variance().items():
        print(f"  {comp}: {var:.1%}")
    
    print(f"\nLoadings PC1 (facteur march√©):")
    pc1_loadings = loadings['PC1'].sort_values(ascending=False)
    for ticker, loading in pc1_loadings.items():
        print(f"  {ticker}: {loading:.3f}")
    
    # Eigenportfolios
    eigen_ret, _ = build_eigenportfolios(returns, n_portfolios=3)
    
    print(f"\nPerformance des Eigenportfolios:")
    for col in eigen_ret.columns:
        sharpe = np.sqrt(252) * eigen_ret[col].mean() / eigen_ret[col].std()
        print(f"  {col}: Sharpe = {sharpe:.2f}")
    
    # Plot
    pca.plot_explained_variance()
    
    return factors, loadings, pca


# factors, loadings, pca = demonstrate_pca()
```

## 13.2 Clustering (K-Means, Hierarchical)

```python
"""
Clustering pour la Finance
==========================
Le clustering groupe les actifs similaires ensemble.

Applications:
- Allocation d'actifs
- D√©tection de r√©gimes de march√©
- Diversification de portefeuille
"""
import numpy as np
import pandas as pd
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
from scipy.cluster.hierarchy import dendrogram, linkage
import matplotlib.pyplot as plt

def cluster_assets(returns_df, n_clusters=5, method='kmeans'):
    """
    Cluster les actifs bas√© sur leurs rendements.
    
    Args:
        returns_df: DataFrame de rendements
        n_clusters: Nombre de clusters
        method: 'kmeans', 'hierarchical', ou 'dbscan'
    
    Returns:
        dict: Assignments et statistiques
    """
    # Calculer les features de chaque actif
    features = pd.DataFrame(index=returns_df.columns)
    features['mean_return'] = returns_df.mean() * 252
    features['volatility'] = returns_df.std() * np.sqrt(252)
    features['sharpe'] = features['mean_return'] / features['volatility']
    features['skewness'] = returns_df.skew()
    features['kurtosis'] = returns_df.kurtosis()
    
    # Standardiser
    scaler = StandardScaler()
    features_scaled = scaler.fit_transform(features)
    
    # Clustering
    if method == 'kmeans':
        model = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    elif method == 'hierarchical':
        model = AgglomerativeClustering(n_clusters=n_clusters)
    elif method == 'dbscan':
        model = DBSCAN(eps=0.5, min_samples=2)
    else:
        raise ValueError(f"M√©thode inconnue: {method}")
    
    labels = model.fit_predict(features_scaled)
    
    # Silhouette score
    if len(set(labels)) > 1:
        sil_score = silhouette_score(features_scaled, labels)
    else:
        sil_score = None
    
    # R√©sultats
    features['cluster'] = labels
    
    return {
        'features': features,
        'labels': labels,
        'silhouette_score': sil_score,
        'cluster_stats': features.groupby('cluster').mean()
    }


def plot_hierarchical_clustering(returns_df, method='ward'):
    """
    Affiche le dendrogramme du clustering hi√©rarchique.
    
    Args:
        returns_df: DataFrame de rendements
        method: M√©thode de linkage ('ward', 'complete', 'average', 'single')
    """
    # Calculer la matrice de corr√©lation
    corr = returns_df.corr()
    
    # Convertir en distance
    distance = 1 - corr
    
    # Linkage
    Z = linkage(distance, method=method)
    
    # Plot
    fig, ax = plt.subplots(figsize=(14, 8))
    dendrogram(Z, labels=returns_df.columns, ax=ax, leaf_rotation=90)
    ax.set_title(f'Hierarchical Clustering (method={method})')
    ax.set_ylabel('Distance')
    plt.tight_layout()
    plt.show()
    
    return Z


def find_optimal_clusters(returns_df, max_clusters=10):
    """
    Trouve le nombre optimal de clusters.
    
    Args:
        returns_df: DataFrame de rendements
        max_clusters: Nombre maximum de clusters √† tester
    
    Returns:
        dict: Scores pour chaque k
    """
    # Features
    features = pd.DataFrame(index=returns_df.columns)
    features['mean_return'] = returns_df.mean() * 252
    features['volatility'] = returns_df.std() * np.sqrt(252)
    features['sharpe'] = features['mean_return'] / features['volatility']
    
    scaler = StandardScaler()
    features_scaled = scaler.fit_transform(features)
    
    results = {'k': [], 'inertia': [], 'silhouette': []}
    
    for k in range(2, max_clusters + 1):
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        labels = kmeans.fit_predict(features_scaled)
        
        results['k'].append(k)
        results['inertia'].append(kmeans.inertia_)
        results['silhouette'].append(silhouette_score(features_scaled, labels))
    
    # Plot
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # Elbow plot
    axes[0].plot(results['k'], results['inertia'], 'bo-')
    axes[0].set_xlabel('Nombre de clusters')
    axes[0].set_ylabel('Inertie')
    axes[0].set_title('M√©thode du coude (Elbow)')
    
    # Silhouette
    axes[1].plot(results['k'], results['silhouette'], 'go-')
    axes[1].set_xlabel('Nombre de clusters')
    axes[1].set_ylabel('Silhouette Score')
    axes[1].set_title('Score Silhouette')
    
    plt.tight_layout()
    plt.show()
    
    return pd.DataFrame(results)
```

## 13.3 Hierarchical Risk Parity (HRP)

```python
"""
Hierarchical Risk Parity (HRP)
==============================
HRP est une m√©thode d'allocation de portefeuille qui utilise
le clustering hi√©rarchique pour construire des portefeuilles diversifi√©s.

Avantages vs Markowitz:
- Pas d'inversion de matrice (plus stable)
- Meilleures performances out-of-sample
- Plus intuitif (diversification hi√©rarchique)

Algorithme:
1. Tree Clustering: Grouper les actifs par corr√©lation
2. Quasi-Diagonalization: R√©organiser la matrice de covariance
3. Recursive Bisection: Allouer le capital r√©cursivement
"""
import numpy as np
import pandas as pd
from scipy.cluster.hierarchy import linkage, dendrogram
from scipy.spatial.distance import squareform

class HierarchicalRiskParity:
    """
    Impl√©mentation de Hierarchical Risk Parity.
    """
    
    def __init__(self):
        self.weights = None
        self.linkage = None
        self.sorted_indices = None
    
    def fit(self, returns):
        """
        Calcule les poids HRP.
        
        Args:
            returns: DataFrame de rendements
        
        Returns:
            Series: Poids optimaux
        """
        # √âtape 1: Matrice de corr√©lation et covariance
        corr = returns.corr()
        cov = returns.cov()
        
        # √âtape 2: Tree Clustering
        dist = self._correlation_distance(corr)
        self.linkage = linkage(squareform(dist), method='single')
        
        # √âtape 3: Quasi-Diagonalization
        self.sorted_indices = self._get_quasi_diag(self.linkage)
        
        # √âtape 4: Recursive Bisection
        weights = self._recursive_bisection(cov, self.sorted_indices)
        
        self.weights = pd.Series(weights, index=returns.columns)
        
        return self.weights
    
    def _correlation_distance(self, corr):
        """
        Convertit la corr√©lation en distance.
        
        d = sqrt(0.5 * (1 - corr))
        """
        return np.sqrt(0.5 * (1 - corr))
    
    def _get_quasi_diag(self, link):
        """
        R√©organise les indices pour quasi-diagonaliser la matrice.
        """
        link = link.astype(int)
        sort_ix = pd.Series([link[-1, 0], link[-1, 1]])
        num_items = link[-1, 3]
        
        while sort_ix.max() >= num_items:
            sort_ix.index = range(0, sort_ix.shape[0] * 2, 2)
            df0 = sort_ix[sort_ix >= num_items]
            i = df0.index
            j = df0.values - num_items
            sort_ix[i] = link[j, 0]
            df0 = pd.Series(link[j, 1], index=i + 1)
            sort_ix = pd.concat([sort_ix, df0])
            sort_ix = sort_ix.sort_index()
            sort_ix.index = range(sort_ix.shape[0])
        
        return sort_ix.tolist()
    
    def _recursive_bisection(self, cov, sorted_indices):
        """
        Alloue le capital r√©cursivement.
        """
        weights = pd.Series(1.0, index=sorted_indices)
        clusters = [sorted_indices]
        
        while len(clusters) > 0:
            # Bisection
            clusters = [
                cluster[j:k]
                for cluster in clusters
                for j, k in ((0, len(cluster) // 2), (len(cluster) // 2, len(cluster)))
                if len(cluster) > 1
            ]
            
            for i in range(0, len(clusters), 2):
                if i + 1 < len(clusters):
                    cluster0 = clusters[i]
                    cluster1 = clusters[i + 1]
                    
                    # Variance de chaque cluster
                    var0 = self._get_cluster_var(cov, cluster0)
                    var1 = self._get_cluster_var(cov, cluster1)
                    
                    # Allocation inversement proportionnelle √† la variance
                    alpha = 1 - var0 / (var0 + var1)
                    
                    weights[cluster0] *= alpha
                    weights[cluster1] *= 1 - alpha
        
        return weights
    
    def _get_cluster_var(self, cov, cluster_items):
        """
        Calcule la variance d'un cluster.
        
        Utilise l'inverse-variance weighting au sein du cluster.
        """
        cov_slice = cov.iloc[cluster_items, cluster_items]
        
        # Poids inverse-variance
        ivp = 1 / np.diag(cov_slice)
        ivp /= ivp.sum()
        
        # Variance du cluster
        return np.dot(ivp, np.dot(cov_slice, ivp))
    
    def plot_dendrogram(self, labels=None):
        """Affiche le dendrogramme."""
        if self.linkage is None:
            raise ValueError("Appelez fit() d'abord")
        
        fig, ax = plt.subplots(figsize=(14, 8))
        dendrogram(self.linkage, labels=labels, ax=ax, leaf_rotation=90)
        ax.set_title('Hierarchical Risk Parity - Dendrogram')
        plt.tight_layout()
        plt.show()


def compare_portfolio_methods(returns, risk_free_rate=0.02):
    """
    Compare diff√©rentes m√©thodes d'allocation.
    
    Args:
        returns: DataFrame de rendements
        risk_free_rate: Taux sans risque
    
    Returns:
        DataFrame: Comparaison des m√©thodes
    """
    results = {}
    
    # 1. Equal Weight
    n = len(returns.columns)
    ew_weights = pd.Series(1/n, index=returns.columns)
    ew_returns = returns.dot(ew_weights)
    results['Equal Weight'] = {
        'weights': ew_weights,
        'return': ew_returns.mean() * 252,
        'volatility': ew_returns.std() * np.sqrt(252),
    }
    results['Equal Weight']['sharpe'] = (
        (results['Equal Weight']['return'] - risk_free_rate) / 
        results['Equal Weight']['volatility']
    )
    
    # 2. HRP
    hrp = HierarchicalRiskParity()
    hrp_weights = hrp.fit(returns)
    hrp_returns = returns.dot(hrp_weights)
    results['HRP'] = {
        'weights': hrp_weights,
        'return': hrp_returns.mean() * 252,
        'volatility': hrp_returns.std() * np.sqrt(252),
    }
    results['HRP']['sharpe'] = (
        (results['HRP']['return'] - risk_free_rate) / 
        results['HRP']['volatility']
    )
    
    # 3. Inverse Volatility
    vol = returns.std()
    iv_weights = (1/vol) / (1/vol).sum()
    iv_returns = returns.dot(iv_weights)
    results['Inverse Vol'] = {
        'weights': iv_weights,
        'return': iv_returns.mean() * 252,
        'volatility': iv_returns.std() * np.sqrt(252),
    }
    results['Inverse Vol']['sharpe'] = (
        (results['Inverse Vol']['return'] - risk_free_rate) / 
        results['Inverse Vol']['volatility']
    )
    
    # Afficher les r√©sultats
    print("Comparaison des m√©thodes d'allocation")
    print("=" * 60)
    
    for method, res in results.items():
        print(f"\n{method}:")
        print(f"  Rendement: {res['return']:.2%}")
        print(f"  Volatilit√©: {res['volatility']:.2%}")
        print(f"  Sharpe: {res['sharpe']:.2f}")
    
    return results


# === Exemple ===
def demonstrate_hrp():
    """D√©montre HRP."""
    import yfinance as yf
    
    tickers = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'JPM', 'BAC', 'XOM', 'CVX', 'JNJ', 'PG']
    data = yf.download(tickers, start='2019-01-01', end='2023-12-31')['Adj Close']
    returns = data.pct_change().dropna()
    
    # HRP
    hrp = HierarchicalRiskParity()
    weights = hrp.fit(returns)
    
    print("Poids HRP:")
    print("-" * 30)
    for ticker, weight in weights.sort_values(ascending=False).items():
        print(f"  {ticker}: {weight:.2%}")
    
    # Dendrogramme
    hrp.plot_dendrogram(labels=returns.columns.tolist())
    
    # Comparaison
    results = compare_portfolio_methods(returns)
    
    return hrp, results


# hrp, results = demonstrate_hrp()
```

---

# 14. TRAITEMENT DU LANGAGE NATUREL (NLP)
## Natural Language Processing

## 14.1 Pipeline NLP avec spaCy

```python
"""
NLP pour la Finance avec spaCy
==============================
Le NLP permet d'extraire des informations des textes financiers:
- Rapports d'analystes
- Articles de presse
- Filings SEC
- R√©seaux sociaux
- Earnings calls transcripts

Pipeline NLP typique:
1. Tokenization: D√©couper en mots/phrases
2. POS Tagging: Identifier les parties du discours
3. NER: Extraire les entit√©s nomm√©es
4. Lemmatization: R√©duire aux formes de base
5. Sentiment Analysis: √âvaluer la tonalit√©
"""
import spacy
import pandas as pd
import numpy as np

# Charger le mod√®le spaCy
# python -m spacy download en_core_web_sm
nlp = spacy.load('en_core_web_sm')

def analyze_text(text):
    """
    Analyse compl√®te d'un texte avec spaCy.
    
    Args:
        text: Texte √† analyser
    
    Returns:
        dict: R√©sultats de l'analyse
    """
    doc = nlp(text)
    
    # Tokens avec leurs propri√©t√©s
    tokens = [{
        'text': token.text,
        'lemma': token.lemma_,
        'pos': token.pos_,          # Part of Speech (Noun, Verb, etc.)
        'tag': token.tag_,          # Fine-grained POS
        'dep': token.dep_,          # Dependency relation
        'is_stop': token.is_stop,   # Est-ce un stopword?
        'is_alpha': token.is_alpha  # Est-ce alphab√©tique?
    } for token in doc]
    
    # Entit√©s nomm√©es
    entities = [{
        'text': ent.text,
        'label': ent.label_,        # Type d'entit√© (ORG, PERSON, MONEY, etc.)
        'start': ent.start_char,
        'end': ent.end_char
    } for ent in doc.ents]
    
    # Phrases
    sentences = [sent.text for sent in doc.sents]
    
    # Noun chunks (groupes nominaux)
    noun_chunks = [chunk.text for chunk in doc.noun_chunks]
    
    return {
        'tokens': tokens,
        'entities': entities,
        'sentences': sentences,
        'noun_chunks': noun_chunks,
        'n_tokens': len(doc),
        'n_sentences': len(sentences),
        'n_entities': len(entities)
    }


def extract_financial_entities(text):
    """
    Extrait les entit√©s financi√®res d'un texte.
    
    Args:
        text: Texte financier
    
    Returns:
        dict: Entit√©s par cat√©gorie
    """
    doc = nlp(text)
    
    entities = {
        'MONEY': [],      # Montants ($10 million)
        'PERCENT': [],    # Pourcentages (15%)
        'ORG': [],        # Organisations (Apple Inc.)
        'PERSON': [],     # Personnes (Tim Cook)
        'DATE': [],       # Dates (Q3 2023)
        'GPE': [],        # Pays/Villes (United States)
        'CARDINAL': [],   # Nombres
    }
    
    for ent in doc.ents:
        if ent.label_ in entities:
            entities[ent.label_].append(ent.text)
    
    return entities


# === TextBlob pour Sentiment ===
from textblob import TextBlob

def analyze_sentiment_textblob(text):
    """
    Analyse le sentiment avec TextBlob.
    
    Args:
        text: Texte √† analyser
    
    Returns:
        dict: Polarit√© (-1 √† 1) et subjectivit√© (0 √† 1)
    """
    blob = TextBlob(text)
    
    return {
        'polarity': blob.sentiment.polarity,      # -1 (n√©gatif) √† 1 (positif)
        'subjectivity': blob.sentiment.subjectivity,  # 0 (objectif) √† 1 (subjectif)
        'sentences': [{
            'text': str(sentence),
            'polarity': sentence.sentiment.polarity,
            'subjectivity': sentence.sentiment.subjectivity
        } for sentence in blob.sentences]
    }


def batch_sentiment_analysis(texts):
    """
    Analyse le sentiment d'une liste de textes.
    
    Args:
        texts: Liste de textes
    
    Returns:
        DataFrame: R√©sultats d'analyse
    """
    results = []
    
    for text in texts:
        sentiment = analyze_sentiment_textblob(text)
        results.append({
            'text': text[:100] + '...' if len(text) > 100 else text,
            'polarity': sentiment['polarity'],
            'subjectivity': sentiment['subjectivity'],
            'sentiment_label': 'positive' if sentiment['polarity'] > 0.1 else 
                              ('negative' if sentiment['polarity'] < -0.1 else 'neutral')
        })
    
    return pd.DataFrame(results)


# === Exemple ===
def demonstrate_nlp():
    """D√©montre le NLP financier."""
    
    # Texte d'exemple (earnings call fictif)
    text = """
    Apple Inc. reported strong Q3 2023 results with revenue of $81.8 billion, 
    up 5% year-over-year. CEO Tim Cook stated that iPhone sales exceeded 
    expectations, particularly in China. The company announced a $90 billion 
    share buyback program. Analysts from Goldman Sachs raised their price 
    target to $200. However, iPad sales declined 8% due to supply chain issues.
    """
    
    print("Analyse NLP du texte financier")
    print("=" * 60)
    
    # Entit√©s
    entities = extract_financial_entities(text)
    print("\nEntit√©s extraites:")
    for entity_type, values in entities.items():
        if values:
            print(f"  {entity_type}: {', '.join(set(values))}")
    
    # Sentiment
    sentiment = analyze_sentiment_textblob(text)
    print(f"\nSentiment global:")
    print(f"  Polarit√©: {sentiment['polarity']:.2f}")
    print(f"  Subjectivit√©: {sentiment['subjectivity']:.2f}")
    
    print(f"\nSentiment par phrase:")
    for sent in sentiment['sentences']:
        label = 'positive' if sent['polarity'] > 0.1 else ('negative' if sent['polarity'] < -0.1 else 'neutral')
        print(f"  [{label:8}] {sent['text'][:60]}...")
    
    return entities, sentiment


# entities, sentiment = demonstrate_nlp()
```

## 14.2 Document-Term Matrix et TF-IDF

```python
"""
Repr√©sentation Vectorielle des Documents
========================================
Pour utiliser le ML sur du texte, il faut convertir les documents
en vecteurs num√©riques.

M√©thodes:
1. Bag of Words (BoW): Compte des mots
2. TF-IDF: Pond√©ration par importance relative
"""
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
import pandas as pd
import numpy as np

def create_document_term_matrix(documents, method='tfidf', 
                                 max_features=1000, ngram_range=(1, 2)):
    """
    Cr√©e une matrice document-terme.
    
    Args:
        documents: Liste de textes
        method: 'count' (BoW) ou 'tfidf'
        max_features: Nombre maximum de features
        ngram_range: Range de n-grams (1,1)=unigrams, (1,2)=uni+bigrams
    
    Returns:
        tuple: (matrice, vectorizer)
    """
    if method == 'count':
        vectorizer = CountVectorizer(
            max_features=max_features,
            ngram_range=ngram_range,
            stop_words='english',
            min_df=2,           # Appara√Æt dans au moins 2 docs
            max_df=0.95         # Pas plus de 95% des docs
        )
    else:
        vectorizer = TfidfVectorizer(
            max_features=max_features,
            ngram_range=ngram_range,
            stop_words='english',
            min_df=2,
            max_df=0.95,
            sublinear_tf=True   # Utilise 1 + log(tf) au lieu de tf
        )
    
    dtm = vectorizer.fit_transform(documents)
    
    # Convertir en DataFrame pour visualisation
    feature_names = vectorizer.get_feature_names_out()
    dtm_df = pd.DataFrame(
        dtm.toarray(),
        columns=feature_names
    )
    
    return dtm_df, vectorizer


def get_top_terms(dtm_df, n_terms=20):
    """
    Retourne les termes les plus fr√©quents.
    
    Args:
        dtm_df: Document-term matrix
        n_terms: Nombre de termes
    
    Returns:
        Series: Top termes avec leurs scores
    """
    term_freq = dtm_df.sum().sort_values(ascending=False)
    return term_freq.head(n_terms)


def classify_documents(train_texts, train_labels, test_texts, vectorizer_type='tfidf'):
    """
    Classification de documents avec Naive Bayes.
    
    Args:
        train_texts: Textes d'entra√Ænement
        train_labels: Labels
        test_texts: Textes de test
        vectorizer_type: 'tfidf' ou 'count'
    
    Returns:
        tuple: (predictions, probabilities)
    """
    from sklearn.naive_bayes import MultinomialNB
    from sklearn.pipeline import Pipeline
    
    # Pipeline
    if vectorizer_type == 'tfidf':
        vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)
    else:
        vectorizer = CountVectorizer(stop_words='english', max_features=5000)
    
    pipeline = Pipeline([
        ('vectorizer', vectorizer),
        ('classifier', MultinomialNB())
    ])
    
    # Entra√Æner
    pipeline.fit(train_texts, train_labels)
    
    # Pr√©dire
    predictions = pipeline.predict(test_texts)
    probabilities = pipeline.predict_proba(test_texts)
    
    return predictions, probabilities, pipeline
```

---

# 15. TOPIC MODELING
## Topic Modeling

## 15.1 LDA (Latent Dirichlet Allocation)

```python
"""
LDA pour l'Analyse de Th√®mes Financiers
=======================================
LDA d√©couvre automatiquement les th√®mes latents dans un corpus de documents.

Applications en finance:
- Analyser les th√®mes des earnings calls
- D√©tecter les sujets d'actualit√©
- Classifier les articles de presse
- Identifier les pr√©occupations des investisseurs
"""
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.feature_extraction.text import CountVectorizer
import numpy as np
import pandas as pd

class LDATopicModel:
    """
    Mod√®le LDA pour d√©couverte de th√®mes.
    """
    
    def __init__(self, n_topics=10, max_features=5000, n_top_words=10):
        """
        Initialise LDA.
        
        Args:
            n_topics: Nombre de th√®mes
            max_features: Vocabulaire maximum
            n_top_words: Mots par th√®me √† afficher
        """
        self.n_topics = n_topics
        self.n_top_words = n_top_words
        
        self.vectorizer = CountVectorizer(
            max_features=max_features,
            stop_words='english',
            min_df=5,
            max_df=0.9
        )
        
        self.lda = LatentDirichletAllocation(
            n_components=n_topics,
            max_iter=20,
            learning_method='online',
            random_state=42,
            n_jobs=-1
        )
        
        self.feature_names = None
    
    def fit(self, documents):
        """
        Entra√Æne LDA sur les documents.
        
        Args:
            documents: Liste de textes
        """
        # Vectoriser
        dtm = self.vectorizer.fit_transform(documents)
        self.feature_names = self.vectorizer.get_feature_names_out()
        
        # Entra√Æner LDA
        self.lda.fit(dtm)
        
        return self
    
    def transform(self, documents):
        """
        Calcule la distribution de th√®mes pour les documents.
        
        Args:
            documents: Liste de textes
        
        Returns:
            array: Distribution de th√®mes (n_docs x n_topics)
        """
        dtm = self.vectorizer.transform(documents)
        return self.lda.transform(dtm)
    
    def get_topics(self):
        """
        Retourne les th√®mes avec leurs mots cl√©s.
        
        Returns:
            dict: Th√®mes avec mots et poids
        """
        topics = {}
        
        for topic_idx, topic in enumerate(self.lda.components_):
            top_indices = topic.argsort()[:-self.n_top_words - 1:-1]
            top_words = [self.feature_names[i] for i in top_indices]
            top_weights = topic[top_indices]
            
            topics[f'Topic_{topic_idx}'] = {
                'words': top_words,
                'weights': top_weights.tolist()
            }
        
        return topics
    
    def print_topics(self):
        """Affiche les th√®mes."""
        topics = self.get_topics()
        
        print("="*60)
        print("TH√àMES D√âCOUVERTS")
        print("="*60)
        
        for topic_name, topic_data in topics.items():
            print(f"\n{topic_name}:")
            for word, weight in zip(topic_data['words'], topic_data['weights']):
                print(f"  {word}: {weight:.3f}")
    
    def get_document_topics(self, documents, threshold=0.1):
        """
        Assigne les th√®mes dominants √† chaque document.
        
        Args:
            documents: Liste de textes
            threshold: Seuil minimum pour consid√©rer un th√®me
        
        Returns:
            DataFrame: Th√®mes par document
        """
        topic_dist = self.transform(documents)
        
        results = []
        for i, dist in enumerate(topic_dist):
            dominant_topic = np.argmax(dist)
            dominant_prob = dist[dominant_topic]
            
            # Tous les th√®mes au-dessus du seuil
            significant_topics = [
                f'Topic_{j}' for j, p in enumerate(dist) if p >= threshold
            ]
            
            results.append({
                'document_idx': i,
                'dominant_topic': f'Topic_{dominant_topic}',
                'dominant_prob': dominant_prob,
                'significant_topics': significant_topics
            })
        
        return pd.DataFrame(results)


# === Exemple avec Gensim (plus avanc√©) ===
"""
Gensim offre plus de contr√¥le et de fonctionnalit√©s pour LDA.
"""
from gensim import corpora
from gensim.models import LdaMulticore
from gensim.models.coherencemodel import CoherenceModel

def train_lda_gensim(documents, n_topics=10, passes=10):
    """
    Entra√Æne LDA avec Gensim.
    
    Args:
        documents: Liste de documents (chaque doc = liste de mots)
        n_topics: Nombre de th√®mes
        passes: Nombre de passes sur le corpus
    
    Returns:
        tuple: (model, dictionary, corpus)
    """
    # Cr√©er le dictionnaire
    dictionary = corpora.Dictionary(documents)
    
    # Filtrer les termes rares et trop fr√©quents
    dictionary.filter_extremes(no_below=5, no_above=0.5)
    
    # Cr√©er le corpus (bag of words)
    corpus = [dictionary.doc2bow(doc) for doc in documents]
    
    # Entra√Æner LDA
    lda_model = LdaMulticore(
        corpus=corpus,
        id2word=dictionary,
        num_topics=n_topics,
        passes=passes,
        workers=4,
        random_state=42
    )
    
    # Coh√©rence (mesure de qualit√©)
    coherence_model = CoherenceModel(
        model=lda_model,
        texts=documents,
        dictionary=dictionary,
        coherence='c_v'
    )
    coherence = coherence_model.get_coherence()
    
    print(f"Coherence Score: {coherence:.4f}")
    
    return lda_model, dictionary, corpus


def find_optimal_topics(documents, min_topics=5, max_topics=20, step=2):
    """
    Trouve le nombre optimal de th√®mes par coh√©rence.
    
    Args:
        documents: Liste de documents tokenis√©s
        min_topics: Minimum de th√®mes
        max_topics: Maximum de th√®mes
        step: Pas
    
    Returns:
        dict: Scores de coh√©rence par nombre de th√®mes
    """
    dictionary = corpora.Dictionary(documents)
    dictionary.filter_extremes(no_below=5, no_above=0.5)
    corpus = [dictionary.doc2bow(doc) for doc in documents]
    
    coherence_scores = {}
    
    for n in range(min_topics, max_topics + 1, step):
        print(f"Testing {n} topics...")
        
        model = LdaMulticore(
            corpus=corpus,
            id2word=dictionary,
            num_topics=n,
            passes=5,
            workers=4,
            random_state=42
        )
        
        cm = CoherenceModel(
            model=model,
            texts=documents,
            dictionary=dictionary,
            coherence='c_v'
        )
        
        coherence_scores[n] = cm.get_coherence()
    
    # Meilleur nombre
    best_n = max(coherence_scores, key=coherence_scores.get)
    print(f"\nMeilleur nombre de th√®mes: {best_n} (coherence: {coherence_scores[best_n]:.4f})")
    
    return coherence_scores
```

---

# 16. WORD EMBEDDINGS
## Word Embeddings

## 16.1 Word2Vec

```python
"""
Word Embeddings pour la Finance
===============================
Les word embeddings repr√©sentent les mots comme des vecteurs denses
capturant les relations s√©mantiques.

word2vec: "king" - "man" + "woman" ‚âà "queen"
finance:  "stock" - "equity" + "debt" ‚âà "bond"

Avantages:
- Capture les relations s√©mantiques
- R√©duit la dimensionnalit√©
- Permet le transfer learning
"""
from gensim.models import Word2Vec, KeyedVectors
import numpy as np
import pandas as pd

def train_word2vec(sentences, vector_size=100, window=5, min_count=5,
                   workers=4, epochs=10):
    """
    Entra√Æne un mod√®le Word2Vec.
    
    Args:
        sentences: Liste de phrases tokenis√©es
        vector_size: Dimension des vecteurs
        window: Taille de la fen√™tre de contexte
        min_count: Fr√©quence minimum
        workers: Nombre de workers
        epochs: Nombre d'√©poques
    
    Returns:
        Word2Vec: Mod√®le entra√Æn√©
    """
    model = Word2Vec(
        sentences=sentences,
        vector_size=vector_size,
        window=window,
        min_count=min_count,
        workers=workers,
        epochs=epochs,
        sg=1,  # Skip-gram (1) vs CBOW (0)
        seed=42
    )
    
    return model


def load_pretrained_embeddings(path):
    """
    Charge des embeddings pr√©-entra√Æn√©s.
    
    Args:
        path: Chemin vers le fichier (GloVe, Word2Vec, etc.)
    
    Returns:
        KeyedVectors: Vecteurs de mots
    """
    # Pour GloVe format texte:
    # from gensim.scripts.glove2word2vec import glove2word2vec
    # glove2word2vec('glove.6B.100d.txt', 'glove.6B.100d.w2v.txt')
    
    return KeyedVectors.load_word2vec_format(path, binary=False)


class FinancialWordEmbeddings:
    """
    Word embeddings sp√©cialis√©s pour la finance.
    """
    
    def __init__(self, model):
        """
        Initialise avec un mod√®le Word2Vec.
        
        Args:
            model: Word2Vec ou KeyedVectors
        """
        if isinstance(model, Word2Vec):
            self.wv = model.wv
        else:
            self.wv = model
    
    def get_vector(self, word):
        """Retourne le vecteur d'un mot."""
        try:
            return self.wv[word]
        except KeyError:
            return None
    
    def most_similar(self, word, topn=10):
        """
        Trouve les mots les plus similaires.
        
        Args:
            word: Mot de r√©f√©rence
            topn: Nombre de r√©sultats
        
        Returns:
            list: Mots similaires avec scores
        """
        try:
            return self.wv.most_similar(word, topn=topn)
        except KeyError:
            return []
    
    def analogy(self, positive, negative, topn=5):
        """
        R√©sout une analogie: positive[0] - negative[0] + positive[1] = ?
        
        Exemple: king - man + woman = queen
        
        Args:
            positive: Liste de mots √† ajouter
            negative: Liste de mots √† soustraire
            topn: Nombre de r√©sultats
        
        Returns:
            list: R√©sultats de l'analogie
        """
        try:
            return self.wv.most_similar(
                positive=positive,
                negative=negative,
                topn=topn
            )
        except KeyError:
            return []
    
    def document_vector(self, tokens, method='mean'):
        """
        Calcule le vecteur d'un document.
        
        Args:
            tokens: Liste de mots
            method: 'mean' ou 'sum'
        
        Returns:
            array: Vecteur du document
        """
        vectors = []
        for token in tokens:
            vec = self.get_vector(token)
            if vec is not None:
                vectors.append(vec)
        
        if not vectors:
            return np.zeros(self.wv.vector_size)
        
        vectors = np.array(vectors)
        
        if method == 'mean':
            return vectors.mean(axis=0)
        return vectors.sum(axis=0)
    
    def similarity(self, word1, word2):
        """Calcule la similarit√© cosinus entre deux mots."""
        try:
            return self.wv.similarity(word1, word2)
        except KeyError:
            return 0.0


# === Doc2Vec pour documents ===
from gensim.models import Doc2Vec
from gensim.models.doc2vec import TaggedDocument

def train_doc2vec(documents, vector_size=100, epochs=20):
    """
    Entra√Æne Doc2Vec pour repr√©senter des documents entiers.
    
    Args:
        documents: Liste de (tokens, tag)
        vector_size: Dimension des vecteurs
        epochs: Nombre d'√©poques
    
    Returns:
        Doc2Vec: Mod√®le entra√Æn√©
    """
    # Pr√©parer les documents tagu√©s
    tagged_docs = [
        TaggedDocument(words=tokens, tags=[str(i)])
        for i, tokens in enumerate(documents)
    ]
    
    model = Doc2Vec(
        documents=tagged_docs,
        vector_size=vector_size,
        window=5,
        min_count=5,
        workers=4,
        epochs=epochs,
        dm=1  # Distributed Memory (1) vs DBOW (0)
    )
    
    return model


# === Exemple ===
def demonstrate_embeddings():
    """D√©montre les word embeddings."""
    
    # Corpus d'exemple (phrases tokenis√©es)
    sentences = [
        ['stock', 'price', 'increased', 'today'],
        ['bond', 'yields', 'fell', 'sharply'],
        ['fed', 'raised', 'interest', 'rates'],
        ['earnings', 'beat', 'expectations'],
        ['market', 'crashed', 'on', 'news'],
        ['investors', 'bought', 'stocks'],
        ['company', 'announced', 'dividend'],
        ['volatility', 'increased', 'significantly'],
    ] * 100  # R√©p√©ter pour avoir assez de donn√©es
    
    # Entra√Æner Word2Vec
    print("Entra√Ænement Word2Vec...")
    model = train_word2vec(sentences, vector_size=50, epochs=20)
    
    embeddings = FinancialWordEmbeddings(model)
    
    print("\nMots similaires √† 'stock':")
    for word, score in embeddings.most_similar('stock', topn=5):
        print(f"  {word}: {score:.3f}")
    
    print(f"\nSimilarit√© 'stock' - 'market': {embeddings.similarity('stock', 'market'):.3f}")
    
    # Vecteur de document
    doc = ['stock', 'price', 'increased']
    doc_vec = embeddings.document_vector(doc)
    print(f"\nVecteur du document (dim {len(doc_vec)}): {doc_vec[:5]}...")
    
    return model, embeddings


# model, embeddings = demonstrate_embeddings()
```

---

# 17. DEEP LEARNING - R√âSEAUX FEEDFORWARD
## Deep Learning - Feedforward Networks

## 17.1 Introduction au Deep Learning

```python
"""
Deep Learning pour le Trading
=============================
Les r√©seaux de neurones profonds peuvent capturer des patterns
complexes et non-lin√©aires dans les donn√©es financi√®res.

Architecture de base:
Input ‚Üí Hidden Layers ‚Üí Output

Chaque couche: z = W¬∑x + b, a = activation(z)
"""
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, Model
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

class TradingNeuralNetwork:
    """
    R√©seau de neurones pour pr√©diction de trading.
    """
    
    def __init__(self, input_dim, task='classification', 
                 hidden_layers=[64, 32], dropout_rate=0.3,
                 learning_rate=0.001):
        """
        Initialise le r√©seau.
        
        Args:
            input_dim: Dimension d'entr√©e
            task: 'classification' ou 'regression'
            hidden_layers: Liste des tailles de couches cach√©es
            dropout_rate: Taux de dropout
            learning_rate: Taux d'apprentissage
        """
        self.input_dim = input_dim
        self.task = task
        self.hidden_layers = hidden_layers
        self.dropout_rate = dropout_rate
        self.learning_rate = learning_rate
        
        self.model = self._build_model()
    
    def _build_model(self):
        """Construit le mod√®le Keras."""
        model = keras.Sequential()
        
        # Premi√®re couche
        model.add(layers.Dense(
            self.hidden_layers[0],
            activation='relu',
            input_shape=(self.input_dim,)
        ))
        model.add(layers.BatchNormalization())
        model.add(layers.Dropout(self.dropout_rate))
        
        # Couches cach√©es
        for units in self.hidden_layers[1:]:
            model.add(layers.Dense(units, activation='relu'))
            model.add(layers.BatchNormalization())
            model.add(layers.Dropout(self.dropout_rate))
        
        # Couche de sortie
        if self.task == 'classification':
            model.add(layers.Dense(1, activation='sigmoid'))
            loss = 'binary_crossentropy'
            metrics = ['accuracy', keras.metrics.AUC(name='auc')]
        else:
            model.add(layers.Dense(1, activation='linear'))
            loss = 'mse'
            metrics = ['mae']
        
        # Compiler
        optimizer = keras.optimizers.Adam(learning_rate=self.learning_rate)
        model.compile(optimizer=optimizer, loss=loss, metrics=metrics)
        
        return model
    
    def fit(self, X_train, y_train, X_val=None, y_val=None,
            epochs=100, batch_size=32, early_stopping=True):
        """
        Entra√Æne le mod√®le.
        
        Args:
            X_train: Features d'entra√Ænement
            y_train: Target d'entra√Ænement
            X_val: Features de validation
            y_val: Target de validation
            epochs: Nombre d'√©poques
            batch_size: Taille des batches
            early_stopping: Arr√™t anticip√©
        """
        callbacks = []
        
        if early_stopping:
            callbacks.append(EarlyStopping(
                monitor='val_loss' if X_val is not None else 'loss',
                patience=10,
                restore_best_weights=True
            ))
        
        validation_data = (X_val, y_val) if X_val is not None else None
        
        history = self.model.fit(
            X_train, y_train,
            validation_data=validation_data,
            epochs=epochs,
            batch_size=batch_size,
            callbacks=callbacks,
            verbose=1
        )
        
        return history
    
    def predict(self, X):
        """Pr√©dit."""
        preds = self.model.predict(X)
        
        if self.task == 'classification':
            return (preds > 0.5).astype(int).flatten()
        return preds.flatten()
    
    def predict_proba(self, X):
        """Pr√©dit les probabilit√©s."""
        return self.model.predict(X).flatten()
    
    def evaluate(self, X, y):
        """√âvalue le mod√®le."""
        return self.model.evaluate(X, y, verbose=0)
    
    def summary(self):
        """Affiche le r√©sum√© du mod√®le."""
        return self.model.summary()


# === PyTorch Alternative ===
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

class TradingNetPyTorch(nn.Module):
    """
    R√©seau de neurones PyTorch pour le trading.
    """
    
    def __init__(self, input_dim, hidden_layers=[64, 32], dropout_rate=0.3):
        super().__init__()
        
        layers_list = []
        prev_dim = input_dim
        
        for hidden_dim in hidden_layers:
            layers_list.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.BatchNorm1d(hidden_dim),
                nn.ReLU(),
                nn.Dropout(dropout_rate)
            ])
            prev_dim = hidden_dim
        
        layers_list.append(nn.Linear(prev_dim, 1))
        layers_list.append(nn.Sigmoid())
        
        self.network = nn.Sequential(*layers_list)
    
    def forward(self, x):
        return self.network(x)


def train_pytorch_model(model, X_train, y_train, X_val=None, y_val=None,
                        epochs=100, batch_size=32, lr=0.001):
    """
    Entra√Æne un mod√®le PyTorch.
    
    Args:
        model: Mod√®le PyTorch
        X_train, y_train: Donn√©es d'entra√Ænement
        X_val, y_val: Donn√©es de validation
        epochs: Nombre d'√©poques
        batch_size: Taille des batches
        lr: Learning rate
    
    Returns:
        dict: Historique d'entra√Ænement
    """
    # Convertir en tenseurs
    X_train_t = torch.FloatTensor(X_train)
    y_train_t = torch.FloatTensor(y_train).unsqueeze(1)
    
    train_dataset = TensorDataset(X_train_t, y_train_t)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    
    # Optimiseur et loss
    optimizer = optim.Adam(model.parameters(), lr=lr)
    criterion = nn.BCELoss()
    
    history = {'train_loss': [], 'val_loss': []}
    
    for epoch in range(epochs):
        model.train()
        train_loss = 0
        
        for X_batch, y_batch in train_loader:
            optimizer.zero_grad()
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()
        
        train_loss /= len(train_loader)
        history['train_loss'].append(train_loss)
        
        # Validation
        if X_val is not None:
            model.eval()
            with torch.no_grad():
                X_val_t = torch.FloatTensor(X_val)
                y_val_t = torch.FloatTensor(y_val).unsqueeze(1)
                val_outputs = model(X_val_t)
                val_loss = criterion(val_outputs, y_val_t).item()
                history['val_loss'].append(val_loss)
        
        if (epoch + 1) % 10 == 0:
            print(f"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}", end='')
            if X_val is not None:
                print(f", Val Loss: {val_loss:.4f}")
            else:
                print()
    
    return history


# === Exemple ===
def demonstrate_neural_network():
    """D√©montre les r√©seaux de neurones pour le trading."""
    np.random.seed(42)
    n = 5000
    
    # Cr√©er des donn√©es
    X = np.random.randn(n, 10)
    y = ((X[:, 0] > 0.5) & (X[:, 1] < 0) | (X[:, 2] > 1)).astype(int)
    
    # Split
    train_size = int(0.7 * n)
    val_size = int(0.15 * n)
    
    X_train = X[:train_size]
    X_val = X[train_size:train_size+val_size]
    X_test = X[train_size+val_size:]
    
    y_train = y[:train_size]
    y_val = y[train_size:train_size+val_size]
    y_test = y[train_size+val_size:]
    
    # Normaliser
    from sklearn.preprocessing import StandardScaler
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_val = scaler.transform(X_val)
    X_test = scaler.transform(X_test)
    
    # Entra√Æner (Keras)
    print("Entra√Ænement Keras...")
    nn = TradingNeuralNetwork(
        input_dim=10,
        task='classification',
        hidden_layers=[64, 32, 16]
    )
    nn.summary()
    
    history = nn.fit(X_train, y_train, X_val, y_val, epochs=50)
    
    # √âvaluer
    from sklearn.metrics import accuracy_score, roc_auc_score
    
    y_pred = nn.predict(X_test)
    y_proba = nn.predict_proba(X_test)
    
    print(f"\nR√©sultats:")
    print(f"  Accuracy: {accuracy_score(y_test, y_pred):.2%}")
    print(f"  ROC AUC: {roc_auc_score(y_test, y_proba):.4f}")
    
    return nn, history


# nn, history = demonstrate_neural_network()
```

---

# 18. R√âSEAUX DE NEURONES CONVOLUTIONNELS (CNN)
## Convolutional Neural Networks

## 18.1 CNN pour S√©ries Temporelles Financi√®res

```python
"""
CNN pour le Trading
===================
Les CNN peuvent extraire des patterns locaux dans les s√©ries temporelles
en traitant les donn√©es comme des "images" 1D ou 2D.

Applications:
- D√©tection de patterns techniques (head & shoulders, etc.)
- Extraction de features √† partir de donn√©es OHLCV
- Classification d'images de graphiques
"""
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import layers, Model
from tensorflow.keras.callbacks import EarlyStopping

class CNNTimeSeries:
    """
    CNN pour s√©ries temporelles financi√®res.
    """
    
    def __init__(self, sequence_length, n_features, task='classification',
                 conv_filters=[64, 128], kernel_size=3, pool_size=2,
                 dense_units=[64], dropout_rate=0.3):
        """
        Initialise le CNN.
        
        Args:
            sequence_length: Longueur de la s√©quence
            n_features: Nombre de features (ex: OHLCV = 5)
            task: 'classification' ou 'regression'
            conv_filters: Filtres par couche conv
            kernel_size: Taille du kernel
            pool_size: Taille du pooling
            dense_units: Unit√©s des couches denses
            dropout_rate: Taux de dropout
        """
        self.sequence_length = sequence_length
        self.n_features = n_features
        self.task = task
        
        self.model = self._build_model(
            conv_filters, kernel_size, pool_size, 
            dense_units, dropout_rate
        )
    
    def _build_model(self, conv_filters, kernel_size, pool_size,
                     dense_units, dropout_rate):
        """Construit le mod√®le CNN."""
        model = tf.keras.Sequential()
        
        # Input shape: (sequence_length, n_features)
        model.add(layers.Input(shape=(self.sequence_length, self.n_features)))
        
        # Couches convolutionnelles
        for i, filters in enumerate(conv_filters):
            model.add(layers.Conv1D(
                filters=filters,
                kernel_size=kernel_size,
                padding='same',
                activation='relu'
            ))
            model.add(layers.BatchNormalization())
            model.add(layers.MaxPooling1D(pool_size=pool_size))
            model.add(layers.Dropout(dropout_rate))
        
        # Flatten
        model.add(layers.GlobalAveragePooling1D())
        
        # Couches denses
        for units in dense_units:
            model.add(layers.Dense(units, activation='relu'))
            model.add(layers.Dropout(dropout_rate))
        
        # Output
        if self.task == 'classification':
            model.add(layers.Dense(1, activation='sigmoid'))
            loss = 'binary_crossentropy'
            metrics = ['accuracy']
        else:
            model.add(layers.Dense(1, activation='linear'))
            loss = 'mse'
            metrics = ['mae']
        
        model.compile(
            optimizer=tf.keras.optimizers.Adam(0.001),
            loss=loss,
            metrics=metrics
        )
        
        return model
    
    def prepare_data(self, data, target_col, lookback):
        """
        Pr√©pare les donn√©es pour le CNN.
        
        Args:
            data: DataFrame OHLCV
            target_col: Nom de la colonne cible
            lookback: Fen√™tre de lookback
        
        Returns:
            tuple: (X, y)
        """
        X, y = [], []
        
        feature_cols = [c for c in data.columns if c != target_col]
        
        for i in range(lookback, len(data)):
            X.append(data[feature_cols].iloc[i-lookback:i].values)
            y.append(data[target_col].iloc[i])
        
        return np.array(X), np.array(y)
    
    def fit(self, X_train, y_train, X_val=None, y_val=None,
            epochs=100, batch_size=32):
        """Entra√Æne le mod√®le."""
        callbacks = [
            EarlyStopping(
                monitor='val_loss' if X_val is not None else 'loss',
                patience=10,
                restore_best_weights=True
            )
        ]
        
        validation_data = (X_val, y_val) if X_val is not None else None
        
        history = self.model.fit(
            X_train, y_train,
            validation_data=validation_data,
            epochs=epochs,
            batch_size=batch_size,
            callbacks=callbacks,
            verbose=1
        )
        
        return history
    
    def predict(self, X):
        """Pr√©dit."""
        preds = self.model.predict(X)
        if self.task == 'classification':
            return (preds > 0.5).astype(int).flatten()
        return preds.flatten()
    
    def predict_proba(self, X):
        """Pr√©dit les probabilit√©s."""
        return self.model.predict(X).flatten()


def create_image_from_ohlcv(ohlcv_data, image_size=(64, 64)):
    """
    Convertit des donn√©es OHLCV en image pour CNN 2D.
    
    Args:
        ohlcv_data: DataFrame avec colonnes OHLCV
        image_size: Taille de l'image (height, width)
    
    Returns:
        array: Image normalis√©e
    """
    import cv2
    
    # Normaliser les donn√©es
    normalized = (ohlcv_data - ohlcv_data.min()) / (ohlcv_data.max() - ohlcv_data.min())
    
    # Cr√©er l'image
    n_bars = len(ohlcv_data)
    bar_width = image_size[1] // n_bars
    
    image = np.zeros(image_size)
    
    for i, (_, row) in enumerate(normalized.iterrows()):
        x = i * bar_width
        
        # Dessiner la bougie
        open_y = int((1 - row['open']) * image_size[0])
        close_y = int((1 - row['close']) * image_size[0])
        high_y = int((1 - row['high']) * image_size[0])
        low_y = int((1 - row['low']) * image_size[0])
        
        # M√®che
        image[high_y:low_y, x:x+bar_width//3] = 0.5
        
        # Corps
        body_top = min(open_y, close_y)
        body_bottom = max(open_y, close_y)
        color = 1.0 if close_y < open_y else 0.3  # Vert si hausse
        image[body_top:body_bottom, x:x+bar_width] = color
    
    return image


# === Exemple ===
def demonstrate_cnn_trading():
    """D√©montre CNN pour le trading."""
    np.random.seed(42)
    
    # Cr√©er des donn√©es OHLCV synth√©tiques
    n = 2000
    dates = pd.date_range('2020-01-01', periods=n, freq='D')
    
    close = np.cumsum(np.random.randn(n) * 0.02) + 100
    data = pd.DataFrame({
        'open': close + np.random.randn(n) * 0.5,
        'high': close + np.abs(np.random.randn(n)),
        'low': close - np.abs(np.random.randn(n)),
        'close': close,
        'volume': np.random.randint(1000000, 10000000, n)
    }, index=dates)
    
    # Normaliser
    data_norm = (data - data.mean()) / data.std()
    
    # Target: rendement positif le lendemain
    data_norm['target'] = (data['close'].pct_change().shift(-1) > 0).astype(int)
    data_norm = data_norm.dropna()
    
    # Pr√©parer les s√©quences
    lookback = 20
    cnn = CNNTimeSeries(
        sequence_length=lookback,
        n_features=5,
        task='classification'
    )
    
    X, y = cnn.prepare_data(data_norm, 'target', lookback)
    
    # Split
    train_size = int(0.8 * len(X))
    X_train, X_test = X[:train_size], X[train_size:]
    y_train, y_test = y[:train_size], y[train_size:]
    
    # Entra√Æner
    print("Entra√Ænement du CNN...")
    cnn.model.summary()
    history = cnn.fit(X_train, y_train, epochs=30, batch_size=32)
    
    # √âvaluer
    from sklearn.metrics import accuracy_score, roc_auc_score
    
    y_pred = cnn.predict(X_test)
    y_proba = cnn.predict_proba(X_test)
    
    print(f"\nR√©sultats CNN:")
    print(f"  Accuracy: {accuracy_score(y_test, y_pred):.2%}")
    print(f"  ROC AUC: {roc_auc_score(y_test, y_proba):.4f}")
    
    return cnn


# cnn = demonstrate_cnn_trading()
```

---

# 19. R√âSEAUX DE NEURONES R√âCURRENTS (RNN)
## Recurrent Neural Networks

## 19.1 LSTM pour S√©ries Temporelles

```python
"""
LSTM pour Pr√©diction Financi√®re
===============================
Les LSTM (Long Short-Term Memory) sont con√ßus pour capturer
les d√©pendances √† long terme dans les s√©ries temporelles.

Structure d'une cellule LSTM:
- Forget gate: Quoi oublier de l'√©tat pr√©c√©dent
- Input gate: Quoi ajouter √† l'√©tat
- Output gate: Quoi outputter

Avantages pour la finance:
- Capture les patterns temporels complexes
- G√®re les d√©pendances √† long terme
- Robuste au vanishing gradient
"""
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import layers, Model
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

class LSTMTradingModel:
    """
    LSTM pour pr√©diction de trading.
    """
    
    def __init__(self, sequence_length, n_features, task='regression',
                 lstm_units=[64, 32], dense_units=[16], dropout_rate=0.2,
                 recurrent_dropout=0.2):
        """
        Initialise le LSTM.
        
        Args:
            sequence_length: Longueur de la s√©quence d'entr√©e
            n_features: Nombre de features
            task: 'classification' ou 'regression'
            lstm_units: Unit√©s par couche LSTM
            dense_units: Unit√©s des couches denses
            dropout_rate: Dropout standard
            recurrent_dropout: Dropout r√©current
        """
        self.sequence_length = sequence_length
        self.n_features = n_features
        self.task = task
        
        self.model = self._build_model(
            lstm_units, dense_units, dropout_rate, recurrent_dropout
        )
    
    def _build_model(self, lstm_units, dense_units, dropout_rate, recurrent_dropout):
        """Construit le mod√®le LSTM."""
        model = tf.keras.Sequential()
        
        # Input
        model.add(layers.Input(shape=(self.sequence_length, self.n_features)))
        
        # Couches LSTM
        for i, units in enumerate(lstm_units):
            return_sequences = i < len(lstm_units) - 1
            model.add(layers.LSTM(
                units,
                return_sequences=return_sequences,
                dropout=dropout_rate,
                recurrent_dropout=recurrent_dropout
            ))
        
        # Couches denses
        for units in dense_units:
            model.add(layers.Dense(units, activation='relu'))
            model.add(layers.Dropout(dropout_rate))
        
        # Output
        if self.task == 'classification':
            model.add(layers.Dense(1, activation='sigmoid'))
            loss = 'binary_crossentropy'
            metrics = ['accuracy']
        else:
            model.add(layers.Dense(1, activation='linear'))
            loss = 'mse'
            metrics = ['mae']
        
        model.compile(
            optimizer=tf.keras.optimizers.Adam(0.001),
            loss=loss,
            metrics=metrics
        )
        
        return model
    
    def create_sequences(self, data, target=None):
        """
        Cr√©e des s√©quences pour le LSTM.
        
        Args:
            data: array de features (n_samples, n_features)
            target: array de target (optionnel)
        
        Returns:
            tuple: (X, y) ou X seul
        """
        X = []
        
        for i in range(self.sequence_length, len(data)):
            X.append(data[i-self.sequence_length:i])
        
        X = np.array(X)
        
        if target is not None:
            y = target[self.sequence_length:]
            return X, y
        
        return X
    
    def fit(self, X_train, y_train, X_val=None, y_val=None,
            epochs=100, batch_size=32):
        """Entra√Æne le mod√®le."""
        callbacks = [
            EarlyStopping(
                monitor='val_loss' if X_val is not None else 'loss',
                patience=15,
                restore_best_weights=True
            ),
            ReduceLROnPlateau(
                monitor='val_loss' if X_val is not None else 'loss',
                factor=0.5,
                patience=5,
                min_lr=1e-6
            )
        ]
        
        validation_data = (X_val, y_val) if X_val is not None else None
        
        history = self.model.fit(
            X_train, y_train,
            validation_data=validation_data,
            epochs=epochs,
            batch_size=batch_size,
            callbacks=callbacks,
            verbose=1
        )
        
        return history
    
    def predict(self, X):
        """Pr√©dit."""
        preds = self.model.predict(X)
        if self.task == 'classification':
            return (preds > 0.5).astype(int).flatten()
        return preds.flatten()
    
    def predict_proba(self, X):
        """Pr√©dit les probabilit√©s."""
        return self.model.predict(X).flatten()


class BidirectionalLSTM:
    """
    LSTM bidirectionnel pour une meilleure compr√©hension du contexte.
    """
    
    def __init__(self, sequence_length, n_features, task='classification',
                 lstm_units=64, dense_units=[32]):
        self.sequence_length = sequence_length
        self.n_features = n_features
        self.task = task
        
        self.model = self._build_model(lstm_units, dense_units)
    
    def _build_model(self, lstm_units, dense_units):
        """Construit le mod√®le BiLSTM."""
        inputs = layers.Input(shape=(self.sequence_length, self.n_features))
        
        # BiLSTM
        x = layers.Bidirectional(layers.LSTM(lstm_units, return_sequences=True))(inputs)
        x = layers.Bidirectional(layers.LSTM(lstm_units // 2))(x)
        
        # Dense
        for units in dense_units:
            x = layers.Dense(units, activation='relu')(x)
            x = layers.Dropout(0.3)(x)
        
        # Output
        if self.task == 'classification':
            outputs = layers.Dense(1, activation='sigmoid')(x)
            loss = 'binary_crossentropy'
        else:
            outputs = layers.Dense(1)(x)
            loss = 'mse'
        
        model = Model(inputs, outputs)
        model.compile(optimizer='adam', loss=loss, metrics=['accuracy' if self.task == 'classification' else 'mae'])
        
        return model


# === Exemple ===
def demonstrate_lstm_trading():
    """D√©montre LSTM pour le trading."""
    np.random.seed(42)
    
    # Cr√©er des donn√©es
    n = 2000
    
    # Features: returns, volatility, momentum
    returns = np.random.randn(n) * 0.02
    volatility = np.abs(np.random.randn(n)) * 0.1 + 0.1
    momentum = np.convolve(returns, np.ones(5)/5, mode='same')
    
    data = np.column_stack([returns, volatility, momentum])
    
    # Target: direction du prochain return
    target = (np.roll(returns, -1) > 0).astype(int)
    
    # Normaliser
    from sklearn.preprocessing import StandardScaler
    scaler = StandardScaler()
    data_scaled = scaler.fit_transform(data)
    
    # Cr√©er le mod√®le
    sequence_length = 20
    lstm = LSTMTradingModel(
        sequence_length=sequence_length,
        n_features=3,
        task='classification',
        lstm_units=[64, 32]
    )
    
    # Cr√©er les s√©quences
    X, y = lstm.create_sequences(data_scaled, target)
    
    # Split temporel
    train_size = int(0.8 * len(X))
    X_train, X_test = X[:train_size], X[train_size:]
    y_train, y_test = y[:train_size], y[train_size:]
    
    # Entra√Æner
    print("Entra√Ænement du LSTM...")
    lstm.model.summary()
    history = lstm.fit(X_train, y_train, epochs=30, batch_size=32)
    
    # √âvaluer
    from sklearn.metrics import accuracy_score, roc_auc_score
    
    y_pred = lstm.predict(X_test)
    y_proba = lstm.predict_proba(X_test)
    
    print(f"\nR√©sultats LSTM:")
    print(f"  Accuracy: {accuracy_score(y_test, y_pred):.2%}")
    print(f"  ROC AUC: {roc_auc_score(y_test, y_proba):.4f}")
    
    return lstm


# lstm = demonstrate_lstm_trading()
```

---

# 20. AUTOENCODEURS
## Autoencoders

## 20.1 Autoencodeur pour Feature Learning

```python
"""
Autoencodeurs pour la Finance
=============================
Les autoencodeurs apprennent des repr√©sentations compress√©es (latentes)
des donn√©es en les encodant puis d√©codant.

Applications:
- R√©duction de dimensionnalit√© non-lin√©aire
- D√©tection d'anomalies
- D√©bruitage (denoising)
- G√©n√©ration de features
"""
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import layers, Model

class FinancialAutoencoder:
    """
    Autoencodeur pour donn√©es financi√®res.
    """
    
    def __init__(self, input_dim, encoding_dim=32, hidden_layers=[64]):
        """
        Initialise l'autoencodeur.
        
        Args:
            input_dim: Dimension d'entr√©e
            encoding_dim: Dimension de l'espace latent
            hidden_layers: Couches cach√©es entre input et encoding
        """
        self.input_dim = input_dim
        self.encoding_dim = encoding_dim
        
        self.autoencoder, self.encoder, self.decoder = self._build_model(hidden_layers)
    
    def _build_model(self, hidden_layers):
        """Construit l'autoencodeur."""
        # Encoder
        inputs = layers.Input(shape=(self.input_dim,))
        x = inputs
        
        for units in hidden_layers:
            x = layers.Dense(units, activation='relu')(x)
            x = layers.BatchNormalization()(x)
        
        encoded = layers.Dense(self.encoding_dim, activation='relu', name='encoding')(x)
        
        # Decoder
        x = encoded
        for units in reversed(hidden_layers):
            x = layers.Dense(units, activation='relu')(x)
            x = layers.BatchNormalization()(x)
        
        decoded = layers.Dense(self.input_dim, activation='linear')(x)
        
        # Mod√®les
        autoencoder = Model(inputs, decoded, name='autoencoder')
        encoder = Model(inputs, encoded, name='encoder')
        
        # Decoder s√©par√©
        decoder_input = layers.Input(shape=(self.encoding_dim,))
        decoder_layers = autoencoder.layers[len(hidden_layers)+3:]  # Skip encoder layers
        x = decoder_input
        for layer in decoder_layers:
            x = layer(x)
        decoder = Model(decoder_input, x, name='decoder')
        
        # Compiler
        autoencoder.compile(optimizer='adam', loss='mse')
        
        return autoencoder, encoder, decoder
    
    def fit(self, X, validation_split=0.1, epochs=100, batch_size=32):
        """Entra√Æne l'autoencodeur."""
        from tensorflow.keras.callbacks import EarlyStopping
        
        callbacks = [
            EarlyStopping(patience=10, restore_best_weights=True)
        ]
        
        history = self.autoencoder.fit(
            X, X,  # Input = Output
            validation_split=validation_split,
            epochs=epochs,
            batch_size=batch_size,
            callbacks=callbacks,
            verbose=1
        )
        
        return history
    
    def encode(self, X):
        """Encode les donn√©es dans l'espace latent."""
        return self.encoder.predict(X)
    
    def decode(self, Z):
        """D√©code depuis l'espace latent."""
        return self.decoder.predict(Z)
    
    def reconstruct(self, X):
        """Reconstruit les donn√©es."""
        return self.autoencoder.predict(X)
    
    def reconstruction_error(self, X):
        """
        Calcule l'erreur de reconstruction.
        
        Utile pour la d√©tection d'anomalies.
        """
        reconstructed = self.reconstruct(X)
        return np.mean((X - reconstructed) ** 2, axis=1)


class VariationalAutoencoder:
    """
    Variational Autoencoder (VAE) pour g√©n√©ration de donn√©es.
    
    Le VAE apprend une distribution dans l'espace latent,
    permettant de g√©n√©rer de nouvelles donn√©es.
    """
    
    def __init__(self, input_dim, latent_dim=16, hidden_layers=[64, 32]):
        self.input_dim = input_dim
        self.latent_dim = latent_dim
        
        self.encoder, self.decoder, self.vae = self._build_model(hidden_layers)
    
    def _sampling(self, args):
        """Reparameterization trick: z = mu + sigma * epsilon"""
        z_mean, z_log_var = args
        batch = tf.shape(z_mean)[0]
        dim = tf.shape(z_mean)[1]
        epsilon = tf.random.normal(shape=(batch, dim))
        return z_mean + tf.exp(0.5 * z_log_var) * epsilon
    
    def _build_model(self, hidden_layers):
        """Construit le VAE."""
        # Encoder
        inputs = layers.Input(shape=(self.input_dim,))
        x = inputs
        for units in hidden_layers:
            x = layers.Dense(units, activation='relu')(x)
        
        z_mean = layers.Dense(self.latent_dim, name='z_mean')(x)
        z_log_var = layers.Dense(self.latent_dim, name='z_log_var')(x)
        z = layers.Lambda(self._sampling, name='z')([z_mean, z_log_var])
        
        encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')
        
        # Decoder
        latent_inputs = layers.Input(shape=(self.latent_dim,))
        x = latent_inputs
        for units in reversed(hidden_layers):
            x = layers.Dense(units, activation='relu')(x)
        outputs = layers.Dense(self.input_dim)(x)
        
        decoder = Model(latent_inputs, outputs, name='decoder')
        
        # VAE
        outputs = decoder(encoder(inputs)[2])
        vae = Model(inputs, outputs, name='vae')
        
        # Loss: reconstruction + KL divergence
        reconstruction_loss = tf.reduce_mean(
            tf.keras.losses.mse(inputs, outputs)
        ) * self.input_dim
        
        kl_loss = -0.5 * tf.reduce_mean(
            1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)
        )
        
        vae.add_loss(reconstruction_loss + kl_loss)
        vae.compile(optimizer='adam')
        
        return encoder, decoder, vae
    
    def fit(self, X, epochs=100, batch_size=32):
        """Entra√Æne le VAE."""
        return self.vae.fit(X, epochs=epochs, batch_size=batch_size, verbose=1)
    
    def encode(self, X):
        """Encode dans l'espace latent."""
        z_mean, z_log_var, z = self.encoder.predict(X)
        return z
    
    def generate(self, n_samples):
        """G√©n√®re de nouvelles donn√©es."""
        z = np.random.normal(size=(n_samples, self.latent_dim))
        return self.decoder.predict(z)


# === D√©tection d'anomalies ===
def detect_anomalies_with_autoencoder(autoencoder, X, threshold_percentile=95):
    """
    D√©tecte les anomalies avec l'erreur de reconstruction.
    
    Args:
        autoencoder: Autoencodeur entra√Æn√©
        X: Donn√©es √† analyser
        threshold_percentile: Percentile pour le seuil
    
    Returns:
        tuple: (anomalies, scores)
    """
    # Calculer les erreurs
    errors = autoencoder.reconstruction_error(X)
    
    # D√©finir le seuil
    threshold = np.percentile(errors, threshold_percentile)
    
    # Identifier les anomalies
    anomalies = errors > threshold
    
    return anomalies, errors, threshold


# === Exemple ===
def demonstrate_autoencoder():
    """D√©montre les autoencodeurs."""
    np.random.seed(42)
    
    # Cr√©er des donn√©es
    n = 2000
    n_features = 20
    
    # Donn√©es normales
    X_normal = np.random.randn(n, n_features)
    
    # Ajouter quelques anomalies
    n_anomalies = 50
    X_anomalies = np.random.randn(n_anomalies, n_features) * 3 + 5
    X = np.vstack([X_normal, X_anomalies])
    labels = np.array([0] * n + [1] * n_anomalies)
    
    # Shuffle
    idx = np.random.permutation(len(X))
    X, labels = X[idx], labels[idx]
    
    # Split (entra√Æner sur donn√©es normales uniquement)
    X_train = X_normal[:int(0.8*n)]
    X_test = X
    
    # Entra√Æner l'autoencodeur
    print("Entra√Ænement de l'autoencodeur...")
    ae = FinancialAutoencoder(
        input_dim=n_features,
        encoding_dim=8,
        hidden_layers=[32, 16]
    )
    ae.fit(X_train, epochs=50)
    
    # D√©tecter les anomalies
    anomalies_detected, scores, threshold = detect_anomalies_with_autoencoder(ae, X_test)
    
    # √âvaluer
    from sklearn.metrics import precision_score, recall_score, f1_score
    
    print(f"\nD√©tection d'anomalies:")
    print(f"  Threshold: {threshold:.4f}")
    print(f"  Precision: {precision_score(labels, anomalies_detected):.2%}")
    print(f"  Recall: {recall_score(labels, anomalies_detected):.2%}")
    print(f"  F1: {f1_score(labels, anomalies_detected):.2%}")
    
    return ae


# ae = demonstrate_autoencoder()
```

---

# 21. R√âSEAUX ADVERSES G√âN√âRATIFS (GAN)
## Generative Adversarial Networks

## 21.1 TimeGAN pour Donn√©es Synth√©tiques

```python
"""
TimeGAN pour G√©n√©ration de S√©ries Temporelles Financi√®res
=========================================================
TimeGAN g√©n√®re des s√©ries temporelles synth√©tiques r√©alistes
qui pr√©servent les propri√©t√©s temporelles des donn√©es originales.

Applications:
- Augmentation de donn√©es
- Test de strat√©gies sur sc√©narios synth√©tiques
- Pr√©servation de la confidentialit√©
- Simulation de stress tests
"""
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import layers, Model

class SimpleTimeGAN:
    """
    Version simplifi√©e de TimeGAN.
    
    TimeGAN complet a 4 composants:
    1. Embedding: Encode les s√©quences
    2. Recovery: D√©code les s√©quences
    3. Generator: G√©n√®re des s√©quences latentes
    4. Discriminator: Distingue vrai de faux
    """
    
    def __init__(self, seq_len, n_features, hidden_dim=24, latent_dim=24):
        """
        Initialise TimeGAN.
        
        Args:
            seq_len: Longueur des s√©quences
            n_features: Nombre de features
            hidden_dim: Dimension cach√©e
            latent_dim: Dimension latente
        """
        self.seq_len = seq_len
        self.n_features = n_features
        self.hidden_dim = hidden_dim
        self.latent_dim = latent_dim
        
        self.embedder, self.recovery, self.generator, self.discriminator = self._build_models()
    
    def _build_models(self):
        """Construit les 4 r√©seaux."""
        # Embedder (Real ‚Üí Hidden)
        emb_input = layers.Input(shape=(self.seq_len, self.n_features))
        e = layers.GRU(self.hidden_dim, return_sequences=True)(emb_input)
        e = layers.GRU(self.hidden_dim, return_sequences=True)(e)
        embedder = Model(emb_input, e, name='embedder')
        
        # Recovery (Hidden ‚Üí Real)
        rec_input = layers.Input(shape=(self.seq_len, self.hidden_dim))
        r = layers.GRU(self.hidden_dim, return_sequences=True)(rec_input)
        r = layers.Dense(self.n_features)(r)
        recovery = Model(rec_input, r, name='recovery')
        
        # Generator (Noise ‚Üí Hidden)
        gen_input = layers.Input(shape=(self.seq_len, self.latent_dim))
        g = layers.GRU(self.hidden_dim, return_sequences=True)(gen_input)
        g = layers.GRU(self.hidden_dim, return_sequences=True)(g)
        generator = Model(gen_input, g, name='generator')
        
        # Discriminator (Hidden ‚Üí Real/Fake)
        disc_input = layers.Input(shape=(self.seq_len, self.hidden_dim))
        d = layers.GRU(self.hidden_dim, return_sequences=True)(disc_input)
        d = layers.GRU(self.hidden_dim)(d)
        d = layers.Dense(1, activation='sigmoid')(d)
        discriminator = Model(disc_input, d, name='discriminator')
        
        return embedder, recovery, generator, discriminator
    
    def train(self, real_data, epochs=1000, batch_size=32):
        """
        Entra√Æne TimeGAN.
        
        Args:
            real_data: Donn√©es r√©elles (n_samples, seq_len, n_features)
            epochs: Nombre d'√©poques
            batch_size: Taille des batches
        """
        optimizer_e = tf.keras.optimizers.Adam(0.001)
        optimizer_g = tf.keras.optimizers.Adam(0.001)
        optimizer_d = tf.keras.optimizers.Adam(0.001)
        
        n_samples = len(real_data)
        
        for epoch in range(epochs):
            # Mini-batch
            idx = np.random.randint(0, n_samples, batch_size)
            real_batch = real_data[idx]
            
            # 1. Entra√Æner Embedder + Recovery (reconstruction)
            with tf.GradientTape() as tape:
                h_real = self.embedder(real_batch, training=True)
                x_reconstructed = self.recovery(h_real, training=True)
                e_loss = tf.reduce_mean(tf.square(real_batch - x_reconstructed))
            
            e_vars = self.embedder.trainable_variables + self.recovery.trainable_variables
            grads = tape.gradient(e_loss, e_vars)
            optimizer_e.apply_gradients(zip(grads, e_vars))
            
            # 2. Entra√Æner Generator
            noise = np.random.normal(size=(batch_size, self.seq_len, self.latent_dim))
            
            with tf.GradientTape() as tape:
                h_fake = self.generator(noise, training=True)
                y_fake = self.discriminator(h_fake, training=True)
                g_loss = -tf.reduce_mean(tf.math.log(y_fake + 1e-8))
            
            g_grads = tape.gradient(g_loss, self.generator.trainable_variables)
            optimizer_g.apply_gradients(zip(g_grads, self.generator.trainable_variables))
            
            # 3. Entra√Æner Discriminator
            with tf.GradientTape() as tape:
                h_real = self.embedder(real_batch, training=False)
                h_fake = self.generator(noise, training=False)
                
                y_real = self.discriminator(h_real, training=True)
                y_fake = self.discriminator(h_fake, training=True)
                
                d_loss = -tf.reduce_mean(tf.math.log(y_real + 1e-8) + 
                                        tf.math.log(1 - y_fake + 1e-8))
            
            d_grads = tape.gradient(d_loss, self.discriminator.trainable_variables)
            optimizer_d.apply_gradients(zip(d_grads, self.discriminator.trainable_variables))
            
            if (epoch + 1) % 100 == 0:
                print(f"Epoch {epoch+1}/{epochs} - E_loss: {e_loss:.4f}, G_loss: {g_loss:.4f}, D_loss: {d_loss:.4f}")
    
    def generate(self, n_samples):
        """
        G√©n√®re des s√©quences synth√©tiques.
        
        Args:
            n_samples: Nombre de s√©quences √† g√©n√©rer
        
        Returns:
            array: S√©quences synth√©tiques
        """
        noise = np.random.normal(size=(n_samples, self.seq_len, self.latent_dim))
        h_fake = self.generator.predict(noise)
        x_fake = self.recovery.predict(h_fake)
        return x_fake


def evaluate_synthetic_data(real_data, synthetic_data):
    """
    √âvalue la qualit√© des donn√©es synth√©tiques.
    
    M√©triques:
    - Distribution: Les distributions sont-elles similaires?
    - Diversit√©: Les donn√©es sont-elles vari√©es?
    - Utilit√©: Peuvent-elles entra√Æner des mod√®les?
    """
    from scipy import stats
    
    results = {}
    
    # 1. Comparaison des distributions (par feature)
    n_features = real_data.shape[2]
    ks_stats = []
    
    for f in range(n_features):
        real_flat = real_data[:, :, f].flatten()
        synth_flat = synthetic_data[:, :, f].flatten()
        ks_stat, _ = stats.ks_2samp(real_flat, synth_flat)
        ks_stats.append(ks_stat)
    
    results['ks_statistic_mean'] = np.mean(ks_stats)
    
    # 2. Corr√©lation temporelle
    real_autocorr = np.mean([
        np.corrcoef(real_data[i, :-1, 0], real_data[i, 1:, 0])[0, 1]
        for i in range(len(real_data))
    ])
    
    synth_autocorr = np.mean([
        np.corrcoef(synthetic_data[i, :-1, 0], synthetic_data[i, 1:, 0])[0, 1]
        for i in range(len(synthetic_data))
    ])
    
    results['autocorr_real'] = real_autocorr
    results['autocorr_synthetic'] = synth_autocorr
    results['autocorr_diff'] = abs(real_autocorr - synth_autocorr)
    
    return results


# === Exemple ===
def demonstrate_timegan():
    """D√©montre TimeGAN."""
    np.random.seed(42)
    
    # Cr√©er des donn√©es r√©alistes (marche al√©atoire avec momentum)
    n_samples = 500
    seq_len = 24
    n_features = 3
    
    real_data = []
    for _ in range(n_samples):
        # Simuler une trajectoire de prix
        returns = np.random.randn(seq_len) * 0.02
        returns = np.convolve(returns, [0.3, 0.5, 0.2], mode='same')  # Momentum
        
        price = 100 * np.exp(np.cumsum(returns))
        volume = np.abs(np.random.randn(seq_len)) * 1000000
        volatility = np.abs(returns) * 10
        
        seq = np.column_stack([price, volume, volatility])
        real_data.append(seq)
    
    real_data = np.array(real_data)
    
    # Normaliser
    mean = real_data.mean(axis=(0, 1))
    std = real_data.std(axis=(0, 1))
    real_data_norm = (real_data - mean) / std
    
    # Entra√Æner TimeGAN
    print("Entra√Ænement de TimeGAN...")
    tgan = SimpleTimeGAN(seq_len, n_features, hidden_dim=16, latent_dim=16)
    tgan.train(real_data_norm.astype(np.float32), epochs=500, batch_size=32)
    
    # G√©n√©rer des donn√©es synth√©tiques
    synthetic_data_norm = tgan.generate(100)
    synthetic_data = synthetic_data_norm * std + mean
    
    # √âvaluer
    eval_results = evaluate_synthetic_data(real_data_norm, synthetic_data_norm)
    
    print("\n√âvaluation des donn√©es synth√©tiques:")
    for metric, value in eval_results.items():
        print(f"  {metric}: {value:.4f}")
    
    return tgan, real_data, synthetic_data


# tgan, real_data, synthetic_data = demonstrate_timegan()
```

---

# 22. APPRENTISSAGE PAR RENFORCEMENT
## Reinforcement Learning

## 22.1 Q-Learning pour Trading

```python
"""
Reinforcement Learning pour le Trading
======================================
L'agent RL apprend une politique de trading en interagissant
avec l'environnement (le march√©).

Composants:
- √âtat (State): Repr√©sentation du march√© (features)
- Action: Buy, Hold, Sell
- R√©compense (Reward): P&L, Sharpe, etc.
- Politique (Policy): √âtat ‚Üí Action

Algorithmes:
- Q-Learning: Table de Q-values
- Deep Q-Network (DQN): Neural network pour Q-values
- Policy Gradient: Optimise directement la politique
"""
import numpy as np
import pandas as pd
from collections import deque
import random

class TradingEnvironment:
    """
    Environnement de trading pour RL.
    
    Actions:
        0: HOLD
        1: BUY
        2: SELL
    """
    
    def __init__(self, prices, features, initial_balance=10000,
                 transaction_cost=0.001, max_position=1):
        """
        Initialise l'environnement.
        
        Args:
            prices: Series de prix
            features: DataFrame de features
            initial_balance: Capital initial
            transaction_cost: Co√ªt de transaction
            max_position: Position max (-1 √† 1)
        """
        self.prices = prices.values
        self.features = features.values
        self.initial_balance = initial_balance
        self.transaction_cost = transaction_cost
        self.max_position = max_position
        
        self.n_features = features.shape[1]
        self.n_steps = len(prices)
        
        self.reset()
    
    def reset(self):
        """R√©initialise l'environnement."""
        self.current_step = 0
        self.balance = self.initial_balance
        self.position = 0  # -1, 0, ou 1
        self.portfolio_value = self.initial_balance
        self.done = False
        
        return self._get_state()
    
    def _get_state(self):
        """Retourne l'√©tat actuel."""
        market_state = self.features[self.current_step]
        position_state = np.array([self.position, self.balance / self.initial_balance])
        return np.concatenate([market_state, position_state])
    
    def step(self, action):
        """
        Ex√©cute une action.
        
        Args:
            action: 0 (HOLD), 1 (BUY), 2 (SELL)
        
        Returns:
            tuple: (next_state, reward, done, info)
        """
        current_price = self.prices[self.current_step]
        
        # Ex√©cuter l'action
        old_position = self.position
        
        if action == 1 and self.position < self.max_position:  # BUY
            self.position = min(self.position + 1, self.max_position)
        elif action == 2 and self.position > -self.max_position:  # SELL
            self.position = max(self.position - 1, -self.max_position)
        
        # Co√ªt de transaction si changement de position
        if self.position != old_position:
            transaction_cost = abs(self.position - old_position) * current_price * self.transaction_cost
            self.balance -= transaction_cost
        
        # Avancer d'un pas
        self.current_step += 1
        
        if self.current_step >= self.n_steps - 1:
            self.done = True
        
        # Calculer la r√©compense (P&L)
        if not self.done:
            next_price = self.prices[self.current_step]
            price_change = (next_price - current_price) / current_price
            reward = self.position * price_change  # Position * Return
        else:
            reward = 0
        
        # Mettre √† jour la valeur du portefeuille
        if not self.done:
            self.portfolio_value = self.balance + self.position * self.prices[self.current_step]
        
        next_state = self._get_state() if not self.done else None
        
        info = {
            'portfolio_value': self.portfolio_value,
            'position': self.position,
            'balance': self.balance
        }
        
        return next_state, reward, self.done, info


class DQNAgent:
    """
    Agent Deep Q-Network pour le trading.
    """
    
    def __init__(self, state_size, action_size, learning_rate=0.001,
                 gamma=0.95, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995):
        """
        Initialise l'agent DQN.
        
        Args:
            state_size: Taille de l'√©tat
            action_size: Nombre d'actions
            learning_rate: Taux d'apprentissage
            gamma: Facteur de discount
            epsilon: Exploration initiale
            epsilon_min: Exploration minimale
            epsilon_decay: D√©croissance de l'exploration
        """
        self.state_size = state_size
        self.action_size = action_size
        self.gamma = gamma
        self.epsilon = epsilon
        self.epsilon_min = epsilon_min
        self.epsilon_decay = epsilon_decay
        
        self.memory = deque(maxlen=10000)
        self.model = self._build_model(learning_rate)
    
    def _build_model(self, learning_rate):
        """Construit le r√©seau Q."""
        from tensorflow.keras import layers, Model
        
        inputs = layers.Input(shape=(self.state_size,))
        x = layers.Dense(64, activation='relu')(inputs)
        x = layers.Dense(32, activation='relu')(x)
        outputs = layers.Dense(self.action_size, activation='linear')(x)
        
        model = Model(inputs, outputs)
        model.compile(
            optimizer=tf.keras.optimizers.Adam(learning_rate),
            loss='mse'
        )
        
        return model
    
    def remember(self, state, action, reward, next_state, done):
        """Stocke une exp√©rience."""
        self.memory.append((state, action, reward, next_state, done))
    
    def act(self, state, training=True):
        """
        Choisit une action.
        
        Args:
            state: √âtat actuel
            training: Si True, utilise epsilon-greedy
        
        Returns:
            int: Action choisie
        """
        if training and np.random.rand() <= self.epsilon:
            return random.randrange(self.action_size)
        
        q_values = self.model.predict(state.reshape(1, -1), verbose=0)
        return np.argmax(q_values[0])
    
    def replay(self, batch_size=32):
        """
        Entra√Æne sur un batch d'exp√©riences.
        
        Args:
            batch_size: Taille du batch
        """
        if len(self.memory) < batch_size:
            return
        
        minibatch = random.sample(self.memory, batch_size)
        
        states = np.array([e[0] for e in minibatch])
        actions = np.array([e[1] for e in minibatch])
        rewards = np.array([e[2] for e in minibatch])
        next_states = np.array([e[3] if e[3] is not None else np.zeros(self.state_size) for e in minibatch])
        dones = np.array([e[4] for e in minibatch])
        
        # Q-values actuels
        q_values = self.model.predict(states, verbose=0)
        
        # Q-values cibles
        next_q_values = self.model.predict(next_states, verbose=0)
        
        # Mise √† jour Q-learning
        for i in range(batch_size):
            if dones[i]:
                q_values[i][actions[i]] = rewards[i]
            else:
                q_values[i][actions[i]] = rewards[i] + self.gamma * np.max(next_q_values[i])
        
        self.model.fit(states, q_values, epochs=1, verbose=0)
        
        # D√©croissance de l'exploration
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay


def train_dqn_agent(env, agent, episodes=100, batch_size=32):
    """
    Entra√Æne l'agent DQN.
    
    Args:
        env: Environnement de trading
        agent: Agent DQN
        episodes: Nombre d'√©pisodes
        batch_size: Taille des batches
    
    Returns:
        list: Historique des r√©compenses
    """
    rewards_history = []
    
    for episode in range(episodes):
        state = env.reset()
        total_reward = 0
        
        while True:
            action = agent.act(state)
            next_state, reward, done, info = env.step(action)
            
            agent.remember(state, action, reward, next_state, done)
            total_reward += reward
            
            state = next_state
            
            if done:
                break
            
            agent.replay(batch_size)
        
        rewards_history.append(total_reward)
        
        if (episode + 1) % 10 == 0:
            avg_reward = np.mean(rewards_history[-10:])
            print(f"Episode {episode+1}/{episodes}, Avg Reward: {avg_reward:.4f}, Epsilon: {agent.epsilon:.3f}")
    
    return rewards_history


# === Exemple ===
def demonstrate_rl_trading():
    """D√©montre le RL pour le trading."""
    np.random.seed(42)
    
    # Cr√©er des donn√©es
    n = 500
    
    returns = np.random.randn(n) * 0.02
    prices = 100 * np.exp(np.cumsum(returns))
    
    # Features
    features = pd.DataFrame({
        'return_1d': pd.Series(prices).pct_change().fillna(0),
        'return_5d': pd.Series(prices).pct_change(5).fillna(0),
        'volatility': pd.Series(prices).pct_change().rolling(10).std().fillna(0.01),
        'momentum': (pd.Series(prices).rolling(10).mean() / pd.Series(prices).rolling(30).mean()).fillna(1) - 1
    })
    
    # Cr√©er l'environnement
    env = TradingEnvironment(
        prices=pd.Series(prices),
        features=features,
        initial_balance=10000
    )
    
    # Cr√©er l'agent
    state_size = env.n_features + 2  # Features + position + balance
    action_size = 3  # HOLD, BUY, SELL
    
    agent = DQNAgent(state_size, action_size)
    
    # Entra√Æner
    print("Entra√Ænement de l'agent DQN...")
    rewards = train_dqn_agent(env, agent, episodes=50)
    
    # √âvaluer
    state = env.reset()
    agent.epsilon = 0  # Pas d'exploration
    
    positions = []
    portfolio_values = []
    
    while True:
        action = agent.act(state, training=False)
        next_state, reward, done, info = env.step(action)
        
        positions.append(info['position'])
        portfolio_values.append(info['portfolio_value'])
        
        if done:
            break
        
        state = next_state
    
    # R√©sultats
    final_value = portfolio_values[-1]
    total_return = (final_value - 10000) / 10000
    
    print(f"\nR√©sultats:")
    print(f"  Valeur finale: ${final_value:,.2f}")
    print(f"  Rendement total: {total_return:.2%}")
    
    # Comparer √† Buy & Hold
    buy_hold_return = (prices[-1] - prices[0]) / prices[0]
    print(f"  Buy & Hold: {buy_hold_return:.2%}")
    
    return agent, rewards


# agent, rewards = demonstrate_rl_trading()
```

---

# 23. PROCHAINES √âTAPES ET RESSOURCES

## 23.1 Bonnes Pratiques

```python
"""
Bonnes Pratiques pour le ML en Finance
======================================
"""

# 1. √âVITER LE LOOK-AHEAD BIAS
# Ne jamais utiliser des donn√©es futures pour pr√©dire le pass√©
# - Toujours utiliser TimeSeriesSplit pour la cross-validation
# - D√©caler les features d'au moins 1 p√©riode
# - V√©rifier les timestamps des donn√©es

# 2. G√âRER L'OVERFITTING
# Les donn√©es financi√®res sont bruit√©es et non-stationnaires
# - Utiliser la r√©gularisation (L1, L2, Dropout)
# - Limiter la complexit√© du mod√®le
# - Valider sur des p√©riodes out-of-sample
# - Utiliser le Deflated Sharpe Ratio pour le multiple testing

# 3. CO√õTS DE TRANSACTION
# Toujours inclure les co√ªts r√©alistes
# - Commission du broker
# - Slippage (diff√©rence entre prix pr√©vu et ex√©cut√©)
# - Market impact pour grandes positions

# 4. DONN√âES DE QUALIT√â
# La qualit√© des donn√©es est plus importante que le mod√®le
# - V√©rifier les ajustements (dividendes, splits)
# - G√©rer les survivorship bias
# - Attention aux donn√©es de point-in-time

# 5. INTERPR√âTABILIT√â
# Comprendre pourquoi le mod√®le fonctionne
# - Utiliser SHAP pour l'interpr√©tation
# - V√©rifier que les features ont du sens √©conomique
# - Tester sur diff√©rentes p√©riodes et march√©s
```

## 23.2 Ressources Additionnelles

```markdown
## Livres Recommand√©s
- "Advances in Financial Machine Learning" - Marcos L√≥pez de Prado
- "Machine Learning for Asset Managers" - Marcos L√≥pez de Prado
- "Machine Learning for Algorithmic Trading" - Stefan Jansen
- "Deep Learning" - Ian Goodfellow

## Cours en Ligne
- Coursera: Machine Learning (Andrew Ng)
- Fast.ai: Practical Deep Learning
- Udacity: AI for Trading

## Biblioth√®ques Python
- scikit-learn: ML classique
- TensorFlow/Keras: Deep Learning
- PyTorch: Deep Learning
- LightGBM, CatBoost, XGBoost: Gradient Boosting
- Zipline, Backtrader: Backtesting
- Alphalens, PyFolio: √âvaluation de strat√©gies

## Datasets
- Yahoo Finance (yfinance)
- Quandl
- WRDS (acad√©mique)
- SEC EDGAR (filings)
- Alpha Vantage
```

---

# FIN DU DOCUMENT

Ce document complet couvre l'ensemble du livre "Machine Learning for Algorithmic Trading" 
de Stefan Jansen. Il fournit les impl√©mentations Python d√©taill√©es pour chaque technique
abord√©e, avec des explications en fran√ßais et anglais.

**Total: ~8000 lignes de code et documentation**
