# üìä FinRL - Deep Reinforcement Learning pour la Finance
## Guide Complet pour HelixOne Visual Code

---

# Table des Mati√®res

1. [Introduction au DRL (Deep Reinforcement Learning)](#1-introduction-au-drl)
2. [Installation et Configuration](#2-installation-et-configuration)
3. [Architecture FinRL](#3-architecture-finrl)
4. [Configuration et Param√®tres](#4-configuration-et-param√®tres)
5. [Data Processing - Traitement des Donn√©es](#5-data-processing)
6. [Environnements de Trading](#6-environnements-de-trading)
7. [Agents DRL - Algorithmes](#7-agents-drl)
8. [Pipeline Train-Test-Trade](#8-pipeline-train-test-trade)
9. [Backtesting et Visualisation](#9-backtesting-et-visualisation)
10. [Paper Trading - Trading Simul√©](#10-paper-trading)
11. [Exemples Complets](#11-exemples-complets)
12. [Optimisation des Hyperparam√®tres](#12-optimisation-hyperparametres)
13. [Glossaire DRL](#13-glossaire-drl)

---

# 1. Introduction au DRL

## 1.1 Qu'est-ce que le Deep Reinforcement Learning ?

Le **DRL (Deep Reinforcement Learning)** combine:
- **RL (Reinforcement Learning)** = Apprentissage par Renforcement : un agent apprend en interagissant avec un environnement
- **Deep Learning** = R√©seaux de neurones profonds pour approximer les fonctions de valeur ou les politiques

### Concepts Fondamentaux

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    CYCLE DRL                                ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     Action (a)      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ    ‚îÇ  AGENT  ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫  ‚îÇ  ENVIRONNEMENT  ‚îÇ     ‚îÇ
‚îÇ    ‚îÇ  (DRL)  ‚îÇ                     ‚îÇ    (March√©)     ‚îÇ     ‚îÇ
‚îÇ    ‚îÇ         ‚îÇ ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  ‚îÇ                 ‚îÇ     ‚îÇ
‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   √âtat (s), Reward  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îÇ                       (r)                                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

| Terme | Signification | Exemple Trading |
|-------|---------------|-----------------|
| **Agent** | L'algorithme qui prend des d√©cisions | Notre mod√®le DRL |
| **Environment** | Le monde avec lequel l'agent interagit | Le march√© financier |
| **State (s)** | L'observation actuelle | Prix, indicateurs techniques, positions |
| **Action (a)** | D√©cision de l'agent | Acheter, Vendre, Conserver |
| **Reward (r)** | R√©compense/p√©nalit√© | Profit ou perte r√©alis√© |
| **Policy (œÄ)** | Strat√©gie de l'agent | Fonction qui mappe √©tat ‚Üí action |

## 1.2 Pourquoi le DRL pour le Trading ?

### Avantages
1. **Apprentissage End-to-End** : Pas besoin de r√®gles manuelles
2. **Adaptation** : S'adapte aux conditions de march√© changeantes
3. **Gestion du risque** : Peut int√©grer la turbulence/volatilit√©
4. **Multi-actifs** : G√®re des portefeuilles complexes

### D√©fis
1. **Non-stationnarit√©** : Les march√©s √©voluent
2. **Donn√©es bruit√©es** : Signal/bruit faible
3. **Co√ªts de transaction** : Impact sur les strat√©gies
4. **Overfitting** : Risque de surajustement

## 1.3 Algorithmes DRL Support√©s par FinRL

| Algorithme | Type | Description |
|------------|------|-------------|
| **A2C** | On-Policy | Advantage Actor-Critic (Synchrone) |
| **PPO** | On-Policy | Proximal Policy Optimization |
| **DDPG** | Off-Policy | Deep Deterministic Policy Gradient |
| **TD3** | Off-Policy | Twin Delayed DDPG |
| **SAC** | Off-Policy | Soft Actor-Critic |

### On-Policy vs Off-Policy

- **On-Policy** (A2C, PPO) : Apprend uniquement des actions de la politique actuelle
  - Plus stable mais moins efficient en donn√©es
- **Off-Policy** (DDPG, TD3, SAC) : Peut apprendre d'exp√©riences pass√©es (Replay Buffer)
  - Plus efficient en donn√©es mais moins stable

---

# 2. Installation et Configuration

## 2.1 Installation de Base

```python
# ============================================================
# INSTALLATION FINRL
# ============================================================

# Installation via pip
# Note: N√©cessite Python 3.8+
!pip install finrl

# OU installation depuis GitHub (derni√®re version)
!pip install git+https://github.com/AI4Finance-Foundation/FinRL.git

# D√©pendances suppl√©mentaires
!pip install swig              # Pour box2d (certains environnements)
!pip install wrds              # Wharton Research Data Services
!pip install pyportfolioopt    # Optimisation de portefeuille classique
```

## 2.2 Installation Compl√®te (avec tous les frameworks DRL)

```python
# ============================================================
# INSTALLATION COMPL√àTE AVEC TOUS LES BACKENDS
# ============================================================

# Syst√®me Linux (apt-get)
!apt-get update -y -qq
!apt-get install -y -qq cmake libopenmpi-dev python3-dev zlib1g-dev libgl1-mesa-glx swig

# Stable Baselines 3 (recommand√© pour d√©buter)
!pip install stable-baselines3[extra]

# ElegantRL (performances optimis√©es)
!pip install elegantrl

# Ray RLlib (computing distribu√©)
!pip install "ray[rllib]"

# FinRL
!pip install finrl

# V√©rification de l'installation
import finrl
print(f"FinRL version: {finrl.__version__}")
```

## 2.3 Structure des Imports

```python
# ============================================================
# IMPORTS STANDARD FINRL
# ============================================================

# Biblioth√®ques Python standard
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import datetime
import itertools
from pprint import pprint

# Configuration FinRL
from finrl import config
from finrl import config_tickers
from finrl.config import (
    DATA_SAVE_DIR,        # Dossier de sauvegarde des donn√©es
    TRAINED_MODEL_DIR,    # Dossier des mod√®les entra√Æn√©s
    TENSORBOARD_LOG_DIR,  # Dossier des logs TensorBoard
    RESULTS_DIR,          # Dossier des r√©sultats
    INDICATORS,           # Liste des indicateurs techniques
    TRAIN_START_DATE,     # Date d√©but entra√Ænement
    TRAIN_END_DATE,       # Date fin entra√Ænement
    TEST_START_DATE,      # Date d√©but test
    TEST_END_DATE,        # Date fin test
    TRADE_START_DATE,     # Date d√©but trading
    TRADE_END_DATE,       # Date fin trading
)

# Data Processing
from finrl.meta.preprocessor.yahoodownloader import YahooDownloader
from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split
from finrl.meta.data_processor import DataProcessor

# Environnements
from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv
from finrl.meta.env_stock_trading.env_stocktrading_np import StockTradingEnv as StockTradingEnvNP
from finrl.meta.env_portfolio_allocation.env_portfolio import StockPortfolioEnv

# Agents DRL
from finrl.agents.stablebaselines3.models import DRLAgent
from finrl.agents.elegantrl.models import DRLAgent as DRLAgent_ERL
from finrl.agents.rllib.models import DRLAgent as DRLAgent_RLlib

# Visualisation et Backtesting
from finrl.plot import backtest_stats, backtest_plot, get_daily_return, get_baseline

# Utilitaires
from finrl.main import check_and_make_directories
```

---

# 3. Architecture FinRL

## 3.1 Vue d'Ensemble

FinRL suit une architecture en **3 couches** :

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        APPLICATIONS LAYER                            ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
‚îÇ  ‚îÇStock Trading ‚îÇ ‚îÇ  Portfolio   ‚îÇ ‚îÇ    Crypto    ‚îÇ ‚îÇ     HFT     ‚îÇ‚îÇ
‚îÇ  ‚îÇ              ‚îÇ ‚îÇ Allocation   ‚îÇ ‚îÇ   Trading    ‚îÇ ‚îÇ             ‚îÇ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                          AGENTS LAYER                                ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                 ‚îÇ
‚îÇ  ‚îÇ  ElegantRL   ‚îÇ ‚îÇ    RLlib     ‚îÇ ‚îÇStable Base-  ‚îÇ                 ‚îÇ
‚îÇ  ‚îÇ              ‚îÇ ‚îÇ              ‚îÇ ‚îÇ  lines 3     ‚îÇ                 ‚îÇ
‚îÇ  ‚îÇ  - PPO       ‚îÇ ‚îÇ  - PPO       ‚îÇ ‚îÇ  - A2C       ‚îÇ                 ‚îÇ
‚îÇ  ‚îÇ  - A2C       ‚îÇ ‚îÇ  - A2C       ‚îÇ ‚îÇ  - PPO       ‚îÇ                 ‚îÇ
‚îÇ  ‚îÇ  - SAC       ‚îÇ ‚îÇ  - DDPG      ‚îÇ ‚îÇ  - DDPG      ‚îÇ                 ‚îÇ
‚îÇ  ‚îÇ  - DDPG      ‚îÇ ‚îÇ  - TD3       ‚îÇ ‚îÇ  - TD3       ‚îÇ                 ‚îÇ
‚îÇ  ‚îÇ  - TD3       ‚îÇ ‚îÇ  - SAC       ‚îÇ ‚îÇ  - SAC       ‚îÇ                 ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                      ENVIRONMENT LAYER (Meta)                        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ                     Data Processors                           ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  Yahoo Finance | Alpaca | WRDS | CCXT | Binance | ...        ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ                   Trading Environments                        ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  StockTradingEnv | PortfolioEnv | CryptoEnv | ...            ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## 3.2 Structure des Fichiers

```
FinRL/
‚îú‚îÄ‚îÄ finrl/                          # Package principal
‚îÇ   ‚îú‚îÄ‚îÄ agents/                     # Impl√©mentations des agents DRL
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ elegantrl/              # Wrapper ElegantRL
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ models.py           # DRLAgent pour ElegantRL
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rllib/                  # Wrapper Ray RLlib
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ models.py           # DRLAgent pour RLlib
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ stablebaselines3/       # Wrapper Stable Baselines 3
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ models.py           # DRLAgent pour SB3
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ hyperparams_opt.py  # Optimisation hyperparam√®tres
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ meta/                       # Couche Meta (donn√©es + environnements)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_processor.py       # Classe unifi√©e DataProcessor
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_processors/        # Processeurs sp√©cifiques
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ processor_yahoofinance.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ processor_alpaca.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ processor_wrds.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ processor_ccxt.py
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ env_stock_trading/      # Environnements de trading
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ env_stocktrading.py        # Env principal
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ env_stocktrading_np.py     # Version NumPy optimis√©e
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ env_stock_papertrading.py  # Paper trading
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ env_portfolio_allocation/  # Allocation de portefeuille
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ env_portfolio.py
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ preprocessor/           # Pr√©traitement des donn√©es
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ preprocessors.py    # FeatureEngineer
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ yahoodownloader.py  # T√©l√©chargement Yahoo
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ config.py                   # Configuration globale
‚îÇ   ‚îú‚îÄ‚îÄ config_tickers.py           # Listes de tickers
‚îÇ   ‚îú‚îÄ‚îÄ train.py                    # Script d'entra√Ænement
‚îÇ   ‚îú‚îÄ‚îÄ test.py                     # Script de test
‚îÇ   ‚îú‚îÄ‚îÄ trade.py                    # Script de trading
‚îÇ   ‚îî‚îÄ‚îÄ plot.py                     # Fonctions de visualisation
‚îÇ
‚îî‚îÄ‚îÄ examples/                       # Notebooks d'exemples
    ‚îú‚îÄ‚îÄ Stock_NeurIPS2018_SB3.ipynb
    ‚îî‚îÄ‚îÄ FinRL_Ensemble_StockTrading.ipynb
```

---

# 4. Configuration et Param√®tres

## 4.1 Fichier de Configuration Principal (config.py)

```python
# ============================================================
# CONFIGURATION FINRL - config.py
# ============================================================

from __future__ import annotations

# ============================================================
# R√âPERTOIRES
# ============================================================
DATA_SAVE_DIR = "datasets"           # Sauvegarde des donn√©es t√©l√©charg√©es
TRAINED_MODEL_DIR = "trained_models" # Mod√®les entra√Æn√©s
TENSORBOARD_LOG_DIR = "tensorboard_log"  # Logs pour TensorBoard
RESULTS_DIR = "results"              # R√©sultats de backtesting

# ============================================================
# DATES - D√©finition des p√©riodes
# ============================================================
# Format: 'YYYY-MM-DD' (ann√©e-mois-jour)

# P√©riode d'entra√Ænement
TRAIN_START_DATE = "2014-01-06"  # D√©but (lundi pour √©viter les probl√®mes de weekend)
TRAIN_END_DATE = "2020-07-31"    # Fin entra√Ænement

# P√©riode de test (validation)
TEST_START_DATE = "2020-08-01"   # D√©but test
TEST_END_DATE = "2021-10-01"     # Fin test

# P√©riode de trading (paper trading ou live)
TRADE_START_DATE = "2021-11-01"  # D√©but trading
TRADE_END_DATE = "2021-12-01"    # Fin trading

# ============================================================
# INDICATEURS TECHNIQUES
# ============================================================
# Liste des indicateurs calcul√©s par stockstats
# Documentation: https://pypi.org/project/stockstats/

INDICATORS = [
    "macd",         # MACD (Moving Average Convergence Divergence)
                    # Diff√©rence entre EMA(12) et EMA(26)
    
    "boll_ub",      # Bande de Bollinger Sup√©rieure (Upper Band)
                    # SMA(20) + 2 * std(20)
    
    "boll_lb",      # Bande de Bollinger Inf√©rieure (Lower Band)
                    # SMA(20) - 2 * std(20)
    
    "rsi_30",       # RSI (Relative Strength Index) sur 30 p√©riodes
                    # Mesure la force relative des mouvements haussiers
    
    "cci_30",       # CCI (Commodity Channel Index) sur 30 p√©riodes
                    # Identifie les conditions de surachat/survente
    
    "dx_30",        # DX (Directional Movement Index) sur 30 p√©riodes
                    # Force de la tendance
    
    "close_30_sma", # SMA (Simple Moving Average) sur 30 p√©riodes
                    # Moyenne mobile simple
    
    "close_60_sma", # SMA sur 60 p√©riodes
                    # Tendance √† plus long terme
]

# ============================================================
# PARAM√àTRES DES MOD√àLES DRL
# ============================================================

# A2C (Advantage Actor-Critic)
# Algorithme on-policy synchrone
A2C_PARAMS = {
    "n_steps": 5,           # Nombre de pas avant mise √† jour
    "ent_coef": 0.01,       # Coefficient d'entropie (exploration)
    "learning_rate": 0.0007 # Taux d'apprentissage
}

# PPO (Proximal Policy Optimization)
# Algorithme on-policy avec contrainte de divergence
PPO_PARAMS = {
    "n_steps": 2048,        # Pas par rollout (collecte d'exp√©rience)
    "ent_coef": 0.01,       # Entropie pour exploration
    "learning_rate": 0.00025,  # Taux d'apprentissage
    "batch_size": 64        # Taille des mini-batches
}

# DDPG (Deep Deterministic Policy Gradient)
# Algorithme off-policy pour actions continues
DDPG_PARAMS = {
    "batch_size": 128,      # Taille batch d'entra√Ænement
    "buffer_size": 50000,   # Taille du replay buffer
    "learning_rate": 0.001  # Taux d'apprentissage
}

# TD3 (Twin Delayed DDPG)
# Am√©lioration de DDPG avec r√©seaux jumeaux
TD3_PARAMS = {
    "batch_size": 100,
    "buffer_size": 1000000, # Buffer plus grand
    "learning_rate": 0.001
}

# SAC (Soft Actor-Critic)
# Algorithme off-policy avec maximisation d'entropie
SAC_PARAMS = {
    "batch_size": 64,
    "buffer_size": 100000,
    "learning_rate": 0.0001,
    "learning_starts": 100,  # Pas avant d√©but entra√Ænement
    "ent_coef": "auto_0.1"   # Entropie automatique avec target 0.1
}

# ElegantRL (param√®tres g√©n√©riques)
ERL_PARAMS = {
    "learning_rate": 3e-5,
    "batch_size": 2048,
    "gamma": 0.985,         # Facteur de discount
    "seed": 312,            # Graine al√©atoire
    "net_dimension": 512,   # Dimension des r√©seaux de neurones
    "target_step": 5000,    # Pas par √©pisode
    "eval_gap": 30,         # √âvaluation tous les N √©pisodes
    "eval_times": 64        # Nombre d'√©valuations
}

# RLlib (Ray)
RLlib_PARAMS = {
    "lr": 5e-5,             # Learning rate
    "train_batch_size": 500,
    "gamma": 0.99           # Discount factor
}

# ============================================================
# FUSEAUX HORAIRES
# ============================================================
TIME_ZONE_SHANGHAI = "Asia/Shanghai"   # HSI, SSE, CSI
TIME_ZONE_USEASTERN = "US/Eastern"     # Dow, Nasdaq, S&P
TIME_ZONE_PARIS = "Europe/Paris"       # CAC
TIME_ZONE_BERLIN = "Europe/Berlin"     # DAX

# ============================================================
# API KEYS (√† configurer dans config_private.py)
# ============================================================
ALPACA_API_KEY = "xxx"
ALPACA_API_SECRET = "xxx"
ALPACA_API_BASE_URL = "https://paper-api.alpaca.markets"
BINANCE_BASE_URL = "https://data.binance.vision/"
```

## 4.2 Listes de Tickers (config_tickers.py)

```python
# ============================================================
# LISTES DE TICKERS PR√â-D√âFINIES
# ============================================================

# Ticker unique pour tests rapides
SINGLE_TICKER = ["AAPL"]

# Dow Jones 30 (USA)
DOW_30_TICKER = [
    "AXP",   # American Express
    "AMGN",  # Amgen
    "AAPL",  # Apple
    "BA",    # Boeing
    "CAT",   # Caterpillar
    "CSCO",  # Cisco
    "CVX",   # Chevron
    "GS",    # Goldman Sachs
    "HD",    # Home Depot
    "HON",   # Honeywell
    "IBM",   # IBM
    "INTC",  # Intel
    "JNJ",   # Johnson & Johnson
    "KO",    # Coca-Cola
    "JPM",   # JPMorgan Chase
    "MCD",   # McDonald's
    "MMM",   # 3M
    "MRK",   # Merck
    "MSFT",  # Microsoft
    "NKE",   # Nike
    "PG",    # Procter & Gamble
    "TRV",   # Travelers
    "UNH",   # UnitedHealth
    "CRM",   # Salesforce
    "VZ",    # Verizon
    "V",     # Visa
    "WBA",   # Walgreens
    "WMT",   # Walmart
    "DIS",   # Disney
    "DOW",   # Dow Inc.
]

# NASDAQ 100 (partiellement)
NAS_100_TICKER = [
    "AMGN", "AAPL", "AMAT", "INTC", "PCAR", "PAYX", "MSFT", "ADBE",
    "CSCO", "XLNX", "QCOM", "COST", "SBUX", "FISV", "CTXS", "INTU",
    "AMZN", "EBAY", "BIIB", "CHKP", "GILD", "NLOK", "CMCSA", "FAST",
    "ADSK", "CTSH", "NVDA", "GOOGL", "ISRG", "VRTX", # ... etc
]

# S&P 500
SP_500_TICKER = [
    "A", "AAL", "AAP", "AAPL", "ABBV", "ABC", # ... (liste compl√®te de ~500 tickers)
]

# CAC 40 (France)
CAC_40_TICKER = [
    "AC.PA",   # Accor
    "AI.PA",   # Air Liquide
    "AIR.PA",  # Airbus
    "BNP.PA",  # BNP Paribas
    "OR.PA",   # L'Or√©al
    "MC.PA",   # LVMH
    "SAN.PA",  # Sanofi
    "FP.PA",   # Total
    # ... etc
]

# DAX 30 (Allemagne)
DAX_30_TICKER = [
    "ALV.DE",  # Allianz
    "BAS.DE",  # BASF
    "BAYN.DE", # Bayer
    "BMW.DE",  # BMW
    "SAP.DE",  # SAP
    "SIE.DE",  # Siemens
    "VOW3.DE", # Volkswagen
    # ... etc
]

# Cryptomonnaies (format CCXT)
CRYPTO_TICKER = [
    "BTC/USDT",
    "ETH/USDT",
    "BNB/USDT",
    "XRP/USDT",
    "ADA/USDT",
]
```

## 4.3 Cr√©ation des R√©pertoires

```python
# ============================================================
# INITIALISATION DES R√âPERTOIRES
# ============================================================

import os
from finrl.config import (
    DATA_SAVE_DIR,
    TRAINED_MODEL_DIR,
    TENSORBOARD_LOG_DIR,
    RESULTS_DIR
)

def check_and_make_directories(directories: list):
    """
    V√©rifie et cr√©e les r√©pertoires n√©cessaires.
    
    Parameters:
    -----------
    directories : list
        Liste des chemins de r√©pertoires √† cr√©er
    """
    for directory in directories:
        if not os.path.exists(directory):
            os.makedirs(directory)
            print(f"R√©pertoire cr√©√©: {directory}")
        else:
            print(f"R√©pertoire existant: {directory}")

# Utilisation
check_and_make_directories([
    DATA_SAVE_DIR,
    TRAINED_MODEL_DIR,
    TENSORBOARD_LOG_DIR,
    RESULTS_DIR
])
```

---

# 5. Data Processing

## 5.1 T√©l√©chargement des Donn√©es

### 5.1.1 Avec YahooDownloader (Simple)

```python
# ============================================================
# T√âL√âCHARGEMENT DE DONN√âES AVEC YAHOO FINANCE
# ============================================================

from finrl.meta.preprocessor.yahoodownloader import YahooDownloader
from finrl.config_tickers import DOW_30_TICKER

# D√©finir les dates
TRAIN_START_DATE = '2010-01-01'
TRAIN_END_DATE = '2021-10-01'
TRADE_START_DATE = '2021-10-01'
TRADE_END_DATE = '2023-03-01'

# T√©l√©charger les donn√©es
# Retourne un DataFrame avec colonnes: date, open, high, low, close, volume, tic
df = YahooDownloader(
    start_date=TRAIN_START_DATE,
    end_date=TRADE_END_DATE,
    ticker_list=DOW_30_TICKER  # Liste de 30 tickers
).fetch_data()

print(f"Shape des donn√©es: {df.shape}")
print(f"Colonnes: {df.columns.tolist()}")
print(f"Tickers uniques: {df.tic.unique()}")
print(f"P√©riode: {df.date.min()} √† {df.date.max()}")

# Aper√ßu des donn√©es
df.head()
```

### 5.1.2 Avec DataProcessor (Unifi√©)

```python
# ============================================================
# DATAPROCESSOR - INTERFACE UNIFI√âE
# ============================================================

from finrl.meta.data_processor import DataProcessor

# Cr√©er le processeur pour Yahoo Finance
dp = DataProcessor(data_source="yahoofinance")

# T√©l√©charger les donn√©es
# time_interval: "1D" (journalier), "1H" (horaire), "1Min" (minute)
df = dp.download_data(
    ticker_list=DOW_30_TICKER,
    start_date=TRAIN_START_DATE,
    end_date=TRADE_END_DATE,
    time_interval="1D"  # Donn√©es journali√®res
)

# Nettoyer les donn√©es (gestion des NaN, ajustement des prix)
df = dp.clean_data(df)

print(f"Donn√©es nettoy√©es: {df.shape}")
```

### 5.1.3 Avec Alpaca (Trading US)

```python
# ============================================================
# ALPACA - DONN√âES EN TEMPS R√âEL
# ============================================================

from finrl.meta.data_processor import DataProcessor

# Configuration Alpaca (n√©cessite un compte)
ALPACA_API_KEY = "votre_api_key"
ALPACA_API_SECRET = "votre_api_secret"
ALPACA_API_BASE_URL = "https://paper-api.alpaca.markets"

# Cr√©er le processeur Alpaca
dp = DataProcessor(
    data_source="alpaca",
    API_KEY=ALPACA_API_KEY,
    API_SECRET=ALPACA_API_SECRET,
    API_BASE_URL=ALPACA_API_BASE_URL
)

# T√©l√©charger (m√™me interface)
df = dp.download_data(
    ticker_list=["AAPL", "MSFT", "GOOGL"],
    start_date="2022-01-01",
    end_date="2023-01-01",
    time_interval="1D"
)
```

## 5.2 Feature Engineering (Ing√©nierie des Caract√©ristiques)

```python
# ============================================================
# FEATURE ENGINEERING - AJOUT D'INDICATEURS TECHNIQUES
# ============================================================

from finrl.meta.preprocessor.preprocessors import FeatureEngineer
from finrl.config import INDICATORS

# Afficher les indicateurs utilis√©s
print("Indicateurs techniques:")
for ind in INDICATORS:
    print(f"  - {ind}")

# Cr√©er le FeatureEngineer
fe = FeatureEngineer(
    use_technical_indicator=True,       # Ajouter indicateurs techniques
    tech_indicator_list=INDICATORS,     # Liste des indicateurs
    use_vix=True,                        # Ajouter VIX (indice de volatilit√©)
    use_turbulence=True,                 # Ajouter indice de turbulence
    user_defined_feature=False           # Pas de features personnalis√©es
)

# Pr√©traiter les donn√©es
processed = fe.preprocess_data(df)

print(f"\nNouvelles colonnes ajout√©es:")
new_cols = [c for c in processed.columns if c not in df.columns]
for col in new_cols:
    print(f"  - {col}")

# V√©rifier les donn√©es
print(f"\nShape apr√®s traitement: {processed.shape}")
processed.head()
```

### Explication des Indicateurs Techniques

```python
# ============================================================
# EXPLICATION DES INDICATEURS
# ============================================================

"""
INDICATEURS TECHNIQUES DANS FINRL
=================================

1. MACD (Moving Average Convergence Divergence)
   - Formule: EMA(12) - EMA(26)
   - Signal: Croisement avec la ligne de signal (EMA(9) du MACD)
   - Interpr√©tation: MACD > 0 = tendance haussi√®re

2. Bandes de Bollinger (boll_ub, boll_lb)
   - Upper Band (boll_ub): SMA(20) + 2 * StdDev(20)
   - Lower Band (boll_lb): SMA(20) - 2 * StdDev(20)
   - Interpr√©tation: Prix proche de boll_ub = surachat potentiel

3. RSI (Relative Strength Index)
   - Formule: 100 - (100 / (1 + RS))
   - RS = Moyenne des hausses / Moyenne des baisses
   - Interpr√©tation: RSI > 70 = surachat, RSI < 30 = survente

4. CCI (Commodity Channel Index)
   - Formule: (Typical Price - SMA) / (0.015 * Mean Deviation)
   - Typical Price = (High + Low + Close) / 3
   - Interpr√©tation: CCI > 100 = surachat, CCI < -100 = survente

5. DX (Directional Movement Index)
   - Mesure la force de la tendance
   - DX √©lev√© = tendance forte

6. SMA (Simple Moving Average)
   - close_30_sma: Moyenne des 30 derniers prix de cl√¥ture
   - close_60_sma: Moyenne des 60 derniers prix de cl√¥ture
   - Utilisation: Identifier la tendance

7. VIX (CBOE Volatility Index)
   - "Indice de la peur" - mesure la volatilit√© attendue du S&P 500
   - VIX √©lev√© = haute incertitude/volatilit√©

8. Turbulence
   - Mesure la d√©viation des rendements par rapport √† leur distribution historique
   - Turbulence √©lev√©e = conditions de march√© anormales
"""
```

## 5.3 Gestion des Donn√©es Manquantes

```python
# ============================================================
# GESTION DES DONN√âES MANQUANTES
# ============================================================

import itertools
import pandas as pd

def fill_missing_data(processed_df):
    """
    Remplit les donn√©es manquantes pour garantir la coh√©rence
    temporelle entre tous les tickers.
    
    Parameters:
    -----------
    processed_df : pd.DataFrame
        DataFrame avec colonnes ['date', 'tic', ...features...]
    
    Returns:
    --------
    pd.DataFrame
        DataFrame complet sans donn√©es manquantes
    """
    # Obtenir tous les tickers uniques
    list_ticker = processed_df["tic"].unique().tolist()
    
    # Obtenir toutes les dates de trading (du min au max)
    list_date = list(pd.date_range(
        processed_df['date'].min(),
        processed_df['date'].max()
    ).astype(str))
    
    # Cr√©er toutes les combinaisons possibles (date, ticker)
    combination = list(itertools.product(list_date, list_ticker))
    
    # Cr√©er un DataFrame avec toutes les combinaisons
    processed_full = pd.DataFrame(
        combination, 
        columns=["date", "tic"]
    ).merge(processed_df, on=["date", "tic"], how="left")
    
    # Garder seulement les dates qui sont dans les donn√©es originales
    # (exclure weekends et jours f√©ri√©s)
    processed_full = processed_full[
        processed_full['date'].isin(processed_df['date'])
    ]
    
    # Trier par date et ticker
    processed_full = processed_full.sort_values(['date', 'tic'])
    
    # Remplir les valeurs manquantes par 0 (ou autre strat√©gie)
    processed_full = processed_full.fillna(0)
    
    return processed_full

# Utilisation
processed_full = fill_missing_data(processed)
print(f"Shape final: {processed_full.shape}")
```

## 5.4 Division Train/Test/Trade

```python
# ============================================================
# DIVISION DES DONN√âES
# ============================================================

from finrl.meta.preprocessor.preprocessors import data_split

# D√©finir les p√©riodes
TRAIN_START_DATE = '2010-01-01'
TRAIN_END_DATE = '2021-10-01'
TRADE_START_DATE = '2021-10-01'
TRADE_END_DATE = '2023-03-01'

# Diviser les donn√©es
train = data_split(processed_full, TRAIN_START_DATE, TRAIN_END_DATE)
trade = data_split(processed_full, TRADE_START_DATE, TRADE_END_DATE)

# Statistiques
print("="*50)
print("STATISTIQUES DES DONN√âES")
print("="*50)
print(f"\nP√©riode d'entra√Ænement: {TRAIN_START_DATE} √† {TRAIN_END_DATE}")
print(f"  - Lignes: {len(train)}")
print(f"  - Jours de trading: {len(train.date.unique())}")

print(f"\nP√©riode de trading: {TRADE_START_DATE} √† {TRADE_END_DATE}")
print(f"  - Lignes: {len(trade)}")
print(f"  - Jours de trading: {len(trade.date.unique())}")

# V√©rifier la continuit√©
print(f"\nDerni√®re date train: {train.date.max()}")
print(f"Premi√®re date trade: {trade.date.min()}")
```

## 5.5 Conversion en Arrays NumPy

```python
# ============================================================
# CONVERSION POUR LES ENVIRONNEMENTS NUMPY
# ============================================================

from finrl.meta.data_processor import DataProcessor

def prepare_arrays(df, indicators, if_vix=True):
    """
    Convertit un DataFrame en arrays NumPy pour les environnements.
    
    Returns:
    --------
    price_array : np.ndarray
        Prix de cl√¥ture, shape (n_days, n_stocks)
    tech_array : np.ndarray
        Indicateurs techniques, shape (n_days, n_stocks * n_indicators)
    turbulence_array : np.ndarray
        Indice de turbulence/VIX, shape (n_days,)
    """
    dp = DataProcessor(data_source="yahoofinance")
    
    # Utiliser la m√©thode df_to_array
    price_array, tech_array, turbulence_array = dp.processor.df_to_array(
        df=df,
        tech_indicator_list=indicators,
        if_vix=if_vix
    )
    
    # Nettoyer les NaN et Inf
    tech_array = np.nan_to_num(tech_array, nan=0.0, posinf=0.0, neginf=0.0)
    
    print(f"Price array shape: {price_array.shape}")
    print(f"Tech array shape: {tech_array.shape}")
    print(f"Turbulence array shape: {turbulence_array.shape}")
    
    return price_array, tech_array, turbulence_array

# Utilisation
price_array, tech_array, turb_array = prepare_arrays(
    processed_full, 
    INDICATORS, 
    if_vix=True
)
```

---

# 6. Environnements de Trading

## 6.1 Vue d'Ensemble des Environnements

FinRL propose plusieurs environnements compatibles OpenAI Gym / Gymnasium :

| Environnement | Fichier | Usage |
|---------------|---------|-------|
| `StockTradingEnv` | env_stocktrading.py | Trading multi-actions, DataFrame |
| `StockTradingEnv (NP)` | env_stocktrading_np.py | Version NumPy optimis√©e |
| `StockPortfolioEnv` | env_portfolio.py | Allocation de portefeuille |
| `AlpacaPaperTrading` | env_stock_papertrading.py | Paper trading temps r√©el |

## 6.2 StockTradingEnv (Principal)

```python
# ============================================================
# ENVIRONNEMENT DE TRADING PRINCIPAL
# ============================================================

from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv
from finrl.config import INDICATORS

# Calculer les dimensions
stock_dimension = len(train.tic.unique())

# State space = cash + prix * n_stocks + positions * n_stocks + indicateurs
state_space = 1 + 2 * stock_dimension + len(INDICATORS) * stock_dimension
print(f"Stock Dimension: {stock_dimension}")
print(f"State Space: {state_space}")

# Configuration de l'environnement
env_kwargs = {
    # Param√®tres de trading
    "hmax": 100,                     # Nombre max d'actions par transaction
    "initial_amount": 1_000_000,     # Capital initial ($1M)
    
    # Positions initiales (0 = pas de positions)
    "num_stock_shares": [0] * stock_dimension,
    
    # Co√ªts de transaction (0.1% = 0.001)
    "buy_cost_pct": [0.001] * stock_dimension,   # 0.1% pour acheter
    "sell_cost_pct": [0.001] * stock_dimension,  # 0.1% pour vendre
    
    # Dimensions
    "state_space": state_space,
    "stock_dim": stock_dimension,
    "action_space": stock_dimension,  # Une action par stock
    
    # Indicateurs techniques
    "tech_indicator_list": INDICATORS,
    
    # Scaling de la r√©compense (important pour la stabilit√©)
    "reward_scaling": 1e-4,
    
    # Gestion du risque (optionnel)
    "turbulence_threshold": None,  # Seuil de turbulence
    "risk_indicator_col": "vix",   # Colonne de risque
    
    # Verbosit√©
    "print_verbosity": 10,  # Afficher stats tous les 10 √©pisodes
}

# Cr√©er l'environnement d'entra√Ænement
e_train_gym = StockTradingEnv(df=train, **env_kwargs)

# Obtenir l'environnement vectoris√© pour Stable Baselines
env_train, obs = e_train_gym.get_sb_env()

print(f"\nType d'environnement: {type(env_train)}")
print(f"Observation shape: {obs.shape}")
print(f"Action space: {e_train_gym.action_space}")
print(f"Observation space: {e_train_gym.observation_space}")
```

### Structure de l'√âtat (State)

```python
# ============================================================
# STRUCTURE DE L'√âTAT DANS STOCKTRADINGENV
# ============================================================

"""
L'√©tat (state) est un vecteur 1D contenant:

state = [
    cash,                        # Index 0: Montant de cash disponible
    price_1, price_2, ...,       # Index 1 √† stock_dim: Prix de cl√¥ture
    shares_1, shares_2, ...,     # Index stock_dim+1 √† 2*stock_dim: Positions
    tech_1_1, tech_1_2, ...,     # Indicateurs pour stock 1
    tech_2_1, tech_2_2, ...,     # Indicateurs pour stock 2
    ...
]

Exemple avec 30 stocks et 8 indicateurs:
- state[0] = cash
- state[1:31] = 30 prix
- state[31:61] = 30 positions
- state[61:301] = 30 * 8 = 240 indicateurs techniques

Total: 1 + 30 + 30 + 240 = 301 dimensions
"""

# V√©rification
state_example = e_train_gym.state
print(f"Longueur de l'√©tat: {len(state_example)}")
print(f"Cash: ${state_example[0]:,.2f}")
print(f"Premier prix: ${state_example[1]:.2f}")
```

### Structure des Actions

```python
# ============================================================
# STRUCTURE DES ACTIONS
# ============================================================

"""
L'espace d'action est continu, shape = (stock_dim,)
Chaque action est dans [-1, 1]

Interpr√©tation:
- action[i] > 0: Acheter min(hmax * action[i], cash_available) actions du stock i
- action[i] < 0: Vendre min(hmax * |action[i]|, current_holdings) actions du stock i
- action[i] ‚âà 0: Conserver (hold)

Exemple avec hmax=100:
- action = [0.5, -0.3, 0.0, ...]
  ‚Üí Stock 0: Acheter 50 actions (0.5 * 100)
  ‚Üí Stock 1: Vendre 30 actions (0.3 * 100)
  ‚Üí Stock 2: Hold
"""

# Test d'une action
import numpy as np
sample_action = np.random.uniform(-1, 1, size=(stock_dimension,))
print(f"Action sample shape: {sample_action.shape}")
print(f"Actions (premiers 5): {sample_action[:5]}")
```

## 6.3 Environnement NumPy Optimis√©

```python
# ============================================================
# ENVIRONNEMENT NUMPY (HAUTE PERFORMANCE)
# ============================================================

from finrl.meta.env_stock_trading.env_stocktrading_np import StockTradingEnv as StockTradingEnvNP

# Pr√©parer la configuration
env_config = {
    "price_array": price_array,       # Prix, shape (n_days, n_stocks)
    "tech_array": tech_array,         # Indicateurs, shape (n_days, n_features)
    "turbulence_array": turb_array,   # Turbulence, shape (n_days,)
    "if_train": True                   # Mode entra√Ænement
}

# Cr√©er l'environnement
env_np = StockTradingEnvNP(
    config=env_config,
    initial_account=1_000_000,     # Capital initial
    gamma=0.99,                     # Facteur de discount
    turbulence_thresh=99,           # Seuil de turbulence
    min_stock_rate=0.1,             # Taux minimum pour trader
    max_stock=100,                  # Nombre max d'actions par trade
    buy_cost_pct=0.001,             # Co√ªt d'achat (0.1%)
    sell_cost_pct=0.001,            # Co√ªt de vente (0.1%)
    reward_scaling=2**-11,          # Scaling de r√©compense
)

print(f"State dim: {env_np.state_dim}")
print(f"Action dim: {env_np.action_dim}")
print(f"Max step: {env_np.max_step}")
```

## 6.4 Environnement d'Allocation de Portefeuille

```python
# ============================================================
# ENVIRONNEMENT PORTFOLIO ALLOCATION
# ============================================================

from finrl.meta.env_portfolio_allocation.env_portfolio import StockPortfolioEnv

"""
Diff√©rence avec StockTradingEnv:
- Actions = poids du portefeuille (somment √† 1)
- Pas de trading discret, mais r√©allocation continue
- Utilise une matrice de covariance
"""

# Pr√©parer les donn√©es avec covariance
def add_covariance(df, lookback=252):
    """Ajoute une matrice de covariance roulante."""
    df_pivot = df.pivot(index='date', columns='tic', values='close')
    
    # Calculer les rendements
    returns = df_pivot.pct_change()
    
    # Covariance roulante
    cov_list = []
    for i in range(len(returns)):
        if i < lookback:
            cov_list.append(np.eye(len(df_pivot.columns)))
        else:
            cov_list.append(returns.iloc[i-lookback:i].cov().values)
    
    # Ajouter au DataFrame
    df_cov = df_pivot.copy()
    df_cov['cov_list'] = cov_list
    
    return df_cov

# Configuration de l'environnement
portfolio_env_kwargs = {
    "hmax": 100,
    "initial_amount": 1_000_000,
    "transaction_cost_pct": 0.001,  # 0.1%
    "reward_scaling": 1,
    "state_space": stock_dimension,
    "stock_dim": stock_dimension,
    "action_space": stock_dimension,
    "tech_indicator_list": INDICATORS,
    "lookback": 252,  # 1 an de lookback pour covariance
}

# L'environnement utilise softmax pour normaliser les poids
# Donc actions peuvent √™tre n'importe quelles valeurs
```

## 6.5 Gestion de la Turbulence

```python
# ============================================================
# GESTION DE LA TURBULENCE (RISK MANAGEMENT)
# ============================================================

"""
La turbulence est calcul√©e comme la distance de Mahalanobis 
des rendements actuels par rapport √† leur distribution historique.

Formule:
turbulence_t = (r_t - Œº)' * Œ£^(-1) * (r_t - Œº)

o√π:
- r_t = vecteur des rendements au temps t
- Œº = moyenne historique des rendements
- Œ£ = matrice de covariance historique

Si turbulence > seuil:
‚Üí L'agent liquide toutes ses positions (risk-off)
"""

# Calculer le seuil de turbulence sur les donn√©es d'entra√Ænement
data_risk = processed_full[
    (processed_full.date < TRAIN_END_DATE) & 
    (processed_full.date >= TRAIN_START_DATE)
]
insample_risk = data_risk.drop_duplicates(subset=['date'])

# Statistiques de turbulence
print("Statistiques de turbulence (in-sample):")
print(insample_risk.turbulence.describe())

# Utiliser le percentile 99.6 comme seuil
turbulence_threshold = insample_risk.turbulence.quantile(0.996)
print(f"\nSeuil de turbulence (99.6%): {turbulence_threshold:.2f}")

# Statistiques VIX
print("\nStatistiques VIX (in-sample):")
print(insample_risk.vix.describe())
vix_threshold = insample_risk.vix.quantile(0.996)
print(f"Seuil VIX (99.6%): {vix_threshold:.2f}")

# Cr√©er l'environnement de trading avec seuil
e_trade_gym = StockTradingEnv(
    df=trade,
    turbulence_threshold=turbulence_threshold,  # Activer le risk management
    risk_indicator_col='turbulence',  # Ou 'vix'
    **env_kwargs
)
```

---

# 7. Agents DRL

## 7.1 Stable Baselines 3 (Recommand√©)

### 7.1.1 Configuration de Base

```python
# ============================================================
# AGENTS STABLE BASELINES 3
# ============================================================

from finrl.agents.stablebaselines3.models import DRLAgent
from stable_baselines3.common.logger import configure

# Cr√©er l'agent
agent = DRLAgent(env=env_train)

# Mod√®les disponibles
MODELS_AVAILABLE = ["a2c", "ppo", "ddpg", "td3", "sac"]
print(f"Mod√®les disponibles: {MODELS_AVAILABLE}")
```

### 7.1.2 A2C (Advantage Actor-Critic)

```python
# ============================================================
# A2C - ADVANTAGE ACTOR-CRITIC
# ============================================================

"""
A2C (Advantage Actor-Critic):
- Algorithme on-policy synchrone
- Utilise plusieurs workers en parall√®le
- Actor: pr√©dit l'action optimale
- Critic: √©value la valeur de l'√©tat

Avantages:
- Simple et stable
- Bon pour commencer
- Fonctionne bien avec peu de donn√©es

Inconv√©nients:
- Moins efficient que PPO
- Pas de replay buffer
"""

# Param√®tres A2C
A2C_PARAMS = {
    "n_steps": 5,           # Nombre de pas avant mise √† jour
                            # Plus petit = mises √† jour plus fr√©quentes
    
    "ent_coef": 0.01,       # Coefficient d'entropie
                            # Plus √©lev√© = plus d'exploration
    
    "learning_rate": 0.0007 # Taux d'apprentissage
}

# Cr√©er le mod√®le A2C
agent = DRLAgent(env=env_train)
model_a2c = agent.get_model(
    "a2c",
    model_kwargs=A2C_PARAMS
)

# Configurer le logger TensorBoard
tmp_path = RESULTS_DIR + '/a2c'
new_logger = configure(tmp_path, ["stdout", "csv", "tensorboard"])
model_a2c.set_logger(new_logger)

# Entra√Æner
trained_a2c = agent.train_model(
    model=model_a2c,
    tb_log_name='a2c',
    total_timesteps=100_000  # Nombre total de pas d'entra√Ænement
)

# Sauvegarder
trained_a2c.save(f"{TRAINED_MODEL_DIR}/a2c_model")
```

### 7.1.3 PPO (Proximal Policy Optimization)

```python
# ============================================================
# PPO - PROXIMAL POLICY OPTIMIZATION
# ============================================================

"""
PPO (Proximal Policy Optimization):
- Algorithme on-policy
- Limite les mises √† jour de politique pour la stabilit√©
- Utilise clipping sur le ratio de probabilit√©

Avantages:
- Tr√®s stable
- Bon compromis exploration/exploitation
- Standard de l'industrie

Param√®tres importants:
- n_steps: taille du rollout buffer
- batch_size: taille des mini-batches
- ent_coef: exploration via entropie
"""

PPO_PARAMS = {
    "n_steps": 2048,         # Nombre de pas par rollout
                             # Plus grand = apprentissage plus stable
    
    "ent_coef": 0.01,        # Coefficient d'entropie
    
    "learning_rate": 0.00025, # Taux d'apprentissage
                             # Plus petit que A2C pour stabilit√©
    
    "batch_size": 128,       # Taille des mini-batches
                             # Doit diviser n_steps
    
    "n_epochs": 10,          # Nombre d'√©poques par mise √† jour
    
    "gamma": 0.99,           # Facteur de discount
    
    "gae_lambda": 0.95,      # Lambda pour GAE (Generalized Advantage Estimation)
    
    "clip_range": 0.2,       # Range de clipping (ratio de politique)
}

# Cr√©er et entra√Æner PPO
agent = DRLAgent(env=env_train)
model_ppo = agent.get_model("ppo", model_kwargs=PPO_PARAMS)

# Logger
tmp_path = RESULTS_DIR + '/ppo'
new_logger = configure(tmp_path, ["stdout", "csv", "tensorboard"])
model_ppo.set_logger(new_logger)

# Entra√Ænement
trained_ppo = agent.train_model(
    model=model_ppo,
    tb_log_name='ppo',
    total_timesteps=200_000
)

trained_ppo.save(f"{TRAINED_MODEL_DIR}/ppo_model")
```

### 7.1.4 DDPG (Deep Deterministic Policy Gradient)

```python
# ============================================================
# DDPG - DEEP DETERMINISTIC POLICY GRADIENT
# ============================================================

"""
DDPG (Deep Deterministic Policy Gradient):
- Algorithme off-policy pour actions continues
- Utilise un replay buffer
- Actor: politique d√©terministe
- Critic: Q-function

Avantages:
- Efficace en donn√©es (replay buffer)
- Bon pour actions continues

Inconv√©nients:
- Peut √™tre instable
- Sensible aux hyperparam√®tres
"""

DDPG_PARAMS = {
    "batch_size": 128,        # Taille des batches du replay buffer
    
    "buffer_size": 50_000,    # Taille du replay buffer
                              # Plus grand = plus de m√©moire
    
    "learning_rate": 0.001,   # Taux d'apprentissage
    
    "tau": 0.005,             # Coefficient de soft update des target networks
    
    "gamma": 0.99,            # Facteur de discount
    
    # Bruit d'exploration (Ornstein-Uhlenbeck)
    "action_noise": "ornstein_uhlenbeck"
}

# Cr√©er et entra√Æner DDPG
agent = DRLAgent(env=env_train)
model_ddpg = agent.get_model("ddpg", model_kwargs=DDPG_PARAMS)

# Logger
tmp_path = RESULTS_DIR + '/ddpg'
new_logger = configure(tmp_path, ["stdout", "csv", "tensorboard"])
model_ddpg.set_logger(new_logger)

# Entra√Ænement
trained_ddpg = agent.train_model(
    model=model_ddpg,
    tb_log_name='ddpg',
    total_timesteps=100_000
)

trained_ddpg.save(f"{TRAINED_MODEL_DIR}/ddpg_model")
```

### 7.1.5 TD3 (Twin Delayed DDPG)

```python
# ============================================================
# TD3 - TWIN DELAYED DDPG
# ============================================================

"""
TD3 (Twin Delayed Deep Deterministic Policy Gradient):
- Am√©lioration de DDPG
- Deux r√©seaux Critic (twin) pour r√©duire overestimation
- Mise √† jour delayed de l'Actor
- Ajout de bruit sur les target actions

Avantages:
- Plus stable que DDPG
- Meilleure performance en g√©n√©ral
"""

TD3_PARAMS = {
    "batch_size": 100,
    
    "buffer_size": 1_000_000,  # Buffer tr√®s large
    
    "learning_rate": 0.001,
    
    "tau": 0.005,
    
    "gamma": 0.99,
    
    "policy_delay": 2,         # Mise √† jour de l'actor tous les 2 pas
    
    "target_policy_noise": 0.2,  # Bruit ajout√© aux target actions
    
    "target_noise_clip": 0.5,    # Clipping du bruit
}

# Cr√©er et entra√Æner TD3
agent = DRLAgent(env=env_train)
model_td3 = agent.get_model("td3", model_kwargs=TD3_PARAMS)

tmp_path = RESULTS_DIR + '/td3'
new_logger = configure(tmp_path, ["stdout", "csv", "tensorboard"])
model_td3.set_logger(new_logger)

trained_td3 = agent.train_model(
    model=model_td3,
    tb_log_name='td3',
    total_timesteps=100_000
)

trained_td3.save(f"{TRAINED_MODEL_DIR}/td3_model")
```

### 7.1.6 SAC (Soft Actor-Critic)

```python
# ============================================================
# SAC - SOFT ACTOR-CRITIC
# ============================================================

"""
SAC (Soft Actor-Critic):
- Algorithme off-policy bas√© sur maximum entropy RL
- Maximise reward + entropie de la politique
- Exploration automatique via entropie

Avantages:
- Tr√®s stable
- Exploration robuste
- Souvent le meilleur pour trading

Particularit√©:
- ent_coef peut √™tre appris automatiquement ("auto")
"""

SAC_PARAMS = {
    "batch_size": 128,
    
    "buffer_size": 100_000,
    
    "learning_rate": 0.0001,   # Learning rate plus petit
    
    "learning_starts": 100,    # Pas avant de commencer l'entra√Ænement
                               # Permet de remplir le buffer
    
    "tau": 0.005,
    
    "gamma": 0.99,
    
    "ent_coef": "auto_0.1",    # Entropie automatique
                               # "auto" ou "auto_X" o√π X est la target
                               # Plus √©lev√© = plus d'exploration
    
    "target_entropy": "auto",  # Target entropy automatique
}

# Cr√©er et entra√Æner SAC
agent = DRLAgent(env=env_train)
model_sac = agent.get_model("sac", model_kwargs=SAC_PARAMS)

tmp_path = RESULTS_DIR + '/sac'
new_logger = configure(tmp_path, ["stdout", "csv", "tensorboard"])
model_sac.set_logger(new_logger)

trained_sac = agent.train_model(
    model=model_sac,
    tb_log_name='sac',
    total_timesteps=100_000
)

trained_sac.save(f"{TRAINED_MODEL_DIR}/sac_model")
```

## 7.2 ElegantRL (Haute Performance)

```python
# ============================================================
# ELEGANTRL - AGENT HAUTE PERFORMANCE
# ============================================================

from finrl.agents.elegantrl.models import DRLAgent as DRLAgent_ERL
from finrl.config import ERL_PARAMS

"""
ElegantRL:
- Framework DRL optimis√© pour la performance
- Impl√©mentation PyTorch efficace
- Support GPU natif
- Entra√Ænement parall√®le
"""

# Configuration ElegantRL
ERL_PARAMS = {
    "learning_rate": 3e-5,
    "batch_size": 2048,
    "gamma": 0.985,           # Facteur de discount
    "seed": 312,
    "net_dimension": 512,     # Dimension des couches cach√©es
    "target_step": 5000,      # Pas par √©pisode d'entra√Ænement
    "eval_gap": 30,           # √âvaluation tous les 30 √©pisodes
    "eval_times": 64,         # Nombre d'√©valuations
}

# Cr√©er l'agent ElegantRL
agent_erl = DRLAgent_ERL(
    env=StockTradingEnvNP,
    price_array=price_array,
    tech_array=tech_array,
    turbulence_array=turb_array
)

# Obtenir le mod√®le (PPO dans cet exemple)
model_erl = agent_erl.get_model(
    model_name="ppo",
    model_kwargs=ERL_PARAMS
)

# Entra√Æner
agent_erl.train_model(
    model=model_erl,
    cwd="./trained_models/elegantrl_ppo",
    total_timesteps=100_000
)
```

## 7.3 Ray RLlib (Distribu√©)

```python
# ============================================================
# RLLIB - APPRENTISSAGE DISTRIBU√â
# ============================================================

import ray
from finrl.agents.rllib.models import DRLAgent as DRLAgent_RLlib
from finrl.config import RLlib_PARAMS

"""
Ray RLlib:
- Framework pour RL distribu√©
- Scaling horizontal sur cluster
- Support multi-GPU
"""

# Initialiser Ray
ray.shutdown()  # Fermer session pr√©c√©dente si existante
ray.init(ignore_reinit_error=True)

# Param√®tres RLlib
RLlib_PARAMS = {
    "lr": 5e-5,               # Learning rate
    "train_batch_size": 500,  # Taille batch d'entra√Ænement
    "gamma": 0.99,            # Discount factor
}

# Cr√©er l'agent RLlib
agent_rllib = DRLAgent_RLlib(
    env=StockTradingEnvNP,
    price_array=price_array,
    tech_array=tech_array,
    turbulence_array=turb_array
)

# Obtenir le mod√®le
model_rllib, model_config = agent_rllib.get_model("ppo")

# Configurer
model_config["lr"] = RLlib_PARAMS["lr"]
model_config["train_batch_size"] = RLlib_PARAMS["train_batch_size"]
model_config["gamma"] = RLlib_PARAMS["gamma"]

# Entra√Æner
trained_rllib = agent_rllib.train_model(
    model=model_rllib,
    model_name="ppo",
    model_config=model_config,
    total_episodes=100
)

# Sauvegarder
trained_rllib.save("./trained_models/rllib_ppo")

# Fermer Ray
ray.shutdown()
```

## 7.4 Comparaison des Frameworks

| Aspect | Stable Baselines 3 | ElegantRL | RLlib |
|--------|-------------------|-----------|-------|
| **Facilit√©** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê |
| **Performance** | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **Documentation** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **Scaling** | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **Debugging** | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê |

**Recommandation:**
- **D√©butant** ‚Üí Stable Baselines 3
- **Production** ‚Üí ElegantRL
- **Cluster/Cloud** ‚Üí RLlib

---

# 8. Pipeline Train-Test-Trade

## 8.1 Script d'Entra√Ænement (train.py)

```python
# ============================================================
# PIPELINE D'ENTRA√éNEMENT COMPLET
# ============================================================

from finrl.train import train
from finrl.config import (
    TRAIN_START_DATE, TRAIN_END_DATE,
    INDICATORS, ERL_PARAMS
)
from finrl.config_tickers import DOW_30_TICKER
from finrl.meta.env_stock_trading.env_stocktrading_np import StockTradingEnv

def run_training(
    model_name="ppo",
    drl_lib="stable_baselines3",
    total_timesteps=100_000
):
    """
    Ex√©cute le pipeline d'entra√Ænement complet.
    
    Parameters:
    -----------
    model_name : str
        Nom du mod√®le ("ppo", "a2c", "ddpg", "td3", "sac")
    drl_lib : str
        Librairie DRL ("stable_baselines3", "elegantrl", "rllib")
    total_timesteps : int
        Nombre de pas d'entra√Ænement
    """
    
    # Appeler la fonction train
    train(
        start_date=TRAIN_START_DATE,
        end_date=TRAIN_END_DATE,
        ticker_list=DOW_30_TICKER,
        data_source="yahoofinance",
        time_interval="1D",
        technical_indicator_list=INDICATORS,
        drl_lib=drl_lib,
        env=StockTradingEnv,
        model_name=model_name,
        cwd=f"./trained_models/{model_name}",
        if_vix=True,
        
        # Param√®tres sp√©cifiques au framework
        erl_params=ERL_PARAMS if drl_lib == "elegantrl" else None,
        break_step=total_timesteps if drl_lib == "elegantrl" else None,
        total_timesteps=total_timesteps if drl_lib == "stable_baselines3" else None,
    )
    
    print(f"‚úÖ Entra√Ænement termin√© pour {model_name} avec {drl_lib}")

# Ex√©cution
if __name__ == "__main__":
    # Entra√Æner PPO avec Stable Baselines 3
    run_training(
        model_name="ppo",
        drl_lib="stable_baselines3",
        total_timesteps=100_000
    )
```

## 8.2 Script de Test (test.py)

```python
# ============================================================
# PIPELINE DE TEST/VALIDATION
# ============================================================

from finrl.test import test
from finrl.config import (
    TEST_START_DATE, TEST_END_DATE,
    INDICATORS
)
from finrl.config_tickers import DOW_30_TICKER
from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv

def run_test(
    model_name="ppo",
    drl_lib="stable_baselines3",
    cwd="./trained_models/ppo"
):
    """
    Teste un mod√®le entra√Æn√© sur la p√©riode de test.
    
    Returns:
    --------
    episode_total_assets : list
        Liste des valeurs totales du portefeuille √† chaque pas
    """
    
    account_value = test(
        start_date=TEST_START_DATE,
        end_date=TEST_END_DATE,
        ticker_list=DOW_30_TICKER,
        data_source="yahoofinance",
        time_interval="1D",
        technical_indicator_list=INDICATORS,
        drl_lib=drl_lib,
        env=StockTradingEnv,
        model_name=model_name,
        cwd=cwd,
        if_vix=True,
        net_dimension=512,  # Pour ElegantRL
    )
    
    print(f"‚úÖ Test termin√©")
    print(f"   Capital initial: $1,000,000")
    print(f"   Capital final: ${account_value[-1]:,.2f}")
    print(f"   Rendement: {(account_value[-1]/1_000_000 - 1)*100:.2f}%")
    
    return account_value

# Ex√©cution
if __name__ == "__main__":
    assets = run_test(
        model_name="ppo",
        drl_lib="stable_baselines3",
        cwd="./trained_models/ppo"
    )
```

## 8.3 Script de Trading (trade.py)

```python
# ============================================================
# PIPELINE DE TRADING
# ============================================================

from finrl.trade import trade
from finrl.config import (
    TRADE_START_DATE, TRADE_END_DATE,
    INDICATORS, ALPACA_API_BASE_URL
)
from finrl.config_tickers import DOW_30_TICKER
from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv

def run_trade(
    model_name="ppo",
    drl_lib="stable_baselines3",
    trade_mode="backtesting",  # ou "paper_trading"
    cwd="./trained_models/ppo",
    api_key=None,
    api_secret=None
):
    """
    Ex√©cute le trading avec un mod√®le entra√Æn√©.
    
    Parameters:
    -----------
    trade_mode : str
        "backtesting" : simulation sur donn√©es historiques
        "paper_trading" : trading simul√© en temps r√©el via Alpaca
    """
    
    kwargs = {}
    
    if trade_mode == "paper_trading":
        # Dimensions n√©cessaires pour paper trading
        kwargs["state_dim"] = len(DOW_30_TICKER) * (len(INDICATORS) + 3) + 3
        kwargs["action_dim"] = len(DOW_30_TICKER)
        kwargs["net_dimension"] = 512
    
    trade(
        start_date=TRADE_START_DATE,
        end_date=TRADE_END_DATE,
        ticker_list=DOW_30_TICKER,
        data_source="yahoofinance",
        time_interval="1D",
        technical_indicator_list=INDICATORS,
        drl_lib=drl_lib,
        env=StockTradingEnv,
        model_name=model_name,
        API_KEY=api_key or "xxx",
        API_SECRET=api_secret or "xxx",
        API_BASE_URL=ALPACA_API_BASE_URL,
        trade_mode=trade_mode,
        cwd=cwd,
        if_vix=True,
        **kwargs
    )
    
    print(f"‚úÖ Trading {'simul√©' if trade_mode == 'backtesting' else 'paper'} termin√©")

# Ex√©cution
if __name__ == "__main__":
    # Backtesting
    run_trade(
        model_name="ppo",
        trade_mode="backtesting"
    )
```

## 8.4 Pipeline Complet avec Pr√©diction

```python
# ============================================================
# PR√âDICTION AVEC MOD√àLE ENTRA√éN√â
# ============================================================

from finrl.agents.stablebaselines3.models import DRLAgent

def predict_with_model(
    model_name,
    trade_df,
    env_kwargs,
    model_path
):
    """
    Effectue des pr√©dictions avec un mod√®le entra√Æn√©.
    
    Parameters:
    -----------
    model_name : str
        Nom du mod√®le
    trade_df : pd.DataFrame
        Donn√©es de trading
    env_kwargs : dict
        Configuration de l'environnement
    model_path : str
        Chemin vers le mod√®le sauvegard√©
    
    Returns:
    --------
    df_account_value : pd.DataFrame
        Valeur du compte √† chaque √©tape
    df_actions : pd.DataFrame
        Actions prises √† chaque √©tape
    """
    
    # Cr√©er l'environnement de trading
    e_trade = StockTradingEnv(
        df=trade_df,
        turbulence_threshold=70,
        risk_indicator_col='vix',
        **env_kwargs
    )
    
    # Charger et pr√©dire
    df_account_value, df_actions = DRLAgent.DRL_prediction_load_from_file(
        model_name=model_name,
        environment=e_trade,
        cwd=model_path,
        deterministic=True  # Actions d√©terministes (pas d'exploration)
    )
    
    return df_account_value, df_actions

# Utilisation
df_account, df_actions = predict_with_model(
    model_name="ppo",
    trade_df=trade,
    env_kwargs=env_kwargs,
    model_path="./trained_models/ppo"
)

print(f"Rendement final: {(df_account['account_value'].iloc[-1] / 1_000_000 - 1) * 100:.2f}%")
```

---

# 9. Backtesting et Visualisation

## 9.1 Statistiques de Performance

```python
# ============================================================
# CALCUL DES STATISTIQUES DE BACKTESTING
# ============================================================

from finrl.plot import backtest_stats, get_daily_return

def calculate_performance_metrics(df_account_value):
    """
    Calcule les m√©triques de performance standard.
    
    Parameters:
    -----------
    df_account_value : pd.DataFrame
        DataFrame avec colonnes ['date', 'account_value']
    
    Returns:
    --------
    dict : Dictionnaire des m√©triques
    """
    
    # Utiliser pyfolio pour les stats
    perf_stats = backtest_stats(
        df_account_value,
        value_col_name="account_value"
    )
    
    return perf_stats

# Exemple de sortie:
"""
                             Backtest
Annual return                  23.5%
Cumulative returns             89.2%
Annual volatility              15.3%
Sharpe ratio                    1.54
Calmar ratio                    2.31
Stability                       0.95
Max drawdown                  -10.2%
Omega ratio                     1.28
Sortino ratio                   2.15
Skew                           -0.12
Kurtosis                        3.45
Tail ratio                      1.08
Daily value at risk            -1.5%
"""
```

## 9.2 Comparaison avec Benchmark

```python
# ============================================================
# COMPARAISON AVEC UN BENCHMARK (ex: DJI)
# ============================================================

from finrl.plot import get_baseline, backtest_plot
import matplotlib.pyplot as plt

def compare_with_benchmark(
    df_account_value,
    benchmark_ticker="^DJI",
    start_date=TRADE_START_DATE,
    end_date=TRADE_END_DATE,
    initial_amount=1_000_000
):
    """
    Compare la strat√©gie DRL avec un benchmark.
    """
    
    # Obtenir les donn√©es du benchmark
    df_baseline = get_baseline(
        ticker=benchmark_ticker,
        start=start_date,
        end=end_date
    )
    
    # Normaliser le benchmark au m√™me capital initial
    df_baseline_normalized = df_baseline.copy()
    df_baseline_normalized['account_value'] = (
        df_baseline['close'] / df_baseline['close'].iloc[0] * initial_amount
    )
    
    # Statistiques du benchmark
    print("="*50)
    print(f"STATISTIQUES DU BENCHMARK ({benchmark_ticker})")
    print("="*50)
    baseline_stats = backtest_stats(df_baseline, value_col_name='close')
    
    # Cr√©er le graphique complet (tear sheet)
    backtest_plot(
        df_account_value,
        baseline_start=start_date,
        baseline_end=end_date,
        baseline_ticker=benchmark_ticker,
        value_col_name="account_value"
    )
    
    return df_baseline_normalized

# Ex√©cution
df_dji = compare_with_benchmark(df_account_value_ppo)
```

## 9.3 Visualisation des Rendements

```python
# ============================================================
# VISUALISATION DES RENDEMENTS
# ============================================================

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

def plot_cumulative_returns(results_dict, initial_amount=1_000_000):
    """
    Trace les rendements cumul√©s de plusieurs strat√©gies.
    
    Parameters:
    -----------
    results_dict : dict
        {nom_strategie: df_account_value}
    """
    
    plt.figure(figsize=(15, 8))
    
    colors = ['blue', 'red', 'green', 'orange', 'purple', 'brown', 'black']
    
    for i, (name, df) in enumerate(results_dict.items()):
        # Calculer les rendements cumul√©s
        returns = (df['account_value'] / initial_amount - 1) * 100
        plt.plot(df['date'], returns, label=name, color=colors[i % len(colors)], linewidth=1.5)
    
    plt.xlabel('Date', fontsize=12)
    plt.ylabel('Rendement Cumul√© (%)', fontsize=12)
    plt.title('Comparaison des Strat√©gies DRL', fontsize=14)
    plt.legend(loc='best', fontsize=10)
    plt.grid(True, alpha=0.3)
    plt.xticks(rotation=45)
    plt.tight_layout()
    
    plt.savefig('results/cumulative_returns.png', dpi=300)
    plt.show()

# Utilisation
results = {
    'A2C': df_account_value_a2c,
    'PPO': df_account_value_ppo,
    'DDPG': df_account_value_ddpg,
    'TD3': df_account_value_td3,
    'SAC': df_account_value_sac,
    'DJI (Benchmark)': df_dji,
}

plot_cumulative_returns(results)
```

## 9.4 Analyse des Transactions

```python
# ============================================================
# ANALYSE DES TRANSACTIONS
# ============================================================

from finrl.plot import trx_plot

def analyze_transactions(df_trade, df_actions, ticker_list):
    """
    Visualise les signaux d'achat/vente pour chaque actif.
    """
    
    # Tracer pour chaque ticker
    trx_plot(
        df_trade=df_trade,
        df_actions=df_actions,
        ticker_list=ticker_list
    )

# Analyse d√©taill√©e des actions
def summarize_actions(df_actions):
    """
    R√©sume les actions prises par l'agent.
    """
    
    # Compter les transactions par ticker
    transactions = {}
    for col in df_actions.columns:
        if col != 'date':
            buys = (df_actions[col] > 0).sum()
            sells = (df_actions[col] < 0).sum()
            holds = (df_actions[col] == 0).sum()
            transactions[col] = {'Achats': buys, 'Ventes': sells, 'Holds': holds}
    
    df_summary = pd.DataFrame(transactions).T
    df_summary['Total Transactions'] = df_summary['Achats'] + df_summary['Ventes']
    
    print("="*60)
    print("R√âSUM√â DES TRANSACTIONS PAR ACTIF")
    print("="*60)
    print(df_summary.sort_values('Total Transactions', ascending=False).head(10))
    
    return df_summary

# Ex√©cution
summarize_actions(df_actions_ppo)
```

## 9.5 Tableau de Bord Complet

```python
# ============================================================
# TABLEAU DE BORD DE PERFORMANCE
# ============================================================

def create_performance_dashboard(
    strategy_name,
    df_account_value,
    df_actions,
    df_baseline,
    initial_amount=1_000_000
):
    """
    Cr√©e un tableau de bord complet de performance.
    """
    
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    
    # 1. √âvolution du portefeuille
    ax1 = axes[0, 0]
    ax1.plot(df_account_value['date'], df_account_value['account_value'], 
             label=strategy_name, color='blue')
    ax1.plot(df_baseline['date'], df_baseline['account_value'], 
             label='Benchmark', color='gray', linestyle='--')
    ax1.set_title('√âvolution du Portefeuille')
    ax1.set_xlabel('Date')
    ax1.set_ylabel('Valeur ($)')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. Drawdown
    ax2 = axes[0, 1]
    running_max = df_account_value['account_value'].cummax()
    drawdown = (df_account_value['account_value'] - running_max) / running_max * 100
    ax2.fill_between(df_account_value['date'], drawdown, 0, color='red', alpha=0.3)
    ax2.set_title('Drawdown')
    ax2.set_xlabel('Date')
    ax2.set_ylabel('Drawdown (%)')
    ax2.grid(True, alpha=0.3)
    
    # 3. Distribution des rendements
    ax3 = axes[1, 0]
    daily_returns = df_account_value['account_value'].pct_change().dropna() * 100
    ax3.hist(daily_returns, bins=50, color='blue', alpha=0.7, edgecolor='black')
    ax3.axvline(daily_returns.mean(), color='red', linestyle='--', 
                label=f'Moyenne: {daily_returns.mean():.2f}%')
    ax3.set_title('Distribution des Rendements Journaliers')
    ax3.set_xlabel('Rendement (%)')
    ax3.set_ylabel('Fr√©quence')
    ax3.legend()
    
    # 4. M√©triques cl√©s
    ax4 = axes[1, 1]
    ax4.axis('off')
    
    # Calculer les m√©triques
    final_value = df_account_value['account_value'].iloc[-1]
    total_return = (final_value / initial_amount - 1) * 100
    sharpe = daily_returns.mean() / daily_returns.std() * np.sqrt(252)
    max_dd = drawdown.min()
    
    metrics_text = f"""
    {'='*40}
    M√âTRIQUES DE PERFORMANCE - {strategy_name}
    {'='*40}
    
    Capital Initial:     ${initial_amount:,.0f}
    Capital Final:       ${final_value:,.0f}
    
    Rendement Total:     {total_return:.2f}%
    Sharpe Ratio:        {sharpe:.2f}
    Max Drawdown:        {max_dd:.2f}%
    
    Rendement Annualis√©: {total_return * 252 / len(daily_returns):.2f}%
    Volatilit√© Ann.:     {daily_returns.std() * np.sqrt(252):.2f}%
    
    Nombre de Jours:     {len(daily_returns)}
    Jours Positifs:      {(daily_returns > 0).sum()}
    Jours N√©gatifs:      {(daily_returns < 0).sum()}
    """
    
    ax4.text(0.1, 0.5, metrics_text, transform=ax4.transAxes, 
             fontsize=11, verticalalignment='center', fontfamily='monospace')
    
    plt.tight_layout()
    plt.savefig(f'results/dashboard_{strategy_name}.png', dpi=300)
    plt.show()

# Utilisation
create_performance_dashboard(
    strategy_name="PPO",
    df_account_value=df_account_value_ppo,
    df_actions=df_actions_ppo,
    df_baseline=df_dji
)
```

---

# 10. Paper Trading

## 10.1 Configuration Alpaca

```python
# ============================================================
# PAPER TRADING AVEC ALPACA
# ============================================================

"""
Alpaca Markets offre:
- API gratuite pour paper trading
- Donn√©es en temps r√©el
- Ex√©cution simul√©e r√©aliste

√âtapes:
1. Cr√©er un compte sur https://alpaca.markets
2. Obtenir API Key et Secret
3. Configurer l'environnement
"""

# Configuration API
ALPACA_CONFIG = {
    "API_KEY": "votre_api_key",
    "API_SECRET": "votre_api_secret",
    "API_BASE_URL": "https://paper-api.alpaca.markets",  # Paper trading
    # Pour live trading: "https://api.alpaca.markets"
}
```

## 10.2 Classe AlpacaPaperTrading

```python
# ============================================================
# ENVIRONNEMENT DE PAPER TRADING
# ============================================================

from finrl.meta.env_stock_trading.env_stock_papertrading import AlpacaPaperTrading

def start_paper_trading(
    model_name,
    model_path,
    ticker_list,
    indicators,
    api_key,
    api_secret,
    api_base_url
):
    """
    D√©marre le paper trading en temps r√©el.
    """
    
    # Calculer les dimensions
    stock_dim = len(ticker_list)
    state_dim = 1 + 2 + 3 * stock_dim + len(indicators) * stock_dim
    action_dim = stock_dim
    
    # Cr√©er l'instance de paper trading
    paper_trading = AlpacaPaperTrading(
        ticker_list=ticker_list,
        time_interval="1Min",         # Intervalle de trading (1 minute)
        drl_lib="stable_baselines3",
        model_name=model_name,
        cwd=model_path,
        net_dim=512,                  # Dimension du r√©seau
        state_dim=state_dim,
        action_dim=action_dim,
        API_KEY=api_key,
        API_SECRET=api_secret,
        API_BASE_URL=api_base_url,
        tech_indicator_list=indicators,
        turbulence_thresh=30,         # Seuil de turbulence
        max_stock=1e2,                # Position max par stock
        latency=None                  # Pas de latence simul√©e
    )
    
    # Lancer le trading (boucle infinie)
    print("üöÄ D√©marrage du paper trading...")
    print("   Appuyez sur Ctrl+C pour arr√™ter")
    
    try:
        paper_trading.run()
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è Paper trading arr√™t√©")

# Ex√©cution
if __name__ == "__main__":
    start_paper_trading(
        model_name="ppo",
        model_path="./trained_models/ppo",
        ticker_list=["AAPL", "MSFT", "GOOGL"],  # Petit portefeuille pour tester
        indicators=INDICATORS,
        api_key=ALPACA_CONFIG["API_KEY"],
        api_secret=ALPACA_CONFIG["API_SECRET"],
        api_base_url=ALPACA_CONFIG["API_BASE_URL"]
    )
```

## 10.3 Monitoring du Paper Trading

```python
# ============================================================
# MONITORING DU PAPER TRADING
# ============================================================

import alpaca_trade_api as tradeapi

def monitor_account(api_key, api_secret, api_base_url):
    """
    Affiche l'√©tat actuel du compte Alpaca.
    """
    
    api = tradeapi.REST(api_key, api_secret, api_base_url, api_version='v2')
    
    # Informations du compte
    account = api.get_account()
    
    print("="*50)
    print("√âTAT DU COMPTE ALPACA")
    print("="*50)
    print(f"ID du compte: {account.id}")
    print(f"Status: {account.status}")
    print(f"\nCash: ${float(account.cash):,.2f}")
    print(f"Valeur du portefeuille: ${float(account.portfolio_value):,.2f}")
    print(f"Equity: ${float(account.equity):,.2f}")
    
    # Positions actuelles
    positions = api.list_positions()
    
    if positions:
        print(f"\nPOSITIONS ({len(positions)}):")
        print("-"*50)
        for pos in positions:
            pl = float(pos.unrealized_pl)
            pl_pct = float(pos.unrealized_plpc) * 100
            print(f"  {pos.symbol}: {pos.qty} actions @ ${float(pos.avg_entry_price):.2f}")
            print(f"    P/L: ${pl:,.2f} ({pl_pct:+.2f}%)")
    else:
        print("\nAucune position ouverte")
    
    # Ordres r√©cents
    orders = api.list_orders(status='all', limit=5)
    
    if orders:
        print(f"\nDERNIERS ORDRES:")
        print("-"*50)
        for order in orders:
            print(f"  {order.side.upper()} {order.symbol}: {order.qty} @ {order.type}")
            print(f"    Status: {order.status}")

# Ex√©cution
monitor_account(
    ALPACA_CONFIG["API_KEY"],
    ALPACA_CONFIG["API_SECRET"],
    ALPACA_CONFIG["API_BASE_URL"]
)
```

---

# 11. Exemples Complets

## 11.1 Exemple Complet: Trading DOW 30

```python
# ============================================================
# EXEMPLE COMPLET: TRADING DOW 30 AVEC MULTIPLE MOD√àLES
# ============================================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from finrl.meta.preprocessor.yahoodownloader import YahooDownloader
from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split
from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv
from finrl.agents.stablebaselines3.models import DRLAgent
from finrl.plot import backtest_stats, get_baseline
from finrl.config import INDICATORS
from finrl.config_tickers import DOW_30_TICKER
from stable_baselines3.common.logger import configure
import itertools
import os

# ============================================================
# √âTAPE 1: CONFIGURATION
# ============================================================

TRAIN_START_DATE = '2010-01-01'
TRAIN_END_DATE = '2021-10-01'
TRADE_START_DATE = '2021-10-01'
TRADE_END_DATE = '2023-03-01'

# Cr√©er les r√©pertoires
os.makedirs('datasets', exist_ok=True)
os.makedirs('trained_models', exist_ok=True)
os.makedirs('results', exist_ok=True)

print("üìä Configuration:")
print(f"   Train: {TRAIN_START_DATE} ‚Üí {TRAIN_END_DATE}")
print(f"   Trade: {TRADE_START_DATE} ‚Üí {TRADE_END_DATE}")
print(f"   Tickers: {len(DOW_30_TICKER)} actions (DOW 30)")

# ============================================================
# √âTAPE 2: T√âL√âCHARGEMENT ET PR√âTRAITEMENT
# ============================================================

print("\nüì• T√©l√©chargement des donn√©es...")
df = YahooDownloader(
    start_date=TRAIN_START_DATE,
    end_date=TRADE_END_DATE,
    ticker_list=DOW_30_TICKER
).fetch_data()

print(f"   Donn√©es brutes: {df.shape}")

# Feature Engineering
print("\nüîß Feature Engineering...")
fe = FeatureEngineer(
    use_technical_indicator=True,
    tech_indicator_list=INDICATORS,
    use_vix=True,
    use_turbulence=True,
    user_defined_feature=False
)
processed = fe.preprocess_data(df)

# Compl√©ter les donn√©es manquantes
list_ticker = processed["tic"].unique().tolist()
list_date = list(pd.date_range(
    processed['date'].min(), 
    processed['date'].max()
).astype(str))

combination = list(itertools.product(list_date, list_ticker))
processed_full = pd.DataFrame(
    combination, 
    columns=["date", "tic"]
).merge(processed, on=["date", "tic"], how="left")

processed_full = processed_full[processed_full['date'].isin(processed['date'])]
processed_full = processed_full.sort_values(['date', 'tic']).fillna(0)

print(f"   Donn√©es trait√©es: {processed_full.shape}")

# ============================================================
# √âTAPE 3: DIVISION TRAIN/TRADE
# ============================================================

train = data_split(processed_full, TRAIN_START_DATE, TRAIN_END_DATE)
trade = data_split(processed_full, TRADE_START_DATE, TRADE_END_DATE)

print(f"\nüìä Division des donn√©es:")
print(f"   Train: {len(train)} lignes, {len(train.date.unique())} jours")
print(f"   Trade: {len(trade)} lignes, {len(trade.date.unique())} jours")

# ============================================================
# √âTAPE 4: CONFIGURATION DE L'ENVIRONNEMENT
# ============================================================

stock_dimension = len(train.tic.unique())
state_space = 1 + 2 * stock_dimension + len(INDICATORS) * stock_dimension

env_kwargs = {
    "hmax": 100,
    "initial_amount": 1_000_000,
    "num_stock_shares": [0] * stock_dimension,
    "buy_cost_pct": [0.001] * stock_dimension,
    "sell_cost_pct": [0.001] * stock_dimension,
    "state_space": state_space,
    "stock_dim": stock_dimension,
    "tech_indicator_list": INDICATORS,
    "action_space": stock_dimension,
    "reward_scaling": 1e-4
}

e_train_gym = StockTradingEnv(df=train, **env_kwargs)
env_train, _ = e_train_gym.get_sb_env()

print(f"\nüéÆ Environnement configur√©:")
print(f"   Stock dimension: {stock_dimension}")
print(f"   State space: {state_space}")

# ============================================================
# √âTAPE 5: ENTRA√éNEMENT DES MOD√àLES
# ============================================================

TIMESTEPS = 50_000  # R√©duire pour test rapide

models = {}
results = {}

# Liste des mod√®les √† entra√Æner
model_configs = {
    "a2c": {"n_steps": 5, "ent_coef": 0.01, "learning_rate": 0.0007},
    "ppo": {"n_steps": 2048, "ent_coef": 0.01, "learning_rate": 0.00025, "batch_size": 64},
    "ddpg": {"batch_size": 128, "buffer_size": 50000, "learning_rate": 0.001},
    "sac": {"batch_size": 64, "buffer_size": 100000, "learning_rate": 0.0001, "learning_starts": 100, "ent_coef": "auto_0.1"},
}

for model_name, params in model_configs.items():
    print(f"\nüöÄ Entra√Ænement {model_name.upper()}...")
    
    agent = DRLAgent(env=env_train)
    model = agent.get_model(model_name, model_kwargs=params)
    
    # Logger
    tmp_path = f'results/{model_name}'
    os.makedirs(tmp_path, exist_ok=True)
    logger = configure(tmp_path, ["stdout", "csv"])
    model.set_logger(logger)
    
    # Entra√Ænement
    trained = agent.train_model(
        model=model,
        tb_log_name=model_name,
        total_timesteps=TIMESTEPS
    )
    
    trained.save(f"trained_models/{model_name}")
    models[model_name] = trained
    print(f"   ‚úÖ {model_name.upper()} entra√Æn√© et sauvegard√©")

# ============================================================
# √âTAPE 6: TEST ET PR√âDICTION
# ============================================================

print("\nüìà Test des mod√®les...")

# Seuil de risque
data_risk = processed_full[
    (processed_full.date < TRAIN_END_DATE) & 
    (processed_full.date >= TRAIN_START_DATE)
].drop_duplicates(subset=['date'])
turbulence_thresh = data_risk.turbulence.quantile(0.996)

# Environnement de trading
e_trade_gym = StockTradingEnv(
    df=trade,
    turbulence_threshold=turbulence_thresh,
    risk_indicator_col='turbulence',
    **env_kwargs
)

for model_name, trained_model in models.items():
    print(f"\n   Testing {model_name.upper()}...")
    
    df_account, df_actions = DRLAgent.DRL_prediction(
        model=trained_model,
        environment=e_trade_gym
    )
    
    results[model_name] = {
        'account': df_account,
        'actions': df_actions,
        'final_value': df_account['account_value'].iloc[-1],
        'return': (df_account['account_value'].iloc[-1] / 1_000_000 - 1) * 100
    }
    
    print(f"      Rendement: {results[model_name]['return']:.2f}%")

# ============================================================
# √âTAPE 7: BENCHMARK
# ============================================================

print("\nüìä Calcul du benchmark...")

df_dji = get_baseline(
    ticker="^DJI",
    start=TRADE_START_DATE,
    end=TRADE_END_DATE
)

# Normaliser
initial_price = df_dji['close'].iloc[0]
df_dji['account_value'] = df_dji['close'] / initial_price * 1_000_000
benchmark_return = (df_dji['account_value'].iloc[-1] / 1_000_000 - 1) * 100

print(f"   DJI Benchmark: {benchmark_return:.2f}%")

# ============================================================
# √âTAPE 8: VISUALISATION FINALE
# ============================================================

print("\nüìä Cr√©ation des visualisations...")

plt.figure(figsize=(15, 8))

colors = {'a2c': 'blue', 'ppo': 'green', 'ddpg': 'orange', 'sac': 'red'}

for model_name, result in results.items():
    df = result['account']
    returns = (df['account_value'] / 1_000_000 - 1) * 100
    plt.plot(df['date'], returns, label=f"{model_name.upper()} ({result['return']:.1f}%)", 
             color=colors.get(model_name, 'gray'), linewidth=1.5)

# Benchmark
bench_returns = (df_dji['account_value'] / 1_000_000 - 1) * 100
plt.plot(df_dji['date'], bench_returns, label=f"DJI ({benchmark_return:.1f}%)", 
         color='black', linestyle='--', linewidth=2)

plt.xlabel('Date', fontsize=12)
plt.ylabel('Rendement Cumul√© (%)', fontsize=12)
plt.title('Comparaison des Strat√©gies DRL vs Benchmark (DOW 30)', fontsize=14)
plt.legend(loc='upper left', fontsize=10)
plt.grid(True, alpha=0.3)
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig('results/comparison_final.png', dpi=300)
plt.show()

# ============================================================
# √âTAPE 9: RAPPORT FINAL
# ============================================================

print("\n" + "="*60)
print("RAPPORT FINAL")
print("="*60)
print(f"\nP√©riode de trading: {TRADE_START_DATE} ‚Üí {TRADE_END_DATE}")
print(f"Capital initial: $1,000,000")
print("\n" + "-"*60)
print(f"{'Mod√®le':<15} {'Valeur Finale':>20} {'Rendement':>15}")
print("-"*60)

for model_name in sorted(results.keys(), key=lambda x: results[x]['return'], reverse=True):
    r = results[model_name]
    print(f"{model_name.upper():<15} ${r['final_value']:>18,.0f} {r['return']:>14.2f}%")

print("-"*60)
print(f"{'DJI (Benchmark)':<15} ${df_dji['account_value'].iloc[-1]:>18,.0f} {benchmark_return:>14.2f}%")
print("="*60)

# Meilleur mod√®le
best_model = max(results.keys(), key=lambda x: results[x]['return'])
print(f"\nüèÜ Meilleur mod√®le: {best_model.upper()} avec {results[best_model]['return']:.2f}%")
```

---

# 12. Optimisation des Hyperparam√®tres

## 12.1 Optuna pour l'Optimisation

```python
# ============================================================
# OPTIMISATION AVEC OPTUNA
# ============================================================

import optuna
from stable_baselines3 import PPO

def objective(trial):
    """
    Fonction objectif pour Optuna.
    Retourne le rendement n√©gatif (Optuna minimise).
    """
    
    # Hyperparam√®tres √† optimiser
    params = {
        "n_steps": trial.suggest_categorical("n_steps", [512, 1024, 2048, 4096]),
        "batch_size": trial.suggest_categorical("batch_size", [32, 64, 128, 256]),
        "learning_rate": trial.suggest_float("learning_rate", 1e-5, 1e-3, log=True),
        "ent_coef": trial.suggest_float("ent_coef", 0.001, 0.1, log=True),
        "gamma": trial.suggest_float("gamma", 0.9, 0.9999, log=True),
        "gae_lambda": trial.suggest_float("gae_lambda", 0.9, 1.0),
    }
    
    # Cr√©er et entra√Æner le mod√®le
    agent = DRLAgent(env=env_train)
    model = agent.get_model("ppo", model_kwargs=params)
    
    trained = agent.train_model(
        model=model,
        tb_log_name="optuna",
        total_timesteps=20_000  # R√©duit pour optimisation rapide
    )
    
    # √âvaluer
    df_account, _ = DRLAgent.DRL_prediction(
        model=trained,
        environment=e_trade_gym
    )
    
    final_return = df_account['account_value'].iloc[-1] / 1_000_000 - 1
    
    return -final_return  # N√©gatif car Optuna minimise

# Lancer l'optimisation
study = optuna.create_study(direction="minimize")
study.optimize(objective, n_trials=50, timeout=3600)  # 50 trials ou 1 heure

print("Meilleurs hyperparam√®tres:")
print(study.best_params)
print(f"Meilleur rendement: {-study.best_value * 100:.2f}%")
```

## 12.2 Grid Search Manuel

```python
# ============================================================
# GRID SEARCH MANUEL
# ============================================================

from itertools import product

def grid_search_ppo():
    """
    Grid search sur les hyperparam√®tres PPO.
    """
    
    # Grille de param√®tres
    param_grid = {
        "n_steps": [1024, 2048],
        "batch_size": [64, 128],
        "learning_rate": [1e-4, 2.5e-4],
        "ent_coef": [0.01, 0.02],
    }
    
    # G√©n√©rer toutes les combinaisons
    keys = list(param_grid.keys())
    combinations = list(product(*param_grid.values()))
    
    results = []
    
    for combo in combinations:
        params = dict(zip(keys, combo))
        print(f"\nTest: {params}")
        
        try:
            agent = DRLAgent(env=env_train)
            model = agent.get_model("ppo", model_kwargs=params)
            
            trained = agent.train_model(
                model=model,
                tb_log_name="grid",
                total_timesteps=30_000
            )
            
            df_account, _ = DRLAgent.DRL_prediction(
                model=trained,
                environment=e_trade_gym
            )
            
            final_return = (df_account['account_value'].iloc[-1] / 1_000_000 - 1) * 100
            
            results.append({**params, 'return': final_return})
            print(f"   Rendement: {final_return:.2f}%")
            
        except Exception as e:
            print(f"   Erreur: {e}")
            results.append({**params, 'return': None})
    
    # Trier par rendement
    df_results = pd.DataFrame(results)
    df_results = df_results.sort_values('return', ascending=False)
    
    print("\n" + "="*60)
    print("TOP 5 CONFIGURATIONS")
    print("="*60)
    print(df_results.head())
    
    return df_results

# Ex√©cution
# results_df = grid_search_ppo()
```

---

# 13. Glossaire DRL

## Termes G√©n√©raux

| Acronyme | Signification | Description |
|----------|---------------|-------------|
| **DRL** | Deep Reinforcement Learning | Apprentissage par renforcement avec r√©seaux de neurones profonds |
| **RL** | Reinforcement Learning | Apprentissage par renforcement |
| **MDP** | Markov Decision Process | Formalisation math√©matique du probl√®me RL |
| **POMDP** | Partially Observable MDP | MDP avec observations partielles |

## Algorithmes

| Acronyme | Signification | Type |
|----------|---------------|------|
| **A2C** | Advantage Actor-Critic | On-Policy |
| **A3C** | Asynchronous A2C | On-Policy, Distribu√© |
| **PPO** | Proximal Policy Optimization | On-Policy |
| **TRPO** | Trust Region Policy Optimization | On-Policy |
| **DDPG** | Deep Deterministic Policy Gradient | Off-Policy |
| **TD3** | Twin Delayed DDPG | Off-Policy |
| **SAC** | Soft Actor-Critic | Off-Policy |
| **DQN** | Deep Q-Network | Off-Policy, Discret |

## Concepts RL

| Terme | Description |
|-------|-------------|
| **Policy (œÄ)** | Strat√©gie qui mappe √©tats vers actions |
| **Value Function (V)** | Esp√©rance des r√©compenses futures depuis un √©tat |
| **Q-Function (Q)** | Esp√©rance des r√©compenses futures pour un couple (√©tat, action) |
| **Advantage (A)** | Diff√©rence entre Q et V: A(s,a) = Q(s,a) - V(s) |
| **Reward (r)** | R√©compense imm√©diate apr√®s une action |
| **Discount (Œ≥)** | Facteur d'actualisation des r√©compenses futures |
| **Episode** | Une s√©quence compl√®te du d√©but √† la fin |
| **Timestep** | Un pas dans l'environnement |
| **Rollout** | Collection d'exp√©riences pendant plusieurs timesteps |

## Indicateurs Techniques

| Acronyme | Signification |
|----------|---------------|
| **MACD** | Moving Average Convergence Divergence |
| **RSI** | Relative Strength Index |
| **CCI** | Commodity Channel Index |
| **SMA** | Simple Moving Average |
| **EMA** | Exponential Moving Average |
| **VIX** | CBOE Volatility Index |

## M√©triques de Performance

| M√©trique | Description |
|----------|-------------|
| **Sharpe Ratio** | Rendement exc√©dentaire / Volatilit√© |
| **Sortino Ratio** | Sharpe avec volatilit√© √† la baisse seulement |
| **Max Drawdown** | Perte maximale depuis un pic |
| **Calmar Ratio** | Rendement annualis√© / Max Drawdown |
| **Alpha** | Rendement exc√©dentaire vs benchmark |
| **Beta** | Sensibilit√© au march√© |

---

## üìö Ressources Additionnelles

### Documentation Officielle
- [FinRL GitHub](https://github.com/AI4Finance-Foundation/FinRL)
- [FinRL Documentation](https://finrl.readthedocs.io/)
- [Stable Baselines 3](https://stable-baselines3.readthedocs.io/)
- [ElegantRL](https://github.com/AI4Finance-Foundation/ElegantRL)

### Papers Acad√©miques
- "Practical Deep Reinforcement Learning Approach for Stock Trading" (NeurIPS 2018)
- "Deep Reinforcement Learning for Automated Stock Trading" (ICAIF 2020)
- "FinRL-Meta: Market Environments and Benchmarks" (NeurIPS 2022)

### Tutoriels
- [FinRL Tutorials](https://github.com/AI4Finance-Foundation/FinRL-Tutorials)
- [AI4Finance YouTube](https://www.youtube.com/channel/UCrVri6k3KPBa3NhapVV4K5g)

---

**Document cr√©√© pour HelixOne Visual Code**
**Version: 1.0**
**Date: 2025**
