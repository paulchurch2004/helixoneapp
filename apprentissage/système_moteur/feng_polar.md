# üìä Ing√©nierie Financi√®re : Une Perspective du Traitement du Signal

## Guide Complet avec Code Python

**Bas√© sur** : "A Signal Processing Perspective on Financial Engineering" - Feng & Palomar (2016)

---

## üìë Table des Mati√®res

1. [Introduction et Vue d'Ensemble](#1-introduction)
2. [Mod√©lisation des S√©ries Temporelles Financi√®res](#2-mod√©lisation)
3. [Estimation des Param√®tres (Moyenne et Covariance)](#3-estimation)
4. [Optimisation de Portefeuille](#4-portefeuille)
5. [Arbitrage Statistique](#5-arbitrage)
6. [Ex√©cution d'Ordres](#6-execution)

---

## 1. Introduction et Vue d'Ensemble {#1-introduction}

### 1.1 Philosophie du Document

L'ing√©nierie financi√®re et le traitement du signal partagent des fondations math√©matiques communes :

| Ing√©nierie Financi√®re | Traitement du Signal |
|----------------------|---------------------|
| Mod√®le ARMA (AutoRegressive Moving Average) | Mod√®le p√¥le-z√©ro rationnel |
| Estimateur de covariance par shrinkage | Diagonal loading en beamforming |
| Optimisation de portefeuille | Design de filtre/beamforming |
| Index tracking sparse | R√©cup√©ration de signaux sparse |

### 1.2 Les Trois Piliers de l'Investissement Quantitatif

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ     Mod√©lisation Financi√®re     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚îÇ
                ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Strat√©gies d'Investissement   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ Portfolio ‚îÇ  ‚îÇ Arbitrage  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   Optim.  ‚îÇ  ‚îÇStatistique ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚îÇ
                ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ      Ex√©cution d'Ordres         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## 2. Mod√©lisation des S√©ries Temporelles Financi√®res {#2-mod√©lisation}

### 2.1 Prix et Rendements

```python
"""
RENDEMENTS ET LOG-RENDEMENTS
============================

En finance, on travaille avec deux types de rendements :
- Rendement simple (lin√©aire) : R_t = (p_t - p_{t-1}) / p_{t-1}
- Log-rendement : r_t = log(p_t / p_{t-1}) = log(p_t) - log(p_{t-1})

Pourquoi les log-rendements ?
1. Additivit√© temporelle : r_t(k) = r_t + r_{t-1} + ... + r_{t-k+1}
2. Propri√©t√©s statistiques plus simples (distribution plus sym√©trique)
3. Plus faciles √† mod√©liser
"""

import numpy as np
import pandas as pd
from typing import Tuple, List, Optional
from dataclasses import dataclass
from scipy import stats
from scipy.optimize import minimize
import matplotlib.pyplot as plt

def compute_returns(prices: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    """
    Calcule les rendements simples et log-rendements √† partir des prix.
    
    Le rendement simple mesure le profit relatif :
        R_t = (prix_final - prix_initial) / prix_initial
    
    Le log-rendement est l'approximation continue :
        r_t = ln(1 + R_t) ‚âà R_t pour R_t petit
    
    Args:
        prices: Array des prix (shape: [T,] ou [T, N])
        
    Returns:
        (simple_returns, log_returns): Tuple de deux arrays
        
    Exemple:
        >>> prices = np.array([100, 105, 103, 108])
        >>> simple, log = compute_returns(prices)
        >>> print(f"Rendement simple jour 1: {simple[0]:.2%}")  # 5%
        >>> print(f"Log-rendement jour 1: {log[0]:.4f}")        # ~0.0488
    """
    prices = np.asarray(prices)
    
    # Rendement simple: R_t = p_t/p_{t-1} - 1
    simple_returns = prices[1:] / prices[:-1] - 1
    
    # Log-rendement: r_t = log(p_t) - log(p_{t-1})
    log_returns = np.log(prices[1:]) - np.log(prices[:-1])
    
    return simple_returns, log_returns


def portfolio_return(weights: np.ndarray, returns: np.ndarray) -> float:
    """
    Calcule le rendement d'un portefeuille.
    
    Le rendement d'un portefeuille est la somme pond√©r√©e des rendements :
        R_p = Œ£ w_i * R_i = w^T * R
    
    Args:
        weights: Vecteur des poids (doit sommer √† 1 pour un portefeuille long-only)
        returns: Vecteur des rendements des actifs
        
    Returns:
        Rendement du portefeuille
        
    Exemple:
        >>> w = np.array([0.6, 0.4])  # 60% action A, 40% action B
        >>> r = np.array([0.05, -0.02])  # A: +5%, B: -2%
        >>> portfolio_return(w, r)  # 0.6*0.05 + 0.4*(-0.02) = 0.022 = 2.2%
    """
    return np.dot(weights, returns)
```

### 2.2 Structure G√©n√©rale d'un Mod√®le

```python
"""
STRUCTURE G√âN√âRALE DES MOD√àLES FINANCIERS
=========================================

La plupart des mod√®les d√©composent le log-rendement r_t comme :

    r_t = Œº_t + w_t

O√π :
- Œº_t = E[r_t | F_{t-1}] : moyenne conditionnelle (partie pr√©visible)
- w_t : bruit blanc avec covariance Œ£_t (partie impr√©visible)

Les mod√®les diff√®rent par la fa√ßon dont ils sp√©cifient Œº_t et Œ£_t.
"""

@dataclass
class FinancialModel:
    """
    Classe de base pour les mod√®les financiers.
    
    Un mod√®le financier doit pouvoir :
    1. Estimer ses param√®tres √† partir de donn√©es
    2. Pr√©dire la moyenne conditionnelle Œº_t
    3. Pr√©dire la covariance conditionnelle Œ£_t
    """
    
    def fit(self, returns: np.ndarray) -> 'FinancialModel':
        """Estime les param√®tres du mod√®le."""
        raise NotImplementedError
    
    def predict_mean(self, history: np.ndarray) -> np.ndarray:
        """Pr√©dit la moyenne conditionnelle Œº_t."""
        raise NotImplementedError
    
    def predict_covariance(self, history: np.ndarray) -> np.ndarray:
        """Pr√©dit la covariance conditionnelle Œ£_t."""
        raise NotImplementedError
```

### 2.3 Mod√®le I.I.D. (Independent and Identically Distributed)

```python
"""
MOD√àLE I.I.D.
=============

Le mod√®le le plus simple : les rendements sont i.i.d. (ind√©pendants et 
identiquement distribu√©s) avec moyenne Œº et covariance Œ£ constantes.

    r_t = Œº + w_t,    w_t ~ N(0, Œ£)

C'est l'hypoth√®se fondamentale de la th√©orie de Markowitz (Nobel 1990).

Avantages :
- Simple √† comprendre et √† estimer
- Base de nombreuses th√©ories fondamentales

Inconv√©nients :
- Ignore la d√©pendance temporelle
- Ignore la volatilit√© variable (clustering de volatilit√©)
"""

class IIDModel(FinancialModel):
    """
    Mod√®le I.I.D. : rendements ind√©pendants avec moyenne et variance constantes.
    
    C'est le mod√®le utilis√© dans l'optimisation de portefeuille classique
    de Markowitz (Mean-Variance Optimization).
    
    Attributs:
        mu: Moyenne des rendements (vecteur N√ó1)
        Sigma: Matrice de covariance (N√óN)
    """
    
    def __init__(self):
        self.mu: Optional[np.ndarray] = None
        self.Sigma: Optional[np.ndarray] = None
    
    def fit(self, returns: np.ndarray) -> 'IIDModel':
        """
        Estime Œº et Œ£ par les estimateurs classiques.
        
        ŒºÃÇ = (1/T) Œ£ r_t           (moyenne empirique)
        Œ£ÃÇ = (1/T) Œ£ (r_t - ŒºÃÇ)(r_t - ŒºÃÇ)^T  (covariance empirique)
        
        Args:
            returns: Matrice T√óN des rendements (T observations, N actifs)
            
        Returns:
            self (pour cha√Ænage)
        """
        # Moyenne empirique (Sample Mean)
        self.mu = np.mean(returns, axis=0)
        
        # Covariance empirique (Sample Covariance Matrix - SCM)
        # Utilise ddof=0 pour √™tre coh√©rent avec la formule MLE
        self.Sigma = np.cov(returns, rowvar=False, ddof=0)
        
        return self
    
    def predict_mean(self, history: np.ndarray = None) -> np.ndarray:
        """La moyenne conditionnelle est constante = Œº."""
        return self.mu
    
    def predict_covariance(self, history: np.ndarray = None) -> np.ndarray:
        """La covariance conditionnelle est constante = Œ£."""
        return self.Sigma


# D√©monstration
def demo_iid_model():
    """D√©montre le mod√®le I.I.D. avec des donn√©es simul√©es."""
    np.random.seed(42)
    
    # Param√®tres vrais
    true_mu = np.array([0.001, 0.002, 0.0015])  # 0.1%, 0.2%, 0.15% par jour
    true_Sigma = np.array([
        [0.0004, 0.0002, 0.0001],
        [0.0002, 0.0006, 0.0002],
        [0.0001, 0.0002, 0.0005]
    ])
    
    # G√©n√©rer T=500 observations
    T = 500
    returns = np.random.multivariate_normal(true_mu, true_Sigma, size=T)
    
    # Estimer le mod√®le
    model = IIDModel().fit(returns)
    
    print("=== Mod√®le I.I.D. ===")
    print(f"Moyenne estim√©e: {model.mu}")
    print(f"Vraie moyenne:   {true_mu}")
    print(f"\nCovariance estim√©e:\n{model.Sigma}")
    print(f"\nVraie covariance:\n{true_Sigma}")
    
    return model
```

### 2.4 Mod√®le Factoriel

```python
"""
MOD√àLE FACTORIEL
================

Id√©e cl√© : Le march√© est de grande dimension (N actifs), mais il est 
r√©ellement "pilot√©" par un petit nombre K de facteurs (K << N).

    r_t = œÜ_0 + Œ† * f_t + w_t

O√π :
- œÜ_0 : constante (N√ó1)
- f_t : vecteur des K facteurs (K√ó1)
- Œ† : matrice de chargement des facteurs (N√óK)
- w_t : bruit idiosyncratique (sp√©cifique √† chaque actif)

Exemples de facteurs explicites :
- Rendement du march√© (CAPM : Capital Asset Pricing Model)
- Taille de l'entreprise, ratio book-to-market (Fama-French)
- Momentum, volatilit√© (facteurs multi-facteurs)

Exemples de facteurs cach√©s :
- Composantes principales (PCA : Principal Component Analysis)
"""

class FactorModel(FinancialModel):
    """
    Mod√®le factoriel pour les rendements d'actifs.
    
    D√©compose les rendements en :
    - Composante syst√©matique : Œ† * f_t (expliqu√©e par les facteurs)
    - Composante idiosyncratique : w_t (sp√©cifique √† chaque actif)
    
    La covariance des rendements se d√©compose comme :
        Œ£ = Œ† * Œ£_f * Œ†^T + Œ£_w
    
    o√π Œ£_f est la covariance des facteurs et Œ£_w celle des r√©sidus.
    """
    
    def __init__(self, n_factors: int = 3):
        """
        Args:
            n_factors: Nombre K de facteurs √† utiliser
        """
        self.n_factors = n_factors
        self.phi0: Optional[np.ndarray] = None  # Constante
        self.Pi: Optional[np.ndarray] = None    # Chargements (loadings)
        self.Sigma_f: Optional[np.ndarray] = None  # Covariance des facteurs
        self.Sigma_w: Optional[np.ndarray] = None  # Covariance r√©siduelle
    
    def fit_with_explicit_factors(
        self, 
        returns: np.ndarray, 
        factors: np.ndarray
    ) -> 'FactorModel':
        """
        Estime le mod√®le avec des facteurs explicites (observables).
        
        Utilise la r√©gression lin√©aire :
            r_t = œÜ_0 + Œ† * f_t + w_t
        
        Args:
            returns: Matrice T√óN des rendements
            factors: Matrice T√óK des facteurs
            
        Returns:
            self
        """
        T, N = returns.shape
        K = factors.shape[1]
        
        # Ajouter une constante aux facteurs pour la r√©gression
        X = np.column_stack([np.ones(T), factors])  # T √ó (K+1)
        
        # R√©gression OLS (Ordinary Least Squares) : Œ≤ = (X^T X)^{-1} X^T Y
        beta = np.linalg.lstsq(X, returns, rcond=None)[0]
        
        self.phi0 = beta[0, :]  # Intercept (N,)
        self.Pi = beta[1:, :].T  # Chargements (N √ó K)
        
        # R√©sidus
        residuals = returns - X @ beta
        
        # Covariances
        self.Sigma_f = np.cov(factors, rowvar=False, ddof=0)
        self.Sigma_w = np.cov(residuals, rowvar=False, ddof=0)
        
        return self
    
    def fit_with_pca(self, returns: np.ndarray) -> 'FactorModel':
        """
        Estime le mod√®le avec des facteurs cach√©s via PCA.
        
        PCA (Principal Component Analysis) trouve les directions de 
        variance maximale dans les donn√©es.
        
        Les K premi√®res composantes principales deviennent les facteurs.
        
        Args:
            returns: Matrice T√óN des rendements
            
        Returns:
            self
        """
        T, N = returns.shape
        K = self.n_factors
        
        # Centrer les donn√©es
        mu = np.mean(returns, axis=0)
        returns_centered = returns - mu
        
        # Covariance empirique
        Sigma_emp = np.cov(returns_centered, rowvar=False, ddof=0)
        
        # D√©composition en valeurs propres
        eigenvalues, eigenvectors = np.linalg.eigh(Sigma_emp)
        
        # Trier par ordre d√©croissant
        idx = np.argsort(eigenvalues)[::-1]
        eigenvalues = eigenvalues[idx]
        eigenvectors = eigenvectors[:, idx]
        
        # Garder les K premiers
        E_K = eigenvectors[:, :K]  # N √ó K
        Lambda_K = np.diag(eigenvalues[:K])  # K √ó K
        
        # Dans le mod√®le PCA :
        # - Les chargements sont les vecteurs propres
        # - Les facteurs sont les projections : f_t = E_K^T * r_t
        self.Pi = E_K  # N √ó K
        
        # Reconstruction de la covariance
        # Œ£ = E_K * Œõ_K * E_K^T + Œ£_w
        self.Sigma_f = Lambda_K
        reconstructed = E_K @ Lambda_K @ E_K.T
        self.Sigma_w = Sigma_emp - reconstructed
        
        # Forcer la positivit√© de la covariance r√©siduelle
        # (peut √™tre n√©gative √† cause des erreurs num√©riques)
        self.Sigma_w = np.maximum(self.Sigma_w, 0)
        
        self.phi0 = mu
        
        return self
    
    def get_covariance(self) -> np.ndarray:
        """
        Retourne la matrice de covariance implicite du mod√®le.
        
        Œ£ = Œ† * Œ£_f * Œ†^T + Œ£_w
        """
        return self.Pi @ self.Sigma_f @ self.Pi.T + self.Sigma_w


class CAPM:
    """
    CAPM : Capital Asset Pricing Model
    
    Le mod√®le √† un facteur le plus c√©l√®bre (Sharpe, 1964 - Nobel 1990).
    
    Pour chaque actif i :
        E[r_i] - r_f = Œ≤_i * (E[r_M] - r_f)
    
    O√π :
    - r_f : taux sans risque
    - r_M : rendement du portefeuille de march√©
    - Œ≤_i : sensibilit√© de l'actif au march√©
    - E[r_M] - r_f : prime de risque du march√©
    
    Le Œ≤ mesure le risque syst√©matique (non-diversifiable).
    """
    
    def __init__(self, risk_free_rate: float = 0.0):
        """
        Args:
            risk_free_rate: Taux sans risque (quotidien)
        """
        self.rf = risk_free_rate
        self.betas: Optional[np.ndarray] = None
        self.alphas: Optional[np.ndarray] = None  # Intercepts (devraient √™tre ~0)
    
    def fit(
        self, 
        asset_returns: np.ndarray, 
        market_returns: np.ndarray
    ) -> 'CAPM':
        """
        Estime les betas par r√©gression.
        
        r_i,t - r_f = Œ±_i + Œ≤_i * (r_M,t - r_f) + Œµ_i,t
        
        Args:
            asset_returns: Rendements des actifs (T √ó N)
            market_returns: Rendements du march√© (T,)
        """
        T, N = asset_returns.shape
        
        # Exc√®s de rendements
        excess_asset = asset_returns - self.rf
        excess_market = market_returns - self.rf
        
        self.betas = np.zeros(N)
        self.alphas = np.zeros(N)
        
        for i in range(N):
            # R√©gression simple : y = Œ± + Œ≤*x
            # Œ≤ = Cov(y, x) / Var(x)
            cov = np.cov(excess_asset[:, i], excess_market)[0, 1]
            var_market = np.var(excess_market)
            
            self.betas[i] = cov / var_market
            self.alphas[i] = np.mean(excess_asset[:, i]) - self.betas[i] * np.mean(excess_market)
        
        return self
    
    def expected_return(self, market_premium: float) -> np.ndarray:
        """
        Calcule le rendement esp√©r√© selon le CAPM.
        
        E[r_i] = r_f + Œ≤_i * (E[r_M] - r_f)
        
        Args:
            market_premium: E[r_M] - r_f (prime de risque du march√©)
        """
        return self.rf + self.betas * market_premium
```

### 2.5 Mod√®les ARMA et VAR

```python
"""
MOD√àLES VARMA (Vector AutoRegressive Moving Average)
====================================================

Ces mod√®les capturent la d√©pendance temporelle dans les rendements.

VAR(p) - Vector AutoRegressive d'ordre p :
    r_t = œÜ_0 + Œ¶_1 * r_{t-1} + ... + Œ¶_p * r_{t-p} + w_t

Les matrices Œ¶_i capturent comment les rendements pass√©s 
affectent les rendements pr√©sents.

VMA(q) - Vector Moving Average d'ordre q :
    r_t = Œº + w_t - Œò_1 * w_{t-1} - ... - Œò_q * w_{t-q}

Capture les "chocs" qui persistent quelques p√©riodes.

VARMA(p,q) combine les deux.
"""

class VARModel(FinancialModel):
    """
    Mod√®le VAR(p) : Vector AutoRegressive d'ordre p.
    
    r_t = œÜ_0 + Œ£_{i=1}^p Œ¶_i * r_{t-i} + w_t
    
    La moyenne conditionnelle d√©pend des p observations pass√©es :
        Œº_t = œÜ_0 + Œ£ Œ¶_i * r_{t-i}
    
    La covariance conditionnelle reste constante :
        Œ£_t = Œ£_w
    """
    
    def __init__(self, order: int = 1):
        """
        Args:
            order: Ordre p du mod√®le (nombre de lags)
        """
        self.p = order
        self.phi0: Optional[np.ndarray] = None
        self.Phi: Optional[List[np.ndarray]] = None  # Liste des Œ¶_i
        self.Sigma_w: Optional[np.ndarray] = None
    
    def fit(self, returns: np.ndarray) -> 'VARModel':
        """
        Estime le VAR(p) par OLS (Ordinary Least Squares).
        
        On r√©√©crit le mod√®le en r√©gression :
            r_t = [1, r_{t-1}^T, ..., r_{t-p}^T] * Œ≤ + w_t
        
        Args:
            returns: Matrice T√óN des rendements
        """
        T, N = returns.shape
        p = self.p
        
        if T <= p:
            raise ValueError(f"Pas assez d'observations. T={T} <= p={p}")
        
        # Construire les matrices pour la r√©gression
        # Y = [r_p, r_{p+1}, ..., r_{T-1}]^T  de taille (T-p) √ó N
        Y = returns[p:]
        
        # X = [1, r_{t-1}, ..., r_{t-p}] de taille (T-p) √ó (1 + p*N)
        X = np.ones((T - p, 1 + p * N))
        for t in range(p, T):
            for lag in range(1, p + 1):
                start_col = 1 + (lag - 1) * N
                end_col = 1 + lag * N
                X[t - p, start_col:end_col] = returns[t - lag]
        
        # OLS : Œ≤ = (X^T X)^{-1} X^T Y
        beta = np.linalg.lstsq(X, Y, rcond=None)[0]
        
        # Extraire les param√®tres
        self.phi0 = beta[0]
        self.Phi = []
        for lag in range(1, p + 1):
            start = 1 + (lag - 1) * N
            end = 1 + lag * N
            self.Phi.append(beta[start:end].T)  # N √ó N
        
        # R√©sidus et covariance
        residuals = Y - X @ beta
        self.Sigma_w = np.cov(residuals, rowvar=False, ddof=0)
        
        return self
    
    def predict_mean(self, history: np.ndarray) -> np.ndarray:
        """
        Pr√©dit la moyenne conditionnelle.
        
        Œº_t = œÜ_0 + Œ£ Œ¶_i * r_{t-i}
        
        Args:
            history: Les p derni√®res observations (p √ó N)
        """
        mu = self.phi0.copy()
        for i, Phi_i in enumerate(self.Phi):
            mu += Phi_i @ history[-(i + 1)]
        return mu
    
    def predict_covariance(self, history: np.ndarray = None) -> np.ndarray:
        """La covariance est constante dans un VAR."""
        return self.Sigma_w
```

### 2.6 Mod√®les de Volatilit√© (GARCH)

```python
"""
MOD√àLES DE VOLATILIT√â CONDITIONNELLE
====================================

Les mod√®les pr√©c√©dents supposent Œ£_t constant. En r√©alit√©, 
la volatilit√© varie dans le temps !

Faits stylis√©s de la volatilit√© financi√®re :
1. Clustering : les p√©riodes de haute volatilit√© se regroupent
2. Mean-reversion : la volatilit√© revient vers un niveau moyen
3. Asym√©trie (leverage effect) : les baisses causent plus de volatilit√©

ARCH(m) - AutoRegressive Conditional Heteroskedasticity (Engle, 1982) :
    œÉ¬≤_t = Œ±_0 + Œ£_{i=1}^m Œ±_i * w¬≤_{t-i}

La variance d√©pend des chocs pass√©s au carr√©.

GARCH(m,s) - Generalized ARCH (Bollerslev, 1986) :
    œÉ¬≤_t = Œ±_0 + Œ£_{i=1}^m Œ±_i * w¬≤_{t-i} + Œ£_{j=1}^s Œ≤_j * œÉ¬≤_{t-j}

Ajoute une composante autor√©gressive sur la variance elle-m√™me.
"""

class GARCH11:
    """
    Mod√®le GARCH(1,1) univari√©.
    
    œÉ¬≤_t = œâ + Œ± * w¬≤_{t-1} + Œ≤ * œÉ¬≤_{t-1}
    
    O√π :
    - œâ > 0 : constante
    - Œ± ‚â• 0 : coefficient ARCH (impact des chocs pass√©s)
    - Œ≤ ‚â• 0 : coefficient GARCH (persistence de la volatilit√©)
    - Œ± + Œ≤ < 1 : condition de stationnarit√©
    
    La variance long-terme est : œÉ¬≤ = œâ / (1 - Œ± - Œ≤)
    
    C'est le mod√®le de volatilit√© le plus utilis√© en pratique.
    """
    
    def __init__(self):
        self.omega: float = 0.0
        self.alpha: float = 0.0
        self.beta: float = 0.0
        self.mu: float = 0.0  # Moyenne des rendements
    
    def fit(self, returns: np.ndarray, method: str = 'mle') -> 'GARCH11':
        """
        Estime les param√®tres par MLE (Maximum Likelihood Estimation).
        
        La log-vraisemblance gaussienne conditionnelle est :
            L = -0.5 * Œ£ [log(œÉ¬≤_t) + w¬≤_t/œÉ¬≤_t]
        
        Args:
            returns: Vecteur des rendements (T,)
            method: 'mle' ou 'moment' (m√©thode des moments)
        """
        returns = np.asarray(returns).flatten()
        T = len(returns)
        
        # Moyenne
        self.mu = np.mean(returns)
        residuals = returns - self.mu
        
        # Variance inconditionnelle (pour initialisation)
        var_unconditional = np.var(residuals)
        
        def negative_log_likelihood(params):
            """N√©gatif de la log-vraisemblance (√† minimiser)."""
            omega, alpha, beta = params
            
            # Contraintes
            if omega <= 0 or alpha < 0 or beta < 0 or alpha + beta >= 1:
                return 1e10
            
            sigma2 = np.zeros(T)
            sigma2[0] = var_unconditional
            
            for t in range(1, T):
                sigma2[t] = omega + alpha * residuals[t-1]**2 + beta * sigma2[t-1]
            
            # Log-vraisemblance gaussienne
            ll = -0.5 * np.sum(np.log(sigma2) + residuals**2 / sigma2)
            
            return -ll  # On minimise le n√©gatif
        
        # Initialisation
        x0 = [var_unconditional * 0.05, 0.05, 0.90]
        
        # Optimisation
        result = minimize(
            negative_log_likelihood,
            x0,
            method='Nelder-Mead',
            options={'maxiter': 1000}
        )
        
        self.omega, self.alpha, self.beta = result.x
        
        return self
    
    def forecast_variance(
        self, 
        last_return: float, 
        last_variance: float,
        horizon: int = 1
    ) -> np.ndarray:
        """
        Pr√©vision de la variance sur un horizon donn√©.
        
        œÉ¬≤_{t+h|t} = œÉ¬≤ + (Œ± + Œ≤)^{h-1} * (œÉ¬≤_{t+1|t} - œÉ¬≤)
        
        o√π œÉ¬≤ = œâ/(1-Œ±-Œ≤) est la variance long-terme.
        
        Args:
            last_return: Dernier rendement observ√©
            last_variance: Derni√®re variance
            horizon: Horizon de pr√©vision
            
        Returns:
            Array des variances pr√©vues
        """
        forecasts = np.zeros(horizon)
        
        # Variance long-terme
        var_lt = self.omega / (1 - self.alpha - self.beta)
        
        # Premi√®re pr√©vision
        residual = last_return - self.mu
        forecasts[0] = self.omega + self.alpha * residual**2 + self.beta * last_variance
        
        # Pr√©visions suivantes (convergent vers var_lt)
        persistence = self.alpha + self.beta
        for h in range(1, horizon):
            forecasts[h] = var_lt + persistence**h * (forecasts[0] - var_lt)
        
        return forecasts
    
    @property
    def long_term_variance(self) -> float:
        """Variance long-terme (inconditionnelle)."""
        return self.omega / (1 - self.alpha - self.beta)
    
    @property
    def half_life(self) -> float:
        """
        Demi-vie de la volatilit√©.
        
        Nombre de p√©riodes pour que la moiti√© du choc soit absorb√©e.
        """
        persistence = self.alpha + self.beta
        if persistence >= 1:
            return float('inf')
        return np.log(0.5) / np.log(persistence)


def demo_garch():
    """D√©montre le mod√®le GARCH(1,1)."""
    np.random.seed(42)
    
    # Simuler un processus GARCH(1,1)
    T = 1000
    omega, alpha, beta = 0.00001, 0.05, 0.93
    
    returns = np.zeros(T)
    sigma2 = np.zeros(T)
    sigma2[0] = omega / (1 - alpha - beta)  # Variance long-terme
    
    for t in range(1, T):
        returns[t] = np.sqrt(sigma2[t-1]) * np.random.randn()
        sigma2[t] = omega + alpha * returns[t-1]**2 + beta * sigma2[t-1]
    
    # Estimer le mod√®le
    model = GARCH11().fit(returns)
    
    print("=== Mod√®le GARCH(1,1) ===")
    print(f"Param√®tres vrais:    œâ={omega:.6f}, Œ±={alpha:.2f}, Œ≤={beta:.2f}")
    print(f"Param√®tres estim√©s:  œâ={model.omega:.6f}, Œ±={model.alpha:.2f}, Œ≤={model.beta:.2f}")
    print(f"Persistence Œ±+Œ≤: {model.alpha + model.beta:.4f}")
    print(f"Demi-vie: {model.half_life:.1f} p√©riodes")
    
    return model
```

---

## 3. Estimation des Param√®tres {#3-estimation}

### 3.1 D√©fis de l'Estimation en Finance

```python
"""
D√âFIS DE L'ESTIMATION EN FINANCE
================================

Deux probl√®mes majeurs rendent l'estimation difficile :

1. R√âGIME DE PETITS √âCHANTILLONS (Small Sample Regime)
   - On a N actifs mais seulement T observations
   - Si T < N, la covariance empirique n'est pas inversible !
   - M√™me si T > N, l'estimation peut √™tre tr√®s bruit√©e
   
   Exemple : 500 actifs du S&P 500, seulement 252 jours de trading/an
   ‚Üí Pour 2 ans de donn√©es : T=504, N=500 ‚Üí T ‚âà N !

2. QUEUES √âPAISSES (Heavy Tails)
   - Les rendements ne sont PAS gaussiens
   - Les √©v√©nements extr√™mes arrivent plus souvent que pr√©vu par la loi normale
   - L'estimateur classique est tr√®s sensible aux outliers

SOLUTIONS :
- Estimateurs de shrinkage (r√©gularisation)
- Estimateurs robustes (Huber, Tyler, etc.)
"""
```

### 3.2 Estimateurs de Shrinkage

```python
"""
ESTIMATEURS DE SHRINKAGE
========================

Id√©e : "R√©tr√©cir" (shrink) l'estimateur vers une cible structur√©e.

Forme g√©n√©rale :
    Œ∏ÃÉ = œÅ * T + (1 - œÅ) * Œ∏ÃÇ

O√π :
- Œ∏ÃÇ : estimateur empirique (bruyant mais non biais√©)
- T : cible (biais√©e mais stable)
- œÅ : param√®tre de shrinkage (compromis biais-variance)

En augmentant œÅ :
- On r√©duit la variance
- On augmente le biais
- On am√©liore souvent le MSE (Mean Squared Error) total !

C'est l'√©quivalent du "diagonal loading" en beamforming.
"""

def shrinkage_mean(
    returns: np.ndarray,
    target: Optional[np.ndarray] = None,
    Sigma: Optional[np.ndarray] = None
) -> np.ndarray:
    """
    Estimateur de shrinkage de James-Stein pour la moyenne.
    
    L'estimateur de James-Stein (1961) montre que la moyenne empirique
    est DOMIN√âE (au sens du MSE) par un estimateur de shrinkage
    quand la dimension N ‚â• 3 !
    
    ŒºÃÉ = œÅ * b + (1 - œÅ) * ŒºÃÇ
    
    Args:
        returns: Matrice T√óN des rendements
        target: Cible b (par d√©faut : grand mean)
        Sigma: Covariance vraie (si connue)
        
    Returns:
        Moyenne shrink√©e
    """
    T, N = returns.shape
    
    # Moyenne empirique
    mu_hat = np.mean(returns, axis=0)
    
    # Cible par d√©faut : grand mean (moyenne des moyennes)
    if target is None:
        target = np.ones(N) * np.mean(mu_hat)
    
    # Covariance (estim√©e si pas fournie)
    if Sigma is None:
        Sigma = np.cov(returns, rowvar=False, ddof=1)
    
    # Param√®tre de shrinkage optimal (formule de James-Stein)
    eigenvalues = np.linalg.eigvalsh(Sigma)
    lambda_avg = np.mean(eigenvalues)
    lambda_max = np.max(eigenvalues)
    
    diff = mu_hat - target
    diff_norm_sq = np.dot(diff, diff)
    
    if diff_norm_sq > 1e-10:
        rho = (1 / T) * (N * lambda_avg - 2 * lambda_max) / diff_norm_sq
        rho = max(0, min(1, rho))  # Borner entre 0 et 1
    else:
        rho = 0
    
    return rho * target + (1 - rho) * mu_hat


class LedoitWolfShrinkage:
    """
    Estimateur de Ledoit-Wolf pour la matrice de covariance.
    
    Shrink la covariance empirique vers une cible simple (souvent Œª*I).
    
    Œ£ÃÉ = œÅ * ŒªÃÉ * I + (1 - œÅ) * Œ£ÃÇ
    
    O√π :
    - Œ£ÃÇ : covariance empirique
    - ŒªÃÉ = Tr(Œ£)/N : moyenne des variances
    - œÅ : param√®tre optimal minimisant E[||Œ£ÃÉ - Œ£||¬≤_F]
    
    Formule de Ledoit-Wolf (2004) pour œÅ optimal :
    
    œÅ = min(1, (1/T) * Œ£ ||r_t r_t^T - Œ£ÃÇ||¬≤_F / ||Œ£ÃÇ - ŒªÃÉI||¬≤_F)
    """
    
    def __init__(self):
        self.rho: float = 0.0
        self.lambda_: float = 0.0
        self.Sigma_shrunk: Optional[np.ndarray] = None
    
    def fit(self, returns: np.ndarray) -> 'LedoitWolfShrinkage':
        """
        Calcule l'estimateur de Ledoit-Wolf.
        
        Args:
            returns: Matrice T√óN des rendements (d√©j√† centr√©s ou non)
            
        Returns:
            self
        """
        T, N = returns.shape
        
        # Centrer les donn√©es
        mean = np.mean(returns, axis=0)
        X = returns - mean  # T √ó N
        
        # Covariance empirique
        Sigma_hat = (X.T @ X) / T  # N √ó N
        
        # Cible : Œª * I
        self.lambda_ = np.trace(Sigma_hat) / N
        
        # Calcul de Œ¥¬≤ = ||Œ£ÃÇ - ŒªI||¬≤_F / N¬≤
        delta_sq = np.sum((Sigma_hat - self.lambda_ * np.eye(N))**2) / N**2
        
        # Calcul de Œ≤¬≤ (terme de correction)
        # Œ≤¬≤ = (1/T¬≤) * Œ£_t ||x_t x_t^T - Œ£ÃÇ||¬≤_F
        beta_sq = 0.0
        for t in range(T):
            x_t = X[t:t+1].T  # N √ó 1
            sample_cov = x_t @ x_t.T  # N √ó N
            beta_sq += np.sum((sample_cov - Sigma_hat)**2)
        beta_sq = beta_sq / (T**2 * N**2)
        
        # Param√®tre de shrinkage
        self.rho = min(1.0, beta_sq / delta_sq) if delta_sq > 0 else 1.0
        
        # Covariance shrink√©e
        self.Sigma_shrunk = (
            self.rho * self.lambda_ * np.eye(N) + 
            (1 - self.rho) * Sigma_hat
        )
        
        return self
    
    def get_covariance(self) -> np.ndarray:
        """Retourne la covariance shrink√©e."""
        return self.Sigma_shrunk


def demo_shrinkage():
    """Compare la covariance empirique et Ledoit-Wolf."""
    np.random.seed(42)
    
    # Vraie covariance
    N = 50
    true_Sigma = np.eye(N)
    for i in range(N):
        for j in range(N):
            true_Sigma[i, j] = 0.5 ** abs(i - j)  # Structure AR(1)
    
    # G√©n√©rer des donn√©es (peu d'√©chantillons)
    T = 60  # T proche de N !
    returns = np.random.multivariate_normal(np.zeros(N), true_Sigma, size=T)
    
    # Covariance empirique
    Sigma_hat = np.cov(returns, rowvar=False, ddof=0)
    
    # Ledoit-Wolf
    lw = LedoitWolfShrinkage().fit(returns)
    
    # Erreurs
    error_scm = np.linalg.norm(Sigma_hat - true_Sigma, 'fro')
    error_lw = np.linalg.norm(lw.Sigma_shrunk - true_Sigma, 'fro')
    
    print("=== Comparaison Shrinkage ===")
    print(f"N = {N}, T = {T}")
    print(f"Erreur covariance empirique : {error_scm:.4f}")
    print(f"Erreur Ledoit-Wolf :          {error_lw:.4f}")
    print(f"Am√©lioration : {(error_scm - error_lw) / error_scm * 100:.1f}%")
    print(f"Param√®tre de shrinkage œÅ : {lw.rho:.4f}")
    
    return lw
```

### 3.3 Estimateurs Robustes

```python
"""
ESTIMATEURS ROBUSTES
====================

Les estimateurs classiques (moyenne, covariance) sont sensibles aux outliers.
Les estimateurs robustes downweight les observations extr√™mes.

M-ESTIMATEURS :
G√©n√©ralisent le MLE (Maximum Likelihood Estimator) avec des poids adaptatifs.

    Œº = Œ£ w_1(d_t) * r_t / Œ£ w_1(d_t)
    Œ£ = (1/T) Œ£ w_2(d_t) * (r_t - Œº)(r_t - Œº)^T

O√π d_t = (r_t - Œº)^T Œ£^{-1} (r_t - Œº) est la distance de Mahalanobis.

Diff√©rents choix de w(d) donnent diff√©rents estimateurs :
- w(d) = 1 : estimateurs classiques (pas robustes)
- w(d) = (N+1)/(1+d) : MLE de Cauchy (robuste, queues tr√®s √©paisses)
- w(d) = N/d : estimateur de Tyler (tr√®s robuste)
"""

class TylerEstimator:
    """
    Estimateur de Tyler pour la matrice de scatter (forme).
    
    C'est l'estimateur MLE de la distribution "Angular Gaussian",
    qui est invariant √† l'√©chelle des observations.
    
    L'estimateur de Tyler est tr√®s robuste car il ne d√©pend que
    des DIRECTIONS des observations, pas de leurs normes.
    
    √âquation du point fixe :
        Œ£ = (N/T) Œ£_t [r_t r_t^T / (r_t^T Œ£^{-1} r_t)]
    
    Conditions d'existence : T ‚â• N + 1
    """
    
    def __init__(self, max_iter: int = 100, tol: float = 1e-6):
        """
        Args:
            max_iter: Nombre maximum d'it√©rations
            tol: Tol√©rance pour la convergence
        """
        self.max_iter = max_iter
        self.tol = tol
        self.Sigma: Optional[np.ndarray] = None
    
    def fit(self, data: np.ndarray, mu: Optional[np.ndarray] = None) -> 'TylerEstimator':
        """
        Estime la matrice de scatter par l'algorithme it√©ratif de Tyler.
        
        Args:
            data: Matrice T√óN des observations
            mu: Moyenne (si connue, sinon suppos√©e 0)
            
        Returns:
            self
        """
        T, N = data.shape
        
        if T < N + 1:
            raise ValueError(f"Pas assez d'√©chantillons. T={T} < N+1={N+1}")
        
        # Centrer si moyenne fournie
        if mu is not None:
            data = data - mu
        
        # Initialisation : identit√©
        Sigma = np.eye(N)
        
        for iteration in range(self.max_iter):
            Sigma_old = Sigma.copy()
            
            # Calcul de la nouvelle estimation
            Sigma_inv = np.linalg.inv(Sigma)
            Sigma_new = np.zeros((N, N))
            
            for t in range(T):
                r_t = data[t]
                # Distance de Mahalanobis au carr√©
                d_t = r_t @ Sigma_inv @ r_t
                # Poids de Tyler : w(d) = N/d
                weight = N / d_t if d_t > 1e-10 else N / 1e-10
                Sigma_new += weight * np.outer(r_t, r_t)
            
            Sigma_new /= T
            
            # Normaliser par la trace (Tyler est d√©fini √† un scalaire pr√®s)
            Sigma = Sigma_new / np.trace(Sigma_new) * N
            
            # V√©rifier la convergence
            diff = np.linalg.norm(Sigma - Sigma_old, 'fro')
            if diff < self.tol:
                break
        
        self.Sigma = Sigma
        return self
    
    def get_scatter(self) -> np.ndarray:
        """Retourne la matrice de scatter estim√©e."""
        return self.Sigma


class RegularizedTyler:
    """
    Estimateur de Tyler r√©gularis√©.
    
    Quand T < N, Tyler classique n'existe pas.
    On ajoute un terme de r√©gularisation :
    
    Œ£ = (1/(1+Œ±)) * Tyler_update + (Œ±/(1+Œ±)) * T
    
    O√π T est une cible (souvent l'identit√©).
    
    C'est l'√©quivalent du shrinkage pour les estimateurs robustes.
    """
    
    def __init__(
        self, 
        alpha: float = 0.1,
        target: Optional[np.ndarray] = None,
        max_iter: int = 100,
        tol: float = 1e-6
    ):
        """
        Args:
            alpha: Param√®tre de r√©gularisation (Œ± ‚â• 0)
            target: Matrice cible (par d√©faut : identit√©)
            max_iter: It√©rations max
            tol: Tol√©rance
        """
        self.alpha = alpha
        self.target = target
        self.max_iter = max_iter
        self.tol = tol
        self.Sigma: Optional[np.ndarray] = None
    
    def fit(self, data: np.ndarray) -> 'RegularizedTyler':
        """
        Estime la matrice de scatter r√©gularis√©e.
        
        Args:
            data: Matrice T√óN des observations (centr√©es)
        """
        T, N = data.shape
        
        # Cible par d√©faut : identit√©
        if self.target is None:
            target = np.eye(N)
        else:
            target = self.target
        
        # Initialisation
        Sigma = np.eye(N)
        alpha = self.alpha
        
        for iteration in range(self.max_iter):
            Sigma_old = Sigma.copy()
            
            # Tyler update
            Sigma_inv = np.linalg.inv(Sigma)
            tyler_part = np.zeros((N, N))
            
            for t in range(T):
                r_t = data[t]
                d_t = r_t @ Sigma_inv @ r_t
                weight = N / d_t if d_t > 1e-10 else N / 1e-10
                tyler_part += weight * np.outer(r_t, r_t)
            
            tyler_part /= T
            
            # Combinaison avec la cible
            Sigma = (1 / (1 + alpha)) * tyler_part + (alpha / (1 + alpha)) * target
            
            # Convergence
            diff = np.linalg.norm(Sigma - Sigma_old, 'fro')
            if diff < self.tol:
                break
        
        self.Sigma = Sigma
        return self


def demo_robust_estimation():
    """Compare les estimateurs classiques et robustes avec outliers."""
    np.random.seed(42)
    
    N = 10
    T = 100
    
    # Vraie covariance
    true_Sigma = np.eye(N)
    
    # G√©n√©rer des donn√©es normales
    data_clean = np.random.multivariate_normal(np.zeros(N), true_Sigma, size=T)
    
    # Ajouter des outliers (5%)
    n_outliers = int(0.05 * T)
    outlier_indices = np.random.choice(T, n_outliers, replace=False)
    data_with_outliers = data_clean.copy()
    data_with_outliers[outlier_indices] = np.random.multivariate_normal(
        np.ones(N) * 5, true_Sigma * 0.1, size=n_outliers
    )
    
    # Estimations
    scm_clean = np.cov(data_clean, rowvar=False, ddof=0)
    scm_outliers = np.cov(data_with_outliers, rowvar=False, ddof=0)
    
    tyler = TylerEstimator().fit(data_with_outliers)
    
    # Erreurs
    error_scm_clean = np.linalg.norm(scm_clean - true_Sigma, 'fro')
    error_scm_outliers = np.linalg.norm(scm_outliers - true_Sigma, 'fro')
    error_tyler = np.linalg.norm(tyler.Sigma - true_Sigma, 'fro')
    
    print("=== Estimation Robuste ===")
    print(f"Erreur SCM (donn√©es propres) :  {error_scm_clean:.4f}")
    print(f"Erreur SCM (avec outliers) :    {error_scm_outliers:.4f}")
    print(f"Erreur Tyler (avec outliers) :  {error_tyler:.4f}")
    
    return tyler
```

---

## 4. Optimisation de Portefeuille {#4-portefeuille}

### 4.1 Framework de Markowitz

```python
"""
OPTIMISATION DE PORTEFEUILLE DE MARKOWITZ
=========================================

Harry Markowitz (1952, Nobel 1990) a formalis√© le compromis rendement-risque.

Probl√®me de base :
    max  w^T Œº - (Œª/2) w^T Œ£ w
    s.t. w^T 1 = 1

O√π :
- w : vecteur des poids du portefeuille
- Œº : vecteur des rendements esp√©r√©s
- Œ£ : matrice de covariance des rendements
- Œª : param√®tre d'aversion au risque

Interpr√©tation :
- w^T Œº : rendement esp√©r√© du portefeuille
- w^T Œ£ w : variance (risque) du portefeuille
- Œª : combien on "sacrifie" de rendement pour r√©duire le risque

FRONTI√àRE EFFICIENTE :
L'ensemble des portefeuilles optimaux forme une hyperbole.
Aucun portefeuille ne peut avoir plus de rendement pour le m√™me risque,
ou moins de risque pour le m√™me rendement.
"""

class MarkowitzOptimizer:
    """
    Optimisation de portefeuille Mean-Variance de Markowitz.
    
    R√©sout plusieurs variantes du probl√®me :
    1. Minimum Variance Portfolio (MVP)
    2. Maximum Sharpe Ratio (tangency portfolio)
    3. Mean-Variance optimal avec Œª fix√©
    4. Target return portfolio
    """
    
    def __init__(
        self, 
        mu: np.ndarray, 
        Sigma: np.ndarray,
        risk_free_rate: float = 0.0
    ):
        """
        Args:
            mu: Vecteur des rendements esp√©r√©s (N,)
            Sigma: Matrice de covariance (N√óN)
            risk_free_rate: Taux sans risque
        """
        self.mu = np.asarray(mu)
        self.Sigma = np.asarray(Sigma)
        self.rf = risk_free_rate
        self.N = len(mu)
    
    def minimum_variance_portfolio(self) -> np.ndarray:
        """
        Portefeuille de variance minimale (MVP).
        
        min  w^T Œ£ w
        s.t. w^T 1 = 1
        
        Solution analytique :
            w_MVP = Œ£^{-1} 1 / (1^T Œ£^{-1} 1)
        
        Ce portefeuille ignore compl√®tement les rendements esp√©r√©s !
        Utile quand on ne fait pas confiance aux estimations de Œº.
        
        Returns:
            Poids du MVP
        """
        ones = np.ones(self.N)
        Sigma_inv = np.linalg.inv(self.Sigma)
        
        w = Sigma_inv @ ones
        w = w / np.sum(w)  # Normaliser
        
        return w
    
    def maximum_sharpe_ratio(self) -> np.ndarray:
        """
        Portefeuille de Sharpe ratio maximal (tangency portfolio).
        
        max  (w^T Œº - r_f) / sqrt(w^T Œ£ w)
        s.t. w^T 1 = 1
        
        Solution analytique :
            w_SR = Œ£^{-1} (Œº - r_f * 1) / [1^T Œ£^{-1} (Œº - r_f * 1)]
        
        C'est le portefeuille tangent √† la fronti√®re efficiente
        depuis le point (0, r_f).
        
        Returns:
            Poids du portefeuille de Sharpe max
        """
        Sigma_inv = np.linalg.inv(self.Sigma)
        excess_return = self.mu - self.rf
        
        w = Sigma_inv @ excess_return
        w = w / np.sum(w)  # Normaliser
        
        return w
    
    def mean_variance_optimal(self, risk_aversion: float) -> np.ndarray:
        """
        Portefeuille MV-optimal pour un niveau d'aversion au risque.
        
        max  w^T Œº - (Œª/2) w^T Œ£ w
        s.t. w^T 1 = 1
        
        Solution via multiplicateurs de Lagrange.
        
        Args:
            risk_aversion: Param√®tre Œª > 0
            
        Returns:
            Poids optimaux
        """
        lam = risk_aversion
        Sigma_inv = np.linalg.inv(self.Sigma)
        ones = np.ones(self.N)
        
        # Termes interm√©diaires
        A = ones @ Sigma_inv @ self.mu
        B = self.mu @ Sigma_inv @ self.mu
        C = ones @ Sigma_inv @ ones
        
        # Multiplicateur de Lagrange
        gamma = (A - lam) / C
        
        # Solution
        w = (1/lam) * Sigma_inv @ (self.mu - gamma * ones)
        
        return w
    
    def target_return_portfolio(self, target_return: float) -> np.ndarray:
        """
        Portefeuille de variance minimale pour un rendement cible.
        
        min  w^T Œ£ w
        s.t. w^T Œº = r_target
             w^T 1 = 1
        
        Args:
            target_return: Rendement cible
            
        Returns:
            Poids optimaux
        """
        Sigma_inv = np.linalg.inv(self.Sigma)
        ones = np.ones(self.N)
        
        # Termes pour la solution analytique
        A = ones @ Sigma_inv @ self.mu
        B = self.mu @ Sigma_inv @ self.mu
        C = ones @ Sigma_inv @ ones
        D = B * C - A**2
        
        # Multiplicateurs de Lagrange
        lambda1 = (C * target_return - A) / D
        lambda2 = (B - A * target_return) / D
        
        # Solution
        w = lambda1 * (Sigma_inv @ self.mu) + lambda2 * (Sigma_inv @ ones)
        
        return w
    
    def efficient_frontier(self, n_points: int = 50) -> Tuple[np.ndarray, np.ndarray]:
        """
        Calcule la fronti√®re efficiente.
        
        Args:
            n_points: Nombre de points sur la fronti√®re
            
        Returns:
            (risks, returns): Arrays des risques et rendements
        """
        # Bornes des rendements
        w_mvp = self.minimum_variance_portfolio()
        min_return = w_mvp @ self.mu
        max_return = np.max(self.mu)  # Approximation
        
        target_returns = np.linspace(min_return, max_return, n_points)
        risks = []
        returns = []
        
        for r in target_returns:
            try:
                w = self.target_return_portfolio(r)
                portfolio_risk = np.sqrt(w @ self.Sigma @ w)
                risks.append(portfolio_risk)
                returns.append(r)
            except:
                pass
        
        return np.array(risks), np.array(returns)
    
    def portfolio_stats(self, weights: np.ndarray) -> dict:
        """
        Calcule les statistiques d'un portefeuille.
        
        Args:
            weights: Poids du portefeuille
            
        Returns:
            Dict avec return, risk, sharpe_ratio
        """
        port_return = weights @ self.mu
        port_risk = np.sqrt(weights @ self.Sigma @ weights)
        sharpe = (port_return - self.rf) / port_risk if port_risk > 0 else 0
        
        return {
            'return': port_return,
            'risk': port_risk,
            'sharpe_ratio': sharpe
        }


def demo_markowitz():
    """D√©montre l'optimisation de Markowitz."""
    np.random.seed(42)
    
    # 5 actifs avec caract√©ristiques diff√©rentes
    mu = np.array([0.10, 0.12, 0.08, 0.15, 0.09])  # Rendements annuels
    
    # Matrice de covariance (corr√©lations r√©alistes)
    volatilities = np.array([0.15, 0.20, 0.10, 0.25, 0.12])
    correlations = np.array([
        [1.0, 0.5, 0.3, 0.4, 0.2],
        [0.5, 1.0, 0.4, 0.6, 0.3],
        [0.3, 0.4, 1.0, 0.3, 0.5],
        [0.4, 0.6, 0.3, 1.0, 0.4],
        [0.2, 0.3, 0.5, 0.4, 1.0]
    ])
    Sigma = np.outer(volatilities, volatilities) * correlations
    
    # Optimiser
    optimizer = MarkowitzOptimizer(mu, Sigma, risk_free_rate=0.02)
    
    w_mvp = optimizer.minimum_variance_portfolio()
    w_sharpe = optimizer.maximum_sharpe_ratio()
    w_mv = optimizer.mean_variance_optimal(risk_aversion=2.0)
    
    print("=== Optimisation de Portefeuille Markowitz ===")
    print("\nPoids du Minimum Variance Portfolio:")
    print(f"  {w_mvp}")
    print(f"  Stats: {optimizer.portfolio_stats(w_mvp)}")
    
    print("\nPoids du Maximum Sharpe Ratio Portfolio:")
    print(f"  {w_sharpe}")
    print(f"  Stats: {optimizer.portfolio_stats(w_sharpe)}")
    
    print("\nPoids du MV-Optimal (Œª=2):")
    print(f"  {w_mv}")
    print(f"  Stats: {optimizer.portfolio_stats(w_mv)}")
    
    return optimizer
```

### 4.2 Optimisation Robuste

```python
"""
OPTIMISATION DE PORTEFEUILLE ROBUSTE
====================================

Le probl√®me de Markowitz est TR√àS sensible aux erreurs d'estimation !

Probl√®me : On ne conna√Æt pas Œº et Œ£ exacts, on les ESTIME.
Les erreurs d'estimation se propagent et amplifient les erreurs de d√©cision.

SOLUTION : Optimisation robuste (worst-case optimization)

Id√©e : Au lieu d'optimiser pour ŒºÃÇ et Œ£ÃÇ estim√©s,
optimiser pour le PIRE CAS dans un ensemble d'incertitude.

    max  min        w^T Œº - (Œª/2) w^T Œ£ w
     w   (Œº,Œ£)‚ààU

O√π U est l'ensemble d'incertitude autour des estimations.
"""

class RobustMarkowitz:
    """
    Optimisation de portefeuille robuste.
    
    Mod√©lise l'incertitude sur Œº et Œ£ et optimise pour le pire cas.
    
    Ensemble d'incertitude sur Œº (ellipso√Ødal) :
        U_Œº = {Œº : (Œº - ŒºÃÇ)^T Œ£_Œº^{-1} (Œº - ŒºÃÇ) ‚â§ Œ∫_Œº¬≤}
    
    Cela donne une formulation robuste :
        max  w^T ŒºÃÇ - Œ∫_Œº ||Œ£_Œº^{1/2} w|| - (Œª/2) w^T Œ£ÃÇ w
    """
    
    def __init__(
        self,
        mu_hat: np.ndarray,
        Sigma_hat: np.ndarray,
        kappa_mu: float = 1.0,
        Sigma_mu: Optional[np.ndarray] = None
    ):
        """
        Args:
            mu_hat: Estimation de Œº
            Sigma_hat: Estimation de Œ£
            kappa_mu: Rayon de l'ensemble d'incertitude sur Œº
            Sigma_mu: Covariance de l'erreur d'estimation de Œº
        """
        self.mu_hat = mu_hat
        self.Sigma_hat = Sigma_hat
        self.kappa_mu = kappa_mu
        self.N = len(mu_hat)
        
        # Par d√©faut, l'incertitude sur Œº est proportionnelle √† Œ£
        if Sigma_mu is None:
            self.Sigma_mu = Sigma_hat / 100  # Heuristique
        else:
            self.Sigma_mu = Sigma_mu
    
    def robust_optimal(self, risk_aversion: float) -> np.ndarray:
        """
        R√©sout le probl√®me robuste par optimisation num√©rique.
        
        Le worst-case (pire cas) sur Œº dans l'ellipso√Øde donne :
            Œº_worst = ŒºÃÇ - Œ∫_Œº * Œ£_Œº^{1/2} * w / ||Œ£_Œº^{1/2} w||
        
        Args:
            risk_aversion: Param√®tre Œª
            
        Returns:
            Poids robustes
        """
        lam = risk_aversion
        
        # Racine carr√©e de Sigma_mu
        eigvals, eigvecs = np.linalg.eigh(self.Sigma_mu)
        Sigma_mu_sqrt = eigvecs @ np.diag(np.sqrt(np.maximum(eigvals, 0))) @ eigvecs.T
        
        def objective(w):
            """Objectif robuste (√† maximiser, donc on retourne -objectif)."""
            # Terme de rendement esp√©r√©
            expected_return = w @ self.mu_hat
            
            # P√©nalit√© robuste (pire cas sur Œº)
            robust_penalty = self.kappa_mu * np.linalg.norm(Sigma_mu_sqrt @ w)
            
            # Terme de variance
            variance = w @ self.Sigma_hat @ w
            
            return -(expected_return - robust_penalty - 0.5 * lam * variance)
        
        # Contrainte : somme des poids = 1
        constraints = {'type': 'eq', 'fun': lambda w: np.sum(w) - 1}
        
        # Point de d√©part : √©quipond√©r√©
        w0 = np.ones(self.N) / self.N
        
        # Optimisation
        result = minimize(
            objective,
            w0,
            method='SLSQP',
            constraints=constraints
        )
        
        return result.x


def demo_robust_portfolio():
    """Compare portefeuille classique et robuste."""
    np.random.seed(42)
    
    N = 5
    
    # "Vrais" param√®tres
    true_mu = np.array([0.10, 0.12, 0.08, 0.15, 0.09])
    volatilities = np.array([0.15, 0.20, 0.10, 0.25, 0.12])
    corr = np.array([
        [1.0, 0.5, 0.3, 0.4, 0.2],
        [0.5, 1.0, 0.4, 0.6, 0.3],
        [0.3, 0.4, 1.0, 0.3, 0.5],
        [0.4, 0.6, 0.3, 1.0, 0.4],
        [0.2, 0.3, 0.5, 0.4, 1.0]
    ])
    true_Sigma = np.outer(volatilities, volatilities) * corr
    
    # Estimations bruit√©es (simulant l'erreur d'estimation)
    mu_hat = true_mu + np.random.randn(N) * 0.03
    Sigma_hat = true_Sigma * (1 + np.random.randn(N, N) * 0.1)
    Sigma_hat = (Sigma_hat + Sigma_hat.T) / 2  # Sym√©triser
    
    # Portefeuille classique
    classic = MarkowitzOptimizer(mu_hat, Sigma_hat)
    w_classic = classic.mean_variance_optimal(risk_aversion=2.0)
    
    # Portefeuille robuste
    robust = RobustMarkowitz(mu_hat, Sigma_hat, kappa_mu=1.5)
    w_robust = robust.robust_optimal(risk_aversion=2.0)
    
    # √âvaluation avec les VRAIS param√®tres
    true_optimizer = MarkowitzOptimizer(true_mu, true_Sigma)
    
    print("=== Comparaison Classique vs Robuste ===")
    print(f"\nPortefeuille classique:")
    print(f"  Poids: {w_classic}")
    print(f"  Performance vraie: {true_optimizer.portfolio_stats(w_classic)}")
    
    print(f"\nPortefeuille robuste:")
    print(f"  Poids: {w_robust}")
    print(f"  Performance vraie: {true_optimizer.portfolio_stats(w_robust)}")
    
    return w_classic, w_robust
```

### 4.3 Risk Parity

```python
"""
PORTEFEUILLE RISK PARITY
========================

Id√©e : Au lieu d'√©galiser les CAPITAUX (√©quipond√©r√©),
√©galiser les CONTRIBUTIONS AU RISQUE.

D√©finitions :
- Risque du portefeuille : œÉ_p = sqrt(w^T Œ£ w)
- Contribution marginale au risque de l'actif i : ‚àÇœÉ_p/‚àÇw_i = (Œ£w)_i / œÉ_p
- Contribution au risque de l'actif i : RC_i = w_i * (Œ£w)_i / œÉ_p

Risk Parity demande : RC_1 = RC_2 = ... = RC_N

C'est √©quivalent √† : w_i * (Œ£w)_i = w_j * (Œ£w)_j pour tous i, j

Avantages :
- Pas besoin d'estimer Œº (seulement Œ£)
- Diversification du risque
- Tr√®s utilis√© en pratique (Bridgewater "All Weather")
"""

class RiskParityPortfolio:
    """
    Calcule le portefeuille Risk Parity.
    
    Chaque actif contribue √©galement au risque total du portefeuille.
    """
    
    def __init__(self, Sigma: np.ndarray):
        """
        Args:
            Sigma: Matrice de covariance
        """
        self.Sigma = Sigma
        self.N = Sigma.shape[0]
    
    def risk_contributions(self, weights: np.ndarray) -> np.ndarray:
        """
        Calcule les contributions au risque de chaque actif.
        
        RC_i = w_i * (Œ£w)_i / œÉ_p
        
        Args:
            weights: Poids du portefeuille
            
        Returns:
            Vecteur des contributions au risque
        """
        sigma_p = np.sqrt(weights @ self.Sigma @ weights)
        marginal_contrib = self.Sigma @ weights / sigma_p
        risk_contrib = weights * marginal_contrib
        return risk_contrib
    
    def optimize(self, budget: Optional[np.ndarray] = None) -> np.ndarray:
        """
        Trouve le portefeuille Risk Parity.
        
        Minimise : Œ£_i Œ£_j [w_i(Œ£w)_i - w_j(Œ£w)_j]¬≤
        
        Args:
            budget: Contributions au risque cibles (par d√©faut : √©gales)
            
        Returns:
            Poids Risk Parity
        """
        if budget is None:
            budget = np.ones(self.N) / self.N  # √âgales
        
        def objective(w):
            """Mesure l'√©cart aux contributions cibles."""
            sigma_p_sq = w @ self.Sigma @ w
            if sigma_p_sq < 1e-10:
                return 1e10
            
            rc = w * (self.Sigma @ w) / np.sqrt(sigma_p_sq)
            
            # √âcart aux contributions cibles
            return np.sum((rc - budget * np.sum(rc))**2)
        
        # Contraintes
        constraints = [
            {'type': 'eq', 'fun': lambda w: np.sum(w) - 1},  # Budget
        ]
        bounds = [(0.001, None) for _ in range(self.N)]  # Long-only
        
        # Point de d√©part : √©quipond√©r√©
        w0 = np.ones(self.N) / self.N
        
        result = minimize(
            objective,
            w0,
            method='SLSQP',
            bounds=bounds,
            constraints=constraints,
            options={'maxiter': 1000}
        )
        
        return result.x / np.sum(result.x)  # Normaliser
    
    def analyze(self, weights: np.ndarray) -> dict:
        """
        Analyse un portefeuille en termes de contributions au risque.
        """
        rc = self.risk_contributions(weights)
        sigma = np.sqrt(weights @ self.Sigma @ weights)
        
        return {
            'weights': weights,
            'risk_contributions': rc,
            'risk_contribution_pct': rc / sigma * 100,
            'portfolio_volatility': sigma,
            'rc_herfindahl': np.sum((rc / sigma)**2)  # Concentration
        }


def demo_risk_parity():
    """D√©montre le portefeuille Risk Parity."""
    np.random.seed(42)
    
    # Covariance avec volatilit√©s diff√©rentes
    volatilities = np.array([0.10, 0.15, 0.20, 0.25, 0.30])
    N = len(volatilities)
    corr = 0.3 * np.ones((N, N)) + 0.7 * np.eye(N)
    Sigma = np.outer(volatilities, volatilities) * corr
    
    rp = RiskParityPortfolio(Sigma)
    
    # Comparer √©quipond√©r√© et risk parity
    w_equal = np.ones(N) / N
    w_rp = rp.optimize()
    
    print("=== Comparaison √âquipond√©r√© vs Risk Parity ===")
    print("\nPortefeuille √©quipond√©r√©:")
    analysis_eq = rp.analyze(w_equal)
    print(f"  Poids: {analysis_eq['weights']}")
    print(f"  Contributions au risque (%): {analysis_eq['risk_contribution_pct']}")
    
    print("\nPortefeuille Risk Parity:")
    analysis_rp = rp.analyze(w_rp)
    print(f"  Poids: {analysis_rp['weights']}")
    print(f"  Contributions au risque (%): {analysis_rp['risk_contribution_pct']}")
    
    print("\nObservation : Les actifs plus volatils ont des poids plus FAIBLES")
    print("dans le portefeuille Risk Parity pour √©galiser les contributions.")
    
    return w_rp
```

---

## 5. Arbitrage Statistique {#5-arbitrage}

### 5.1 Co√Ønt√©gration

```python
"""
CO√èNT√âGRATION ET ARBITRAGE STATISTIQUE
======================================

CO√èNT√âGRATION vs CORR√âLATION :

Corr√©lation : mesure si deux s√©ries BOUGENT ENSEMBLE √† court terme.
Co√Ønt√©gration : mesure si deux s√©ries RESTENT PROCHES √† long terme.

Deux s√©ries I(1) (integrated of order 1) sont co√Ønt√©gr√©es si une
combinaison lin√©aire est I(0) (stationnaire).

Exemple intuitif : Un ivrogne et son chien.
- Le chien court partout (non stationnaire)
- L'ivrogne marche au hasard (non stationnaire)
- Mais la DISTANCE entre eux reste born√©e (stationnaire) !

PAIRS TRADING :
1. Trouver deux actifs co√Ønt√©gr√©s
2. Quand le spread diverge ‚Üí parier sur la convergence
   - Long l'actif sous-√©valu√©
   - Short l'actif sur√©valu√©
3. Fermer quand le spread revient √† la moyenne
"""

class CointegrationTest:
    """
    Test de co√Ønt√©gration d'Engle-Granger.
    
    Proc√©dure :
    1. R√©gresser y sur x : y_t = Œ± + Œ≤*x_t + Œµ_t
    2. Tester si les r√©sidus Œµ_t sont stationnaires (test ADF)
    
    Si les r√©sidus sont stationnaires, y et x sont co√Ønt√©gr√©s.
    """
    
    def __init__(self):
        self.beta: Optional[float] = None
        self.alpha: Optional[float] = None
        self.residuals: Optional[np.ndarray] = None
        self.adf_stat: Optional[float] = None
        self.is_cointegrated: Optional[bool] = None
    
    def fit(
        self, 
        y: np.ndarray, 
        x: np.ndarray, 
        significance: float = 0.05
    ) -> 'CointegrationTest':
        """
        Effectue le test de co√Ønt√©gration.
        
        Args:
            y: Premi√®re s√©rie (d√©pendante)
            x: Deuxi√®me s√©rie (ind√©pendante)
            significance: Niveau de significativit√©
            
        Returns:
            self
        """
        T = len(y)
        
        # R√©gression : y = Œ± + Œ≤*x + Œµ
        X = np.column_stack([np.ones(T), x])
        coeffs = np.linalg.lstsq(X, y, rcond=None)[0]
        self.alpha = coeffs[0]
        self.beta = coeffs[1]
        
        # R√©sidus (spread)
        self.residuals = y - self.alpha - self.beta * x
        
        # Test ADF (Augmented Dickey-Fuller) sur les r√©sidus
        self.adf_stat = self._adf_test(self.residuals)
        
        # Valeurs critiques approximatives (Engle-Granger)
        # Ces valeurs sont diff√©rentes du test ADF standard !
        critical_values = {0.01: -3.96, 0.05: -3.37, 0.10: -3.07}
        
        self.is_cointegrated = self.adf_stat < critical_values.get(significance, -3.37)
        
        return self
    
    def _adf_test(self, series: np.ndarray, max_lags: int = None) -> float:
        """
        Test ADF (Augmented Dickey-Fuller).
        
        Teste H0: la s√©rie a une racine unitaire (non stationnaire)
        contre H1: la s√©rie est stationnaire.
        
        Returns:
            Statistique ADF (plus n√©gative = plus de preuves de stationnarit√©)
        """
        T = len(series)
        if max_lags is None:
            max_lags = int((T - 1)**(1/3))
        
        # Diff√©rence de la s√©rie
        diff = np.diff(series)
        lagged = series[:-1]
        
        # Construire les lags pour augmentation
        X = np.column_stack([np.ones(T-1), lagged])
        
        # Ajouter les lags des diff√©rences
        for lag in range(1, max_lags + 1):
            if lag < len(diff):
                lagged_diff = np.zeros(T - 1)
                lagged_diff[lag:] = diff[:-lag]
                X = np.column_stack([X, lagged_diff])
        
        # R√©gression
        y = diff
        beta = np.linalg.lstsq(X, y, rcond=None)[0]
        
        # R√©sidus
        residuals = y - X @ beta
        
        # Statistique t pour le coefficient de series[t-1]
        sigma = np.std(residuals)
        se_beta = sigma / np.std(lagged)
        t_stat = beta[1] / se_beta
        
        return t_stat
    
    def get_spread(self, y: np.ndarray, x: np.ndarray) -> np.ndarray:
        """
        Calcule le spread (z-score) pour de nouvelles donn√©es.
        """
        spread = y - self.alpha - self.beta * x
        return (spread - np.mean(self.residuals)) / np.std(self.residuals)


class PairsTrading:
    """
    Strat√©gie de Pairs Trading bas√©e sur la co√Ønt√©gration.
    
    Trading rules :
    - Open long spread (long y, short Œ≤*x) quand z-score < -entry_threshold
    - Open short spread (short y, long Œ≤*x) quand z-score > +entry_threshold
    - Close position quand z-score revient √† ¬±exit_threshold
    
    Le profit vient de la MEAN-REVERSION du spread.
    """
    
    def __init__(
        self,
        entry_threshold: float = 2.0,  # Nombre de œÉ pour entrer
        exit_threshold: float = 0.5,   # Nombre de œÉ pour sortir
        stop_loss: float = 4.0         # Stop loss en œÉ
    ):
        """
        Args:
            entry_threshold: Seuil d'entr√©e (en z-scores)
            exit_threshold: Seuil de sortie
            stop_loss: Stop loss
        """
        self.entry = entry_threshold
        self.exit = exit_threshold
        self.stop = stop_loss
        
        self.coint_test: Optional[CointegrationTest] = None
    
    def fit(self, y: np.ndarray, x: np.ndarray) -> 'PairsTrading':
        """
        Calibre la strat√©gie sur des donn√©es historiques.
        
        Args:
            y: Prix de l'actif y (√† acheter quand spread bas)
            x: Prix de l'actif x (√† shorter quand spread bas)
        """
        self.coint_test = CointegrationTest().fit(y, x)
        
        if not self.coint_test.is_cointegrated:
            print("‚ö†Ô∏è Attention: les actifs ne semblent pas co√Ønt√©gr√©s!")
        
        return self
    
    def generate_signals(
        self, 
        y: np.ndarray, 
        x: np.ndarray
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        G√©n√®re les signaux de trading.
        
        Args:
            y, x: S√©ries de prix
            
        Returns:
            (positions, z_scores): Position (-1, 0, +1) et z-scores
        """
        T = len(y)
        z = self.coint_test.get_spread(y, x)
        
        positions = np.zeros(T)
        current_position = 0
        
        for t in range(1, T):
            if current_position == 0:
                # Pas de position ‚Üí chercher entr√©e
                if z[t] < -self.entry:
                    current_position = 1  # Long spread
                elif z[t] > self.entry:
                    current_position = -1  # Short spread
            
            elif current_position == 1:  # Long spread
                # Sortie si revient √† la moyenne ou stop loss
                if z[t] > -self.exit or z[t] < -self.stop:
                    current_position = 0
            
            elif current_position == -1:  # Short spread
                if z[t] < self.exit or z[t] > self.stop:
                    current_position = 0
            
            positions[t] = current_position
        
        return positions, z
    
    def backtest(
        self, 
        y: np.ndarray, 
        x: np.ndarray
    ) -> dict:
        """
        Backteste la strat√©gie.
        
        Args:
            y, x: S√©ries de prix
            
        Returns:
            Statistiques du backtest
        """
        positions, z = self.generate_signals(y, x)
        
        # Rendements du spread
        # Long spread = long y - Œ≤*short x
        beta = self.coint_test.beta
        
        returns_y = np.diff(y) / y[:-1]
        returns_x = np.diff(x) / x[:-1]
        
        spread_returns = returns_y - beta * returns_x
        
        # PnL de la strat√©gie
        strategy_returns = positions[:-1] * spread_returns
        
        # Statistiques
        total_return = np.prod(1 + strategy_returns) - 1
        sharpe = np.mean(strategy_returns) / np.std(strategy_returns) * np.sqrt(252)
        n_trades = np.sum(np.abs(np.diff(positions)) > 0) // 2
        
        return {
            'total_return': total_return,
            'sharpe_ratio': sharpe,
            'n_trades': n_trades,
            'positions': positions,
            'z_scores': z,
            'strategy_returns': strategy_returns
        }


def demo_pairs_trading():
    """D√©montre le pairs trading avec donn√©es simul√©es."""
    np.random.seed(42)
    T = 500
    
    # Simuler deux actifs co√Ønt√©gr√©s
    # x suit un random walk
    x = np.cumsum(np.random.randn(T) * 0.02) + 100
    
    # y = 0.5*x + spread stationnaire
    beta_true = 0.5
    spread = np.zeros(T)
    for t in range(1, T):
        spread[t] = 0.8 * spread[t-1] + np.random.randn() * 0.5  # AR(1)
    
    y = beta_true * x + 50 + spread
    
    # Strat√©gie
    strategy = PairsTrading(entry_threshold=1.5, exit_threshold=0.2)
    strategy.fit(y[:250], x[:250])  # Train sur premi√®re moiti√©
    
    print("=== Pairs Trading ===")
    print(f"Beta estim√©: {strategy.coint_test.beta:.4f} (vrai: {beta_true})")
    print(f"Co√Ønt√©gration: {'Oui' if strategy.coint_test.is_cointegrated else 'Non'}")
    
    # Backtest sur deuxi√®me moiti√©
    results = strategy.backtest(y[250:], x[250:])
    
    print(f"\nBacktest (250 jours):")
    print(f"  Rendement total: {results['total_return']:.2%}")
    print(f"  Sharpe ratio: {results['sharpe_ratio']:.2f}")
    print(f"  Nombre de trades: {results['n_trades']}")
    
    return strategy, results
```

---

## 6. Ex√©cution d'Ordres {#6-execution}

```python
"""
EX√âCUTION OPTIMALE D'ORDRES
===========================

Probl√®me : Ex√©cuter un gros ordre (ex: vendre 1M d'actions) impacte le march√© !

MARKET IMPACT :
- Impact temporaire : affecte le prix pendant l'ex√©cution
- Impact permanent : shift durable du prix

Mod√®le d'Almgren-Chriss (2001) :

Prix d'ex√©cution : S_k = S_0 - Œ≥*Œ£_{j<k} n_j - Œ∑*n_k/œÑ + œÉ*Œµ_k

O√π :
- S_0 : prix initial
- n_k : quantit√© trad√©e au pas k
- Œ≥ : impact permanent (par unit√© trad√©e)
- Œ∑ : impact temporaire (par unit√©/temps)
- œÑ : intervalle de temps
- œÉ : volatilit√©

OBJECTIF : Minimiser le co√ªt d'ex√©cution
- Trader vite ‚Üí gros impact temporaire
- Trader lentement ‚Üí exposition au risque de prix

C'est un compromis rendement-risque similaire au beamforming !
"""

class AlmgrenChrissModel:
    """
    Mod√®le d'Almgren-Chriss pour l'ex√©cution optimale.
    
    Minimise : E[Co√ªt] + Œª * Var[Co√ªt]
    
    O√π Œª est l'aversion au risque de l'ex√©cuteur.
    """
    
    def __init__(
        self,
        total_shares: float,
        T: int,
        sigma: float,
        gamma: float,
        eta: float,
        tau: float = 1.0
    ):
        """
        Args:
            total_shares: X, quantit√© totale √† ex√©cuter
            T: Nombre de p√©riodes d'ex√©cution
            sigma: Volatilit√© du prix
            gamma: Param√®tre d'impact permanent
            eta: Param√®tre d'impact temporaire
            tau: Dur√©e d'une p√©riode
        """
        self.X = total_shares
        self.T = T
        self.sigma = sigma
        self.gamma = gamma
        self.eta = eta
        self.tau = tau
    
    def optimal_trajectory(self, risk_aversion: float) -> np.ndarray:
        """
        Calcule la trajectoire d'ex√©cution optimale.
        
        La solution est exponentielle pour Œª > 0 :
        
        n_k = X * sinh(Œ∫*(T-k)) / sinh(Œ∫*T)
        
        o√π Œ∫ = ‚àö(ŒªœÉ¬≤/Œ∑)
        
        Args:
            risk_aversion: Param√®tre Œª
            
        Returns:
            Quantit√©s n_k √† trader √† chaque p√©riode
        """
        lam = risk_aversion
        
        if lam == 0:
            # Solution TWAP : trade uniforme
            return np.ones(self.T) * self.X / self.T
        
        # Param√®tre Œ∫
        kappa = np.sqrt(lam * self.sigma**2 * self.tau / self.eta)
        
        # Trajectoire optimale
        n = np.zeros(self.T)
        denom = np.sinh(kappa * self.T)
        
        for k in range(self.T):
            n[k] = self.X * np.sinh(kappa * (self.T - k)) / denom
            # Correction : la formule donne le remaining, on veut le trade
        
        # Convertir position restante en trades
        remaining = np.zeros(self.T + 1)
        remaining[0] = self.X
        for k in range(self.T):
            # remaining[k+1] = remaining[k] * sinh(Œ∫*(T-k-1)) / sinh(Œ∫*(T-k))
            if k < self.T - 1:
                remaining[k+1] = self.X * np.sinh(kappa * (self.T - k - 1)) / denom
            else:
                remaining[k+1] = 0
        
        trades = -np.diff(remaining)
        
        return trades
    
    def expected_cost(
        self, 
        trades: np.ndarray, 
        S0: float = 100.0
    ) -> float:
        """
        Calcule le co√ªt esp√©r√© d'ex√©cution.
        
        E[Co√ªt] = (1/2)*Œ≥*X¬≤ + Œ∑*Œ£ n_k¬≤/œÑ
        
        Args:
            trades: Quantit√©s √† chaque p√©riode
            S0: Prix initial
            
        Returns:
            Co√ªt esp√©r√©
        """
        permanent_cost = 0.5 * self.gamma * self.X**2
        temporary_cost = self.eta * np.sum(trades**2) / self.tau
        
        return permanent_cost + temporary_cost
    
    def variance_cost(self, trades: np.ndarray) -> float:
        """
        Calcule la variance du co√ªt d'ex√©cution.
        
        Var[Co√ªt] = œÉ¬≤ * œÑ * Œ£_k (Œ£_{j‚â•k} n_j)¬≤
        
        Args:
            trades: Quantit√©s √† chaque p√©riode
            
        Returns:
            Variance du co√ªt
        """
        remaining = np.cumsum(trades[::-1])[::-1]  # Position restante
        return self.sigma**2 * self.tau * np.sum(remaining**2)
    
    def simulate_execution(
        self, 
        trades: np.ndarray, 
        S0: float = 100.0,
        n_simulations: int = 1000
    ) -> dict:
        """
        Simule l'ex√©cution pour estimer le co√ªt r√©el.
        
        Args:
            trades: Strat√©gie d'ex√©cution
            S0: Prix initial
            n_simulations: Nombre de simulations
            
        Returns:
            Statistiques du co√ªt
        """
        costs = []
        
        for _ in range(n_simulations):
            S = S0
            total_cost = 0
            remaining = self.X
            
            for k, n_k in enumerate(trades):
                # Bruit de prix
                noise = self.sigma * np.sqrt(self.tau) * np.random.randn()
                
                # Prix d'ex√©cution avec impact
                S_exec = S - self.eta * n_k / self.tau + noise
                
                # Co√ªt
                total_cost += n_k * (S0 - S_exec)
                
                # Mettre √† jour le prix (impact permanent)
                S = S - self.gamma * n_k + noise
                remaining -= n_k
            
            costs.append(total_cost)
        
        costs = np.array(costs)
        
        return {
            'mean_cost': np.mean(costs),
            'std_cost': np.std(costs),
            'var_cost': np.var(costs),
            '5th_percentile': np.percentile(costs, 5),
            '95th_percentile': np.percentile(costs, 95)
        }


def demo_execution():
    """D√©montre l'ex√©cution optimale d'ordres."""
    
    # Param√®tres
    model = AlmgrenChrissModel(
        total_shares=10000,  # 10,000 actions √† vendre
        T=20,                # 20 p√©riodes
        sigma=0.02,          # 2% volatilit√© par p√©riode
        gamma=0.001,         # Impact permanent
        eta=0.01,            # Impact temporaire
        tau=1.0
    )
    
    print("=== Ex√©cution Optimale d'Ordres (Almgren-Chriss) ===")
    
    # Diff√©rentes strat√©gies
    strategies = {
        'TWAP (Œª=0)': model.optimal_trajectory(0),
        'Averse (Œª=0.1)': model.optimal_trajectory(0.1),
        'Tr√®s averse (Œª=1)': model.optimal_trajectory(1.0)
    }
    
    for name, trades in strategies.items():
        expected = model.expected_cost(trades)
        variance = model.variance_cost(trades)
        
        print(f"\n{name}:")
        print(f"  Trades: {trades[:5].round(0)}...")
        print(f"  Co√ªt esp√©r√©: {expected:.2f}")
        print(f"  Variance: {variance:.2f}")
        print(f"  Std: {np.sqrt(variance):.2f}")
    
    print("\nObservation:")
    print("- TWAP (Time-Weighted Average Price) trade uniform√©ment")
    print("- Strat√©gie averse trade plus au d√©but pour r√©duire le risque")
    
    return model


# =============================================================================
# EX√âCUTION DES D√âMOS
# =============================================================================

if __name__ == "__main__":
    print("="*70)
    print("ING√âNIERIE FINANCI√àRE : UNE PERSPECTIVE DU TRAITEMENT DU SIGNAL")
    print("="*70)
    
    print("\n" + "="*70)
    demo_iid_model()
    
    print("\n" + "="*70)
    demo_garch()
    
    print("\n" + "="*70)
    demo_shrinkage()
    
    print("\n" + "="*70)
    demo_robust_estimation()
    
    print("\n" + "="*70)
    demo_markowitz()
    
    print("\n" + "="*70)
    demo_robust_portfolio()
    
    print("\n" + "="*70)
    demo_risk_parity()
    
    print("\n" + "="*70)
    demo_pairs_trading()
    
    print("\n" + "="*70)
    demo_execution()
    
    print("\n" + "="*70)
    print("FIN DES D√âMONSTRATIONS")
    print("="*70)
```

---

## üìö Glossaire des Acronymes

| Acronyme | Signification | Traduction/Explication |
|----------|--------------|------------------------|
| **ARMA** | AutoRegressive Moving Average | Moyenne mobile autor√©gressive |
| **VAR** | Vector AutoRegressive | Vecteur autor√©gressif |
| **GARCH** | Generalized AutoRegressive Conditional Heteroskedasticity | Mod√®le de volatilit√© conditionnelle |
| **MLE** | Maximum Likelihood Estimation | Estimation par maximum de vraisemblance |
| **OLS** | Ordinary Least Squares | Moindres carr√©s ordinaires |
| **SCM** | Sample Covariance Matrix | Matrice de covariance empirique |
| **MVP** | Minimum Variance Portfolio | Portefeuille de variance minimale |
| **CAPM** | Capital Asset Pricing Model | Mod√®le d'√©valuation des actifs financiers |
| **PCA** | Principal Component Analysis | Analyse en composantes principales |
| **ADF** | Augmented Dickey-Fuller | Test de stationnarit√© |
| **I(d)** | Integrated of order d | Int√©gr√© d'ordre d (d diff√©renciations pour stationnarit√©) |
| **VaR** | Value at Risk | Valeur √† risque |
| **CVaR** | Conditional Value at Risk | Valeur √† risque conditionnelle |
| **TWAP** | Time-Weighted Average Price | Prix moyen pond√©r√© dans le temps |

---

## üîó Connexions avec le Traitement du Signal

| Finance | Traitement du Signal |
|---------|---------------------|
| Optimisation de portefeuille | Design de beamforming |
| Shrinkage covariance | Diagonal loading |
| Mod√®le ARMA | Mod√®le p√¥le-z√©ro |
| Index tracking sparse | Compressed sensing |
| Ex√©cution d'ordres | Scheduling r√©seau |

---

*Document g√©n√©r√© √† partir de "A Signal Processing Perspective on Financial Engineering" - Feng & Palomar (2016)*
