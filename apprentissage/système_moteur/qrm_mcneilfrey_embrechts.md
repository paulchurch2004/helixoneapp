# üìö QUANTITATIVE RISK MANAGEMENT - McNeil, Frey & Embrechts
## Guide Complet avec Code Python pour HelixOne

**Source**: Quantitative Risk Management: Concepts, Techniques and Tools (Princeton, 2015)  
**Auteurs**: Alexander J. McNeil, R√ºdiger Frey, Paul Embrechts  
**Conversion**: Python pour HelixOne  
**Date**: 2026-01-29

---

## üìã TABLE DES MATI√àRES

1. [Glossaire Complet](#glossaire-complet)
2. [Chapitre 5: Extreme Value Theory (EVT)](#chapitre-5-extreme-value-theory)
3. [Chapitre 7: Copulas et D√©pendance](#chapitre-7-copulas-et-d√©pendance)
4. [Chapitre 8: Mesures de Risque Coh√©rentes](#chapitre-8-mesures-de-risque-coh√©rentes)
5. [Chapitre 9: Market Risk - VaR et Backtesting](#chapitre-9-market-risk)
6. [Chapitre 10: Credit Risk](#chapitre-10-credit-risk)
7. [Code Python Int√©gr√©](#code-python-int√©gr√©)

---

## üìñ GLOSSAIRE COMPLET

### Acronymes Principaux

| Acronyme | Signification | Explication |
|----------|---------------|-------------|
| **EVT** | Extreme Value Theory | Th√©orie des valeurs extr√™mes pour mod√©liser les queues de distribution |
| **GEV** | Generalized Extreme Value | Distribution des maxima : Gumbel, Fr√©chet, Weibull |
| **GPD** | Generalized Pareto Distribution | Distribution des exc√®s au-dessus d'un seuil |
| **POT** | Peaks Over Threshold | M√©thode des pics au-dessus d'un seuil |
| **VaR** | Value-at-Risk | Quantile Œ± de la distribution des pertes |
| **ES** | Expected Shortfall | Esp√©rance conditionnelle au-del√† du VaR |
| **CVaR** | Conditional VaR | Synonyme d'Expected Shortfall |
| **CDS** | Credit Default Swap | D√©riv√© de cr√©dit, assurance contre d√©faut |
| **PD** | Probability of Default | Probabilit√© qu'un emprunteur fasse d√©faut |
| **LGD** | Loss Given Default | Perte en cas de d√©faut (1 - taux de recouvrement) |
| **EAD** | Exposure at Default | Exposition au moment du d√©faut |
| **IRB** | Internal Ratings-Based | Approche B√¢le II pour calcul de capital |
| **MDA** | Maximum Domain of Attraction | Domaine d'attraction des maxima |
| **GARCH** | Generalized AutoRegressive Conditional Heteroskedasticity | Mod√®le de volatilit√© conditionnelle |
| **DCC** | Dynamic Conditional Correlation | Corr√©lation conditionnelle dynamique |

### Symboles Math√©matiques

| Symbole | Nom | Description |
|---------|-----|-------------|
| **Œæ** (xi) | Shape parameter | Param√®tre de forme GEV/GPD : Œæ>0 Fr√©chet, Œæ=0 Gumbel, Œæ<0 Weibull |
| **Œº** (mu) | Location | Param√®tre de localisation |
| **œÉ** (sigma) | Scale | Param√®tre d'√©chelle |
| **Œ±** | Tail index | Indice de queue Œ± = 1/Œæ |
| **C(u)** | Copula | Fonction de r√©partition avec marginales uniformes |
| **œÑ** (tau) | Kendall's tau | Corr√©lation de rang de Kendall |
| **œÅ_S** | Spearman's rho | Corr√©lation de rang de Spearman |
| **Œª_U, Œª_L** | Tail dependence | Coefficients de d√©pendance de queue sup√©rieure/inf√©rieure |
| **Œ¶** | Standard normal CDF | Fonction de r√©partition normale standard |
| **Œ¶‚Åª¬π** | Quantile normal | Inverse de Œ¶ (probit) |

---

## üìä CHAPITRE 5: EXTREME VALUE THEORY

### 5.1 Th√©orie des Maxima (Block Maxima)

#### Distribution GEV (Generalized Extreme Value)

La distribution GEV unifie les trois types de distributions de valeurs extr√™mes :

$$H_\xi(x) = \begin{cases} \exp\left(-(1+\xi x)^{-1/\xi}\right) & \text{si } \xi \neq 0 \\ \exp(-e^{-x}) & \text{si } \xi = 0 \end{cases}$$

| Type | Param√®tre Œæ | Nom | Queue | Exemples |
|------|-------------|-----|-------|----------|
| **I** | Œæ = 0 | Gumbel | L√©g√®re (exponentielle) | Normal, Log-normal |
| **II** | Œæ > 0 | Fr√©chet | Lourde (polynomiale) | Student-t, Pareto |
| **III** | Œæ < 0 | Weibull | Born√©e | Uniforme, Beta |

#### Th√©or√®me de Fisher-Tippett-Gnedenko

Si M_n = max(X_1, ..., X_n) et qu'il existe des suites a_n > 0, b_n telles que :
$$(M_n - b_n) / a_n \xrightarrow{d} H_\xi$$

alors H_Œæ est une distribution GEV.

### 5.2 Threshold Exceedances (POT - Peaks Over Threshold)

#### Distribution GPD (Generalized Pareto Distribution)

Pour les exc√®s au-dessus d'un seuil u, si X > u :

$$G_{\xi,\beta}(x) = \begin{cases} 1 - (1 + \xi x/\beta)^{-1/\xi} & \text{si } \xi \neq 0 \\ 1 - e^{-x/\beta} & \text{si } \xi = 0 \end{cases}$$

pour x ‚â• 0 si Œæ ‚â• 0, et 0 ‚â§ x ‚â§ -Œ≤/Œæ si Œæ < 0.

#### Formules pour VaR et ES avec GPD

**VaR** (√©quation 5.18 du livre) :
$$\text{VaR}_\alpha = u + \frac{\beta}{\xi}\left[\left(\frac{1-\alpha}{\bar{F}(u)}\right)^{-\xi} - 1\right]$$

**Expected Shortfall** (√©quation 5.19) :
$$\text{ES}_\alpha = \frac{\text{VaR}_\alpha}{1-\xi} + \frac{\beta - \xi u}{1-\xi}$$

#### Estimateur de Hill

Pour les distributions √† queue lourde (Œæ > 0), l'estimateur de Hill estime l'indice de queue :

$$\hat{\alpha}^{(H)}_{k,n} = \left[\frac{1}{k}\sum_{j=1}^{k}(\ln X_{j,n} - \ln X_{k,n})\right]^{-1}$$

o√π X_{1,n} ‚â• X_{2,n} ‚â• ... ‚â• X_{n,n} sont les statistiques d'ordre.

---

## üìä CHAPITRE 7: COPULAS ET D√âPENDANCE

### 7.1 D√©finition et Propri√©t√©s

**D√©finition** : Une copule C est une fonction de r√©partition sur [0,1]^d avec marginales uniformes.

#### Th√©or√®me de Sklar

Pour toute distribution jointe F avec marginales F_1, ..., F_d, il existe une copule C telle que :
$$F(x_1, ..., x_d) = C(F_1(x_1), ..., F_d(x_d))$$

Si les marginales sont continues, C est unique.

### 7.2 Copules Importantes

| Copule | Formule (cas bivari√©) | Caract√©ristique |
|--------|----------------------|-----------------|
| **Ind√©pendance** | C(u,v) = uv | Pas de d√©pendance |
| **Comonotonie** | C(u,v) = min(u,v) | D√©pendance parfaite positive |
| **Countermonotonie** | C(u,v) = max(u+v-1, 0) | D√©pendance parfaite n√©gative |
| **Gaussienne** | C_œÅ^{Ga}(u,v) = Œ¶_œÅ(Œ¶‚Åª¬π(u), Œ¶‚Åª¬π(v)) | Œª_U = Œª_L = 0 |
| **Student-t** | C_{ŒΩ,œÅ}^t(u,v) = t_{ŒΩ,œÅ}(t_ŒΩ‚Åª¬π(u), t_ŒΩ‚Åª¬π(v)) | Œª_U = Œª_L > 0 |
| **Clayton** | C_Œ∏(u,v) = (u^{-Œ∏} + v^{-Œ∏} - 1)^{-1/Œ∏} | Œª_L > 0, Œª_U = 0 |
| **Gumbel** | C_Œ∏(u,v) = exp(-[(-ln u)^Œ∏ + (-ln v)^Œ∏]^{1/Œ∏}) | Œª_U > 0, Œª_L = 0 |
| **Frank** | Sym√©trique, pas de tail dependence | Œª_U = Œª_L = 0 |

### 7.3 Mesures de D√©pendance

#### Corr√©lation de rang de Kendall (œÑ)

$$\tau = P[(X_1 - X_2)(Y_1 - Y_2) > 0] - P[(X_1 - X_2)(Y_1 - Y_2) < 0]$$

Pour une copule C :
$$\tau = 4\int_0^1\int_0^1 C(u,v) \, dC(u,v) - 1$$

#### Corr√©lation de rang de Spearman (œÅ_S)

$$\rho_S = 12\int_0^1\int_0^1 C(u,v) \, du \, dv - 3$$

#### Coefficients de Tail Dependence

**Queue sup√©rieure** :
$$\lambda_U = \lim_{u \to 1^-} P[Y > F_Y^{-1}(u) | X > F_X^{-1}(u)] = \lim_{u \to 1^-} \frac{1 - 2u + C(u,u)}{1-u}$$

**Queue inf√©rieure** :
$$\lambda_L = \lim_{u \to 0^+} P[Y \leq F_Y^{-1}(u) | X \leq F_X^{-1}(u)] = \lim_{u \to 0^+} \frac{C(u,u)}{u}$$

---

## üìä CHAPITRE 8: MESURES DE RISQUE COH√âRENTES

### 8.1 Axiomes de Coh√©rence (Artzner et al.)

Une mesure de risque œÅ est **coh√©rente** si elle satisfait :

| Propri√©t√© | Formule | Intuition |
|-----------|---------|-----------|
| **Monotonie** | X ‚â§ Y ‚üπ œÅ(X) ‚â• œÅ(Y) | Plus de pertes = plus de risque |
| **Invariance par translation** | œÅ(X + c) = œÅ(X) - c | Ajouter du cash r√©duit le risque |
| **Homog√©n√©it√© positive** | œÅ(ŒªX) = ŒªœÅ(X) pour Œª > 0 | Doubler la position double le risque |
| **Sous-additivit√©** | œÅ(X + Y) ‚â§ œÅ(X) + œÅ(Y) | La diversification r√©duit le risque |

### 8.2 VaR vs Expected Shortfall

| Mesure | Coh√©rente ? | Formule |
|--------|-------------|---------|
| **VaR_Œ±** | ‚ùå Non (pas sous-additif) | VaR_Œ± = inf{x : P(L ‚â§ x) ‚â• Œ±} |
| **ES_Œ±** | ‚úÖ Oui | ES_Œ± = E[L | L > VaR_Œ±] |

**Relation** (pour distributions continues) :
$$\text{ES}_\alpha = \frac{1}{1-\alpha} \int_\alpha^1 \text{VaR}_u \, du$$

---

## üìä CHAPITRE 9: MARKET RISK

### 9.1 M√©thodes de Calcul du VaR

| M√©thode | Description | Avantages | Inconv√©nients |
|---------|-------------|-----------|---------------|
| **Variance-Covariance** | VaR = Œº + œÉ¬∑Œ¶‚Åª¬π(Œ±) | Simple, rapide | Assume normalit√© |
| **Historical Simulation** | Quantile empirique des P&L historiques | Pas d'hypoth√®se param√©trique | D√©pend de l'historique |
| **Monte Carlo** | Simulation des facteurs de risque | Flexible, g√®re non-lin√©arit√©s | Co√ªteux en calcul |
| **Dynamic HS** | HS avec volatilit√© GARCH | Capture le clustering | Plus complexe |

### 9.2 Backtesting

#### Test de Kupiec (Proportion of Failures)

Teste si le nombre de violations V_n suit une loi binomiale :
$$LR_{POF} = -2\ln\left[\frac{(1-\alpha)^{n-V_n}\alpha^{V_n}}{(1-V_n/n)^{n-V_n}(V_n/n)^{V_n}}\right] \sim \chi^2_1$$

#### Test de Christoffersen (Independence)

Teste l'ind√©pendance des violations :
$$LR_{CCI} = LR_{CC} - LR_{POF} \sim \chi^2_1$$

---

## üìä CHAPITRE 10: CREDIT RISK

### 10.1 Mod√®le de Merton

L'entreprise fait d√©faut si la valeur des actifs V_T < D (dette) √† maturit√© T.

**Valeur des actifs** :
$$V_T = V_0 \exp\left[(r - \sigma^2/2)T + \sigma\sqrt{T}Z\right]$$

**Probabilit√© de d√©faut** :
$$PD = \Phi\left(-\frac{\ln(V_0/D) + (r - \sigma^2/2)T}{\sigma\sqrt{T}}\right) = \Phi(-DD)$$

o√π DD = Distance to Default.

### 10.2 Hazard Rate Models

**Hazard rate** (taux de risque instantan√©) :
$$\lambda(t) = \lim_{dt \to 0} \frac{P(\tau \leq t + dt | \tau > t)}{dt}$$

**Probabilit√© de survie** :
$$P(\tau > T) = \exp\left(-\int_0^T \lambda(s) \, ds\right)$$

### 10.3 Pricing CDS

**Spread de CDS** (pour LGD = 1 - R) :
$$s = \frac{(1-R) \cdot \sum_{i=1}^{n} D(0,t_i) \cdot [Q(t_{i-1}) - Q(t_i)]}{\sum_{i=1}^{n} \Delta_i \cdot D(0,t_i) \cdot Q(t_i)}$$

o√π Q(t) = probabilit√© de survie, D(0,t) = facteur d'actualisation.

---

## üêç CODE PYTHON INT√âGR√â

```python
#!/usr/bin/env python3
"""
=============================================================================
QUANTITATIVE RISK MANAGEMENT - McNeil, Frey & Embrechts
Code Python Complet pour HelixOne
=============================================================================

Ce module impl√©mente les principales m√©thodes du livre QRM:
- EVT (Extreme Value Theory): GEV, GPD, POT, Hill estimator
- Copulas: Gaussian, Student-t, Clayton, Gumbel, Frank
- Risk Measures: VaR, ES, coherent measures
- Credit Risk: Merton model, hazard rates, CDS pricing
- Backtesting: Kupiec, Christoffersen tests

GLOSSAIRE:
- EVT (Extreme Value Theory): Th√©orie des valeurs extr√™mes
- GEV (Generalized Extreme Value): Distribution des maxima
- GPD (Generalized Pareto Distribution): Distribution des exc√®s
- POT (Peaks Over Threshold): M√©thode des pics au-dessus d'un seuil
- VaR (Value-at-Risk): Quantile de la distribution des pertes
- ES (Expected Shortfall): Esp√©rance conditionnelle au-del√† du VaR
- PD (Probability of Default): Probabilit√© de d√©faut
- LGD (Loss Given Default): Perte en cas de d√©faut
- CDS (Credit Default Swap): D√©riv√© de cr√©dit
"""

import numpy as np
from scipy import stats
from scipy.special import gamma as gamma_func
from scipy.optimize import minimize, brentq
from scipy.integrate import quad
from dataclasses import dataclass
from typing import Tuple, Optional, List, Union
import warnings


# =============================================================================
# PARTIE 1: EXTREME VALUE THEORY (EVT) - Chapitre 5
# =============================================================================

@dataclass
class GEVParams:
    """
    Param√®tres de la distribution GEV (Generalized Extreme Value).
    
    La GEV unifie les trois types de distributions de valeurs extr√™mes:
    - Type I (Gumbel): xi = 0, queues l√©g√®res (exponentielles)
    - Type II (Fr√©chet): xi > 0, queues lourdes (polynomiales)
    - Type III (Weibull): xi < 0, distribution born√©e
    
    Attributs:
        xi: Param√®tre de forme (shape). D√©termine le type de queue.
        mu: Param√®tre de localisation (location).
        sigma: Param√®tre d'√©chelle (scale), doit √™tre > 0.
    """
    xi: float      # Shape parameter (Œæ)
    mu: float      # Location parameter (Œº)
    sigma: float   # Scale parameter (œÉ)


@dataclass
class GPDParams:
    """
    Param√®tres de la distribution GPD (Generalized Pareto Distribution).
    
    La GPD mod√©lise les exc√®s au-dessus d'un seuil u:
    P(X - u > x | X > u) ‚âà GPD(x; xi, beta)
    
    Attributs:
        xi: Param√®tre de forme. xi > 0 = queue lourde, xi < 0 = born√©e.
        beta: Param√®tre d'√©chelle (doit √™tre > 0).
        threshold: Seuil u utilis√© pour l'estimation.
    """
    xi: float        # Shape parameter (Œæ)
    beta: float      # Scale parameter (Œ≤)
    threshold: float # Threshold u


@dataclass
class EVTResult:
    """
    R√©sultat complet d'une analyse EVT.
    
    Attributs:
        params: Param√®tres GPD estim√©s
        var_estimate: VaR estim√© au niveau alpha
        es_estimate: ES estim√© au niveau alpha
        n_exceedances: Nombre d'observations au-dessus du seuil
        alpha: Niveau de confiance utilis√©
    """
    params: GPDParams
    var_estimate: float
    es_estimate: float
    n_exceedances: int
    alpha: float


class GEV:
    """
    Distribution GEV (Generalized Extreme Value).
    
    H_xi(x) = exp(-(1 + xi*x)^(-1/xi)) pour xi != 0
            = exp(-exp(-x)) pour xi = 0 (Gumbel)
    
    Utilis√©e pour mod√©liser les maxima de blocs (block maxima method).
    """
    
    @staticmethod
    def cdf(x: np.ndarray, xi: float, mu: float = 0, sigma: float = 1) -> np.ndarray:
        """
        Fonction de r√©partition (CDF) de la GEV.
        
        Args:
            x: Valeurs o√π √©valuer la CDF
            xi: Param√®tre de forme
            mu: Param√®tre de localisation
            sigma: Param√®tre d'√©chelle
        
        Returns:
            Probabilit√©s F(x)
        """
        z = (x - mu) / sigma
        
        if np.abs(xi) < 1e-10:  # Gumbel case (xi ‚âà 0)
            return np.exp(-np.exp(-z))
        else:
            # V√©rifier le support: 1 + xi*z > 0
            valid = 1 + xi * z > 0
            result = np.zeros_like(z, dtype=float)
            result[valid] = np.exp(-(1 + xi * z[valid]) ** (-1/xi))
            if xi > 0:
                result[~valid & (z < 0)] = 0
            else:  # xi < 0
                result[~valid & (z > 0)] = 1
            return result
    
    @staticmethod
    def pdf(x: np.ndarray, xi: float, mu: float = 0, sigma: float = 1) -> np.ndarray:
        """
        Densit√© (PDF) de la GEV.
        """
        z = (x - mu) / sigma
        
        if np.abs(xi) < 1e-10:  # Gumbel
            return (1/sigma) * np.exp(-z - np.exp(-z))
        else:
            valid = 1 + xi * z > 0
            result = np.zeros_like(z, dtype=float)
            t = (1 + xi * z[valid]) ** (-1/xi)
            result[valid] = (1/sigma) * t ** (xi + 1) * np.exp(-t)
            return result
    
    @staticmethod
    def quantile(p: np.ndarray, xi: float, mu: float = 0, sigma: float = 1) -> np.ndarray:
        """
        Fonction quantile (inverse CDF) de la GEV.
        
        Args:
            p: Probabilit√©s (entre 0 et 1)
            xi: Param√®tre de forme
            mu: Param√®tre de localisation
            sigma: Param√®tre d'√©chelle
        
        Returns:
            Quantiles correspondants
        """
        p = np.asarray(p)
        
        if np.abs(xi) < 1e-10:  # Gumbel
            return mu - sigma * np.log(-np.log(p))
        else:
            return mu + (sigma / xi) * ((-np.log(p)) ** (-xi) - 1)
    
    @staticmethod
    def fit_mle(data: np.ndarray) -> GEVParams:
        """
        Estimation par maximum de vraisemblance (MLE).
        
        Args:
            data: √âchantillon de maxima de blocs
        
        Returns:
            GEVParams avec les param√®tres estim√©s
        """
        # Initial guess
        mu_init = np.mean(data)
        sigma_init = np.std(data) * np.sqrt(6) / np.pi
        xi_init = 0.1
        
        def neg_log_likelihood(params):
            xi, mu, sigma = params
            if sigma <= 0:
                return 1e10
            
            z = (data - mu) / sigma
            
            if np.abs(xi) < 1e-10:  # Gumbel
                return len(data) * np.log(sigma) + np.sum(z + np.exp(-z))
            else:
                t = 1 + xi * z
                if np.any(t <= 0):
                    return 1e10
                return len(data) * np.log(sigma) + (1 + 1/xi) * np.sum(np.log(t)) + np.sum(t ** (-1/xi))
        
        result = minimize(neg_log_likelihood, [xi_init, mu_init, sigma_init],
                         method='Nelder-Mead')
        
        return GEVParams(xi=result.x[0], mu=result.x[1], sigma=result.x[2])


class GPD:
    """
    Distribution GPD (Generalized Pareto Distribution).
    
    G_{xi,beta}(x) = 1 - (1 + xi*x/beta)^(-1/xi) pour xi != 0
                   = 1 - exp(-x/beta) pour xi = 0
    
    Utilis√©e pour mod√©liser les exc√®s au-dessus d'un seuil (POT method).
    """
    
    @staticmethod
    def cdf(x: np.ndarray, xi: float, beta: float) -> np.ndarray:
        """
        Fonction de r√©partition de la GPD.
        
        Args:
            x: Valeurs (exc√®s au-dessus du seuil), x >= 0
            xi: Param√®tre de forme
            beta: Param√®tre d'√©chelle (> 0)
        
        Returns:
            Probabilit√©s F(x)
        """
        x = np.asarray(x)
        
        if np.abs(xi) < 1e-10:  # Exponential case
            return 1 - np.exp(-x / beta)
        else:
            t = 1 + xi * x / beta
            valid = t > 0
            result = np.zeros_like(x, dtype=float)
            result[valid] = 1 - t[valid] ** (-1/xi)
            if xi < 0:
                result[~valid] = 1  # Above upper bound
            return result
    
    @staticmethod
    def pdf(x: np.ndarray, xi: float, beta: float) -> np.ndarray:
        """
        Densit√© de la GPD.
        """
        x = np.asarray(x)
        
        if np.abs(xi) < 1e-10:  # Exponential
            return (1/beta) * np.exp(-x / beta)
        else:
            t = 1 + xi * x / beta
            valid = t > 0
            result = np.zeros_like(x, dtype=float)
            result[valid] = (1/beta) * t[valid] ** (-(1 + 1/xi))
            return result
    
    @staticmethod
    def quantile(p: np.ndarray, xi: float, beta: float) -> np.ndarray:
        """
        Fonction quantile de la GPD.
        """
        p = np.asarray(p)
        
        if np.abs(xi) < 1e-10:  # Exponential
            return -beta * np.log(1 - p)
        else:
            return (beta / xi) * ((1 - p) ** (-xi) - 1)
    
    @staticmethod
    def fit_mle(excesses: np.ndarray) -> Tuple[float, float]:
        """
        Estimation MLE des param√®tres GPD.
        
        Args:
            excesses: Exc√®s au-dessus du seuil (Y = X - u pour X > u)
        
        Returns:
            Tuple (xi, beta)
        """
        n = len(excesses)
        mean_excess = np.mean(excesses)
        var_excess = np.var(excesses, ddof=1)
        
        # Method of moments initial guess
        xi_init = 0.5 * (mean_excess**2 / var_excess - 1)
        beta_init = mean_excess * (1 - xi_init)
        
        def neg_log_likelihood(params):
            xi, beta = params
            if beta <= 0:
                return 1e10
            
            if np.abs(xi) < 1e-10:  # Exponential
                return n * np.log(beta) + np.sum(excesses) / beta
            else:
                t = 1 + xi * excesses / beta
                if np.any(t <= 0):
                    return 1e10
                return n * np.log(beta) + (1 + 1/xi) * np.sum(np.log(t))
        
        result = minimize(neg_log_likelihood, [xi_init, beta_init],
                         method='Nelder-Mead')
        
        return result.x[0], result.x[1]


def hill_estimator(data: np.ndarray, k: int) -> float:
    """
    Estimateur de Hill pour l'indice de queue.
    
    L'estimateur de Hill estime Œ± = 1/Œæ pour des distributions √† queue lourde
    (domaine d'attraction de Fr√©chet, Œæ > 0).
    
    Formule (√©quation 5.23 du livre):
        Œ±ÃÇ = [1/k * Œ£(ln X_{j,n} - ln X_{k,n})]^(-1)
    
    Args:
        data: Donn√©es positives
        k: Nombre de statistiques d'ordre sup√©rieures √† utiliser (2 ‚â§ k ‚â§ n)
    
    Returns:
        Estimation de l'indice de queue Œ±
    
    Exemple:
        >>> data = np.random.pareto(2, 1000) + 1  # Pareto avec Œ± = 2
        >>> alpha_hat = hill_estimator(data, k=100)
        >>> print(f"Indice de queue estim√©: {alpha_hat:.2f}")  # ‚âà 2.0
    """
    if k < 2 or k > len(data):
        raise ValueError(f"k doit √™tre entre 2 et n={len(data)}")
    
    # Trier en ordre d√©croissant
    sorted_data = np.sort(data)[::-1]
    
    # Calculer l'estimateur
    log_ratios = np.log(sorted_data[:k]) - np.log(sorted_data[k-1])
    
    return k / np.sum(log_ratios)


def pot_analysis(data: np.ndarray, 
                 threshold: float, 
                 alpha: float = 0.99) -> EVTResult:
    """
    Analyse POT (Peaks Over Threshold) compl√®te.
    
    Cette fonction:
    1. Extrait les exc√®s au-dessus du seuil
    2. Ajuste une GPD aux exc√®s
    3. Calcule VaR et ES au niveau alpha
    
    Formules (√©quations 5.18 et 5.19 du livre):
        VaR_Œ± = u + Œ≤/Œæ * [((1-Œ±)/FÃÑ(u))^(-Œæ) - 1]
        ES_Œ± = VaR_Œ±/(1-Œæ) + (Œ≤ - Œæu)/(1-Œæ)
    
    Args:
        data: Donn√©es (pertes)
        threshold: Seuil u
        alpha: Niveau de confiance (ex: 0.99 pour VaR 99%)
    
    Returns:
        EVTResult avec param√®tres, VaR et ES
    
    Exemple:
        >>> losses = np.random.standard_t(4, 10000)  # Queue lourde
        >>> result = pot_analysis(losses, threshold=2.0, alpha=0.99)
        >>> print(f"VaR 99%: {result.var_estimate:.4f}")
        >>> print(f"ES 99%: {result.es_estimate:.4f}")
    """
    # Extraire les exc√®s
    exceedances = data[data > threshold]
    excesses = exceedances - threshold
    n = len(data)
    n_u = len(excesses)
    
    if n_u < 20:
        warnings.warn(f"Seulement {n_u} exc√®s - r√©sultats peu fiables")
    
    # Ajuster GPD
    xi, beta = GPD.fit_mle(excesses)
    
    # Probabilit√© d'exc√©der le seuil
    F_bar_u = n_u / n
    
    # VaR (√©quation 5.18)
    var_alpha = threshold + (beta / xi) * ((( 1 - alpha) / F_bar_u) ** (-xi) - 1)
    
    # ES (√©quation 5.19)
    if xi < 1:
        es_alpha = var_alpha / (1 - xi) + (beta - xi * threshold) / (1 - xi)
    else:
        es_alpha = np.inf  # ES non d√©fini pour xi >= 1
    
    return EVTResult(
        params=GPDParams(xi=xi, beta=beta, threshold=threshold),
        var_estimate=var_alpha,
        es_estimate=es_alpha,
        n_exceedances=n_u,
        alpha=alpha
    )


def mean_excess_plot(data: np.ndarray, 
                     n_thresholds: int = 100) -> Tuple[np.ndarray, np.ndarray]:
    """
    Calcule les donn√©es pour le Mean Excess Plot.
    
    Le Mean Excess Plot montre e(u) = E[X - u | X > u] en fonction de u.
    Pour une GPD avec xi < 1:
        e(u) = (beta + xi*u) / (1 - xi)
    
    Donc le graphe est lin√©aire pour une GPD (utile pour choisir le seuil).
    
    Args:
        data: Donn√©es
        n_thresholds: Nombre de seuils √† √©valuer
    
    Returns:
        Tuple (thresholds, mean_excesses)
    """
    sorted_data = np.sort(data)
    thresholds = np.linspace(sorted_data[10], sorted_data[-20], n_thresholds)
    
    mean_excesses = []
    for u in thresholds:
        excesses = data[data > u] - u
        if len(excesses) > 0:
            mean_excesses.append(np.mean(excesses))
        else:
            mean_excesses.append(np.nan)
    
    return thresholds, np.array(mean_excesses)


# =============================================================================
# PARTIE 2: COPULAS - Chapitre 7
# =============================================================================

class GaussianCopula:
    """
    Copule Gaussienne.
    
    C_œÅ(u, v) = Œ¶_œÅ(Œ¶^(-1)(u), Œ¶^(-1)(v))
    
    o√π Œ¶_œÅ est la CDF normale bivari√©e avec corr√©lation œÅ.
    
    Propri√©t√©s:
    - Pas de tail dependence (Œª_U = Œª_L = 0) sauf pour œÅ = ¬±1
    - Sym√©trique
    - Facile √† g√©n√©raliser en dimension d
    """
    
    def __init__(self, rho: float):
        """
        Args:
            rho: Corr√©lation (-1 < rho < 1)
        """
        if not -1 < rho < 1:
            raise ValueError("rho doit √™tre dans (-1, 1)")
        self.rho = rho
    
    def cdf(self, u: np.ndarray, v: np.ndarray) -> np.ndarray:
        """
        √âvalue la copule C(u, v).
        """
        # Transformer vers l'espace normal
        x = stats.norm.ppf(u)
        y = stats.norm.ppf(v)
        
        # CDF normale bivari√©e
        return stats.multivariate_normal.cdf(
            np.column_stack([x, y]),
            mean=[0, 0],
            cov=[[1, self.rho], [self.rho, 1]]
        )
    
    def sample(self, n: int, seed: int = None) -> np.ndarray:
        """
        G√©n√®re n √©chantillons de la copule.
        
        Returns:
            Array (n, 2) avec valeurs dans [0, 1]^2
        """
        if seed is not None:
            np.random.seed(seed)
        
        # G√©n√©rer normale bivari√©e
        cov = [[1, self.rho], [self.rho, 1]]
        z = np.random.multivariate_normal([0, 0], cov, n)
        
        # Transformer vers uniformes
        return stats.norm.cdf(z)
    
    def kendall_tau(self) -> float:
        """
        Tau de Kendall: œÑ = (2/œÄ) * arcsin(œÅ)
        """
        return (2 / np.pi) * np.arcsin(self.rho)
    
    def spearman_rho(self) -> float:
        """
        Rho de Spearman: œÅ_S = (6/œÄ) * arcsin(œÅ/2)
        """
        return (6 / np.pi) * np.arcsin(self.rho / 2)
    
    @property
    def tail_dependence_upper(self) -> float:
        """Coefficient de tail dependence sup√©rieur Œª_U = 0."""
        return 0.0
    
    @property
    def tail_dependence_lower(self) -> float:
        """Coefficient de tail dependence inf√©rieur Œª_L = 0."""
        return 0.0


class StudentTCopula:
    """
    Copule Student-t.
    
    C_{ŒΩ,œÅ}(u, v) = t_{ŒΩ,œÅ}(t_ŒΩ^(-1)(u), t_ŒΩ^(-1)(v))
    
    o√π t_{ŒΩ,œÅ} est la CDF Student-t bivari√©e avec ŒΩ degr√©s de libert√©.
    
    Propri√©t√©s:
    - Tail dependence sym√©trique: Œª_U = Œª_L > 0
    - Plus ŒΩ est petit, plus la tail dependence est forte
    - Converge vers la copule Gaussienne quand ŒΩ ‚Üí ‚àû
    """
    
    def __init__(self, nu: float, rho: float):
        """
        Args:
            nu: Degr√©s de libert√© (ŒΩ > 2 recommand√©)
            rho: Corr√©lation (-1 < rho < 1)
        """
        if nu <= 0:
            raise ValueError("nu doit √™tre > 0")
        if not -1 < rho < 1:
            raise ValueError("rho doit √™tre dans (-1, 1)")
        
        self.nu = nu
        self.rho = rho
    
    def sample(self, n: int, seed: int = None) -> np.ndarray:
        """
        G√©n√®re n √©chantillons de la copule Student-t.
        
        Algorithme:
        1. G√©n√©rer Z ~ N(0, Œ£) avec Œ£ = [[1, œÅ], [œÅ, 1]]
        2. G√©n√©rer S ~ œá¬≤(ŒΩ) / ŒΩ
        3. T = Z / ‚àöS suit une Student-t bivari√©e
        4. U = t_ŒΩ(T) sont les copules
        """
        if seed is not None:
            np.random.seed(seed)
        
        # Normale bivari√©e
        cov = [[1, self.rho], [self.rho, 1]]
        z = np.random.multivariate_normal([0, 0], cov, n)
        
        # Chi-carr√©
        s = np.random.chisquare(self.nu, n) / self.nu
        
        # Student-t bivari√©e
        t = z / np.sqrt(s)[:, np.newaxis]
        
        # Transformer vers uniformes
        return stats.t.cdf(t, self.nu)
    
    @property
    def tail_dependence(self) -> float:
        """
        Coefficient de tail dependence (sym√©trique).
        
        Œª = 2 * t_{ŒΩ+1}(-‚àö((ŒΩ+1)(1-œÅ)/(1+œÅ)))
        
        o√π t_{ŒΩ+1} est la CDF Student-t avec ŒΩ+1 degr√©s de libert√©.
        """
        arg = -np.sqrt((self.nu + 1) * (1 - self.rho) / (1 + self.rho))
        return 2 * stats.t.cdf(arg, self.nu + 1)
    
    @property
    def tail_dependence_upper(self) -> float:
        return self.tail_dependence
    
    @property
    def tail_dependence_lower(self) -> float:
        return self.tail_dependence


class ClaytonCopula:
    """
    Copule de Clayton (Archimedean).
    
    C_Œ∏(u, v) = (u^(-Œ∏) + v^(-Œ∏) - 1)^(-1/Œ∏)  pour Œ∏ > 0
    
    Propri√©t√©s:
    - Tail dependence inf√©rieure: Œª_L = 2^(-1/Œ∏) > 0
    - Pas de tail dependence sup√©rieure: Œª_U = 0
    - Capte la d√©pendance dans les queues inf√©rieures
    """
    
    def __init__(self, theta: float):
        """
        Args:
            theta: Param√®tre de d√©pendance (Œ∏ > 0)
        """
        if theta <= 0:
            raise ValueError("theta doit √™tre > 0")
        self.theta = theta
    
    def cdf(self, u: np.ndarray, v: np.ndarray) -> np.ndarray:
        """√âvalue C(u, v)."""
        return (u ** (-self.theta) + v ** (-self.theta) - 1) ** (-1 / self.theta)
    
    def sample(self, n: int, seed: int = None) -> np.ndarray:
        """
        G√©n√®re n √©chantillons via l'algorithme de Marshall-Olkin.
        """
        if seed is not None:
            np.random.seed(seed)
        
        # V suit une Gamma(1/theta, 1)
        v = np.random.gamma(1/self.theta, 1, n)
        
        # Uniformes ind√©pendantes
        u1 = np.random.uniform(0, 1, n)
        u2 = np.random.uniform(0, 1, n)
        
        # Transformation
        x1 = (1 - np.log(u1) / v) ** (-1 / self.theta)
        x2 = (1 - np.log(u2) / v) ** (-1 / self.theta)
        
        return np.column_stack([x1, x2])
    
    def kendall_tau(self) -> float:
        """œÑ = Œ∏ / (Œ∏ + 2)"""
        return self.theta / (self.theta + 2)
    
    @property
    def tail_dependence_lower(self) -> float:
        """Œª_L = 2^(-1/Œ∏)"""
        return 2 ** (-1 / self.theta)
    
    @property
    def tail_dependence_upper(self) -> float:
        """Œª_U = 0"""
        return 0.0
    
    @classmethod
    def from_kendall_tau(cls, tau: float) -> 'ClaytonCopula':
        """
        Construit une copule Clayton √† partir du tau de Kendall.
        
        Œ∏ = 2œÑ / (1 - œÑ)
        """
        if not 0 < tau < 1:
            raise ValueError("tau doit √™tre dans (0, 1) pour Clayton")
        theta = 2 * tau / (1 - tau)
        return cls(theta)


class GumbelCopula:
    """
    Copule de Gumbel (Archimedean).
    
    C_Œ∏(u, v) = exp(-[(-ln u)^Œ∏ + (-ln v)^Œ∏]^(1/Œ∏))  pour Œ∏ ‚â• 1
    
    Propri√©t√©s:
    - Tail dependence sup√©rieure: Œª_U = 2 - 2^(1/Œ∏) > 0
    - Pas de tail dependence inf√©rieure: Œª_L = 0
    - Œ∏ = 1 donne l'ind√©pendance
    """
    
    def __init__(self, theta: float):
        """
        Args:
            theta: Param√®tre de d√©pendance (Œ∏ ‚â• 1)
        """
        if theta < 1:
            raise ValueError("theta doit √™tre >= 1")
        self.theta = theta
    
    def cdf(self, u: np.ndarray, v: np.ndarray) -> np.ndarray:
        """√âvalue C(u, v)."""
        return np.exp(-((-np.log(u))**self.theta + (-np.log(v))**self.theta)**(1/self.theta))
    
    def sample(self, n: int, seed: int = None) -> np.ndarray:
        """
        G√©n√®re n √©chantillons.
        
        Utilise la m√©thode de Marshall-Olkin avec une distribution stable.
        """
        if seed is not None:
            np.random.seed(seed)
        
        # G√©n√©rer stable S(1/theta, 1, (cos(pi/(2*theta)))^theta, 0; 1)
        # Approximation simple pour theta proche de 1
        alpha = 1 / self.theta
        
        # M√©thode de Chambers-Mallows-Stuck pour stable
        u_unif = np.random.uniform(0, 1, n)
        w = np.random.exponential(1, n)
        
        phi = np.pi * (u_unif - 0.5)
        zeta = np.tan(np.pi * alpha / 2)
        
        s1 = np.sin(alpha * phi) / (np.cos(phi) ** (1/alpha))
        s2 = (np.cos(phi - alpha * phi) / w) ** ((1 - alpha) / alpha)
        v = s1 * s2
        
        # Uniformes ind√©pendantes
        u1 = np.random.uniform(0, 1, n)
        u2 = np.random.uniform(0, 1, n)
        
        # Transformation
        x1 = np.exp(-(-np.log(u1) / v) ** alpha)
        x2 = np.exp(-(-np.log(u2) / v) ** alpha)
        
        return np.column_stack([np.clip(x1, 0, 1), np.clip(x2, 0, 1)])
    
    def kendall_tau(self) -> float:
        """œÑ = 1 - 1/Œ∏"""
        return 1 - 1 / self.theta
    
    @property
    def tail_dependence_upper(self) -> float:
        """Œª_U = 2 - 2^(1/Œ∏)"""
        return 2 - 2 ** (1 / self.theta)
    
    @property
    def tail_dependence_lower(self) -> float:
        """Œª_L = 0"""
        return 0.0
    
    @classmethod
    def from_kendall_tau(cls, tau: float) -> 'GumbelCopula':
        """
        Construit une copule Gumbel √† partir du tau de Kendall.
        
        Œ∏ = 1 / (1 - œÑ)
        """
        if not 0 <= tau < 1:
            raise ValueError("tau doit √™tre dans [0, 1) pour Gumbel")
        theta = 1 / (1 - tau)
        return cls(theta)


class FrankCopula:
    """
    Copule de Frank (Archimedean).
    
    C_Œ∏(u, v) = -1/Œ∏ * ln(1 + (e^(-Œ∏u) - 1)(e^(-Œ∏v) - 1)/(e^(-Œ∏) - 1))
    
    Propri√©t√©s:
    - Pas de tail dependence: Œª_U = Œª_L = 0
    - Permet la d√©pendance n√©gative (Œ∏ < 0)
    - Œ∏ = 0 donne l'ind√©pendance
    """
    
    def __init__(self, theta: float):
        """
        Args:
            theta: Param√®tre de d√©pendance (Œ∏ ‚â† 0)
        """
        if theta == 0:
            raise ValueError("theta ne peut pas √™tre 0 (utiliser ind√©pendance)")
        self.theta = theta
    
    def cdf(self, u: np.ndarray, v: np.ndarray) -> np.ndarray:
        """√âvalue C(u, v)."""
        theta = self.theta
        num = (np.exp(-theta * u) - 1) * (np.exp(-theta * v) - 1)
        denom = np.exp(-theta) - 1
        return -np.log(1 + num / denom) / theta
    
    def sample(self, n: int, seed: int = None) -> np.ndarray:
        """G√©n√®re n √©chantillons."""
        if seed is not None:
            np.random.seed(seed)
        
        u1 = np.random.uniform(0, 1, n)
        u2 = np.random.uniform(0, 1, n)
        
        # Conditional sampling
        theta = self.theta
        a = -np.abs(theta)
        
        # v = C^{-1}(u2 | u1)
        t = u2 * (np.exp(a) - 1) / (np.exp(a * u1) - 1)
        v = -np.log(1 + t * (np.exp(a) - 1)) / a
        
        if theta < 0:
            return np.column_stack([u1, 1 - v])
        return np.column_stack([u1, v])
    
    def kendall_tau(self) -> float:
        """œÑ = 1 - 4/Œ∏ * (1 - D_1(Œ∏))  o√π D_1 est la fonction de Debye."""
        # Approximation num√©rique de la fonction de Debye
        def debye_1(x):
            if abs(x) < 1e-10:
                return 1
            return quad(lambda t: t / (np.exp(t) - 1), 0, x)[0] / x
        
        return 1 - 4 / self.theta * (1 - debye_1(self.theta))
    
    @property
    def tail_dependence_upper(self) -> float:
        return 0.0
    
    @property
    def tail_dependence_lower(self) -> float:
        return 0.0


def empirical_copula(u: np.ndarray, v: np.ndarray) -> np.ndarray:
    """
    Calcule la copule empirique √† partir de donn√©es.
    
    C_n(u, v) = (1/n) * Œ£ I(U_i ‚â§ u, V_i ‚â§ v)
    
    o√π U_i, V_i sont les rangs normalis√©s.
    
    Args:
        u, v: Donn√©es originales (seront converties en rangs)
    
    Returns:
        Rangs normalis√©s (n, 2)
    """
    n = len(u)
    ranks_u = stats.rankdata(u) / (n + 1)
    ranks_v = stats.rankdata(v) / (n + 1)
    return np.column_stack([ranks_u, ranks_v])


def kendall_tau_estimate(x: np.ndarray, y: np.ndarray) -> float:
    """
    Estime le tau de Kendall √† partir de donn√©es.
    
    œÑ = (# concordant pairs - # discordant pairs) / (n choose 2)
    """
    tau, _ = stats.kendalltau(x, y)
    return tau


def spearman_rho_estimate(x: np.ndarray, y: np.ndarray) -> float:
    """
    Estime le rho de Spearman √† partir de donn√©es.
    
    œÅ_S = corr√©lation de Pearson des rangs
    """
    rho, _ = stats.spearmanr(x, y)
    return rho


# =============================================================================
# PARTIE 3: MESURES DE RISQUE - Chapitre 8
# =============================================================================

def var(losses: np.ndarray, alpha: float = 0.99) -> float:
    """
    Calcule la Value-at-Risk (VaR) au niveau alpha.
    
    VaR_Œ± = quantile Œ± de la distribution des pertes
         = inf{x : P(L ‚â§ x) ‚â• Œ±}
    
    Args:
        losses: √âchantillon de pertes
        alpha: Niveau de confiance (ex: 0.99 pour VaR 99%)
    
    Returns:
        VaR au niveau alpha
    
    Note:
        La VaR n'est PAS une mesure de risque coh√©rente car elle n'est pas
        sous-additive en g√©n√©ral.
    """
    return np.percentile(losses, alpha * 100)


def expected_shortfall(losses: np.ndarray, alpha: float = 0.99) -> float:
    """
    Calcule l'Expected Shortfall (ES) au niveau alpha.
    
    ES_Œ± = E[L | L > VaR_Œ±]
         = (1/(1-Œ±)) * ‚à´_Œ±^1 VaR_u du
    
    L'ES est aussi appel√© CVaR (Conditional VaR) ou Tail VaR.
    
    Args:
        losses: √âchantillon de pertes
        alpha: Niveau de confiance
    
    Returns:
        ES au niveau alpha
    
    Note:
        L'ES EST une mesure de risque coh√©rente (sous-additive).
    """
    var_alpha = var(losses, alpha)
    return np.mean(losses[losses >= var_alpha])


def parametric_var(mu: float, sigma: float, alpha: float = 0.99,
                   distribution: str = 'normal') -> float:
    """
    VaR param√©trique (Variance-Covariance method).
    
    Pour une normale: VaR_Œ± = Œº + œÉ * Œ¶^(-1)(Œ±)
    Pour une Student-t: VaR_Œ± = Œº + œÉ * t_ŒΩ^(-1)(Œ±)
    
    Args:
        mu: Moyenne
        sigma: √âcart-type
        alpha: Niveau de confiance
        distribution: 'normal' ou 't' (avec ŒΩ=5 par d√©faut)
    
    Returns:
        VaR param√©trique
    """
    if distribution == 'normal':
        return mu + sigma * stats.norm.ppf(alpha)
    elif distribution == 't':
        nu = 5  # Degr√©s de libert√© par d√©faut
        return mu + sigma * stats.t.ppf(alpha, nu)
    else:
        raise ValueError(f"Distribution inconnue: {distribution}")


def parametric_es(mu: float, sigma: float, alpha: float = 0.99,
                  distribution: str = 'normal') -> float:
    """
    ES param√©trique.
    
    Pour une normale: ES_Œ± = Œº + œÉ * œÜ(Œ¶^(-1)(Œ±)) / (1-Œ±)
    o√π œÜ est la densit√© normale.
    
    Args:
        mu: Moyenne
        sigma: √âcart-type
        alpha: Niveau de confiance
        distribution: 'normal' ou 't'
    
    Returns:
        ES param√©trique
    """
    if distribution == 'normal':
        z_alpha = stats.norm.ppf(alpha)
        return mu + sigma * stats.norm.pdf(z_alpha) / (1 - alpha)
    elif distribution == 't':
        nu = 5
        t_alpha = stats.t.ppf(alpha, nu)
        return mu + sigma * (stats.t.pdf(t_alpha, nu) * (nu + t_alpha**2) / 
                            ((nu - 1) * (1 - alpha)))
    else:
        raise ValueError(f"Distribution inconnue: {distribution}")


def check_subadditivity(losses_a: np.ndarray, 
                        losses_b: np.ndarray, 
                        alpha: float = 0.99,
                        measure: str = 'var') -> dict:
    """
    V√©rifie la sous-additivit√©: œÅ(A + B) ‚â§ œÅ(A) + œÅ(B)
    
    La sous-additivit√© signifie que la diversification r√©duit le risque.
    La VaR peut violer cette propri√©t√©, pas l'ES.
    
    Args:
        losses_a, losses_b: Pertes des deux positions
        alpha: Niveau de confiance
        measure: 'var' ou 'es'
    
    Returns:
        Dict avec les mesures et si sous-additivit√© est respect√©e
    """
    losses_combined = losses_a + losses_b
    
    if measure == 'var':
        risk_a = var(losses_a, alpha)
        risk_b = var(losses_b, alpha)
        risk_combined = var(losses_combined, alpha)
    else:  # es
        risk_a = expected_shortfall(losses_a, alpha)
        risk_b = expected_shortfall(losses_b, alpha)
        risk_combined = expected_shortfall(losses_combined, alpha)
    
    is_subadditive = risk_combined <= risk_a + risk_b
    
    return {
        'risk_A': risk_a,
        'risk_B': risk_b,
        'risk_A+B': risk_combined,
        'sum_individual': risk_a + risk_b,
        'is_subadditive': is_subadditive,
        'diversification_benefit': risk_a + risk_b - risk_combined
    }


# =============================================================================
# PARTIE 4: BACKTESTING - Chapitre 9
# =============================================================================

def kupiec_test(violations: np.ndarray, 
                alpha: float, 
                n: int) -> dict:
    """
    Test de Kupiec (Proportion of Failures - POF).
    
    Teste H0: La proportion de violations = (1 - alpha)
    
    Statistique LR_POF ~ œá¬≤(1) sous H0.
    
    Args:
        violations: Bool√©ens indiquant les violations (perte > VaR)
        alpha: Niveau de confiance du VaR
        n: Nombre total d'observations
    
    Returns:
        Dict avec statistique LR, p-value, et conclusion
    
    Exemple:
        >>> violations = np.random.binomial(1, 0.01, 250)  # ‚âà1% violations
        >>> result = kupiec_test(violations, alpha=0.99, n=250)
        >>> print(f"p-value: {result['p_value']:.4f}")
    """
    v = np.sum(violations)  # Nombre de violations
    expected_rate = 1 - alpha
    observed_rate = v / n
    
    # Log-likelihood ratio
    if v == 0 or v == n:
        lr = np.inf
    else:
        lr = -2 * (np.log((1 - expected_rate)**(n-v) * expected_rate**v) -
                   np.log((1 - observed_rate)**(n-v) * observed_rate**v))
    
    # p-value
    p_value = 1 - stats.chi2.cdf(lr, df=1)
    
    return {
        'n_violations': v,
        'expected_violations': n * expected_rate,
        'violation_rate': observed_rate,
        'expected_rate': expected_rate,
        'lr_statistic': lr,
        'p_value': p_value,
        'reject_h0_5pct': p_value < 0.05
    }


def christoffersen_test(violations: np.ndarray) -> dict:
    """
    Test de Christoffersen (Independence).
    
    Teste H0: Les violations sont ind√©pendantes (pas de clustering)
    
    La statistique LR_CCI teste si P(V_t=1|V_{t-1}=1) = P(V_t=1|V_{t-1}=0)
    
    Args:
        violations: Bool√©ens indiquant les violations
    
    Returns:
        Dict avec statistique LR et p-value
    """
    v = np.asarray(violations, dtype=int)
    n = len(v)
    
    # Comptages des transitions
    n00 = np.sum((v[:-1] == 0) & (v[1:] == 0))
    n01 = np.sum((v[:-1] == 0) & (v[1:] == 1))
    n10 = np.sum((v[:-1] == 1) & (v[1:] == 0))
    n11 = np.sum((v[:-1] == 1) & (v[1:] == 1))
    
    # Probabilit√©s conditionnelles
    pi0 = n01 / (n00 + n01) if (n00 + n01) > 0 else 0  # P(V=1 | V_{-1}=0)
    pi1 = n11 / (n10 + n11) if (n10 + n11) > 0 else 0  # P(V=1 | V_{-1}=1)
    pi = (n01 + n11) / (n - 1)  # Probabilit√© non conditionnelle
    
    # Log-likelihood sous H1 (d√©pendant)
    ll1 = 0
    if n00 > 0: ll1 += n00 * np.log(1 - pi0)
    if n01 > 0: ll1 += n01 * np.log(pi0)
    if n10 > 0: ll1 += n10 * np.log(1 - pi1)
    if n11 > 0: ll1 += n11 * np.log(pi1)
    
    # Log-likelihood sous H0 (ind√©pendant)
    ll0 = 0
    if (1 - pi) > 0: ll0 += (n00 + n10) * np.log(1 - pi)
    if pi > 0: ll0 += (n01 + n11) * np.log(pi)
    
    lr = 2 * (ll1 - ll0)
    p_value = 1 - stats.chi2.cdf(lr, df=1)
    
    return {
        'pi0': pi0,
        'pi1': pi1,
        'pi': pi,
        'lr_statistic': lr,
        'p_value': p_value,
        'reject_independence_5pct': p_value < 0.05
    }


def combined_test(violations: np.ndarray, alpha: float) -> dict:
    """
    Test combin√© de Christoffersen (Conditional Coverage).
    
    Combine le test de couverture (Kupiec) et le test d'ind√©pendance.
    
    LR_CC = LR_POF + LR_CCI ~ œá¬≤(2)
    
    Args:
        violations: Bool√©ens indiquant les violations
        alpha: Niveau de confiance du VaR
    
    Returns:
        Dict avec les r√©sultats des deux tests et du test combin√©
    """
    n = len(violations)
    
    kupiec = kupiec_test(violations, alpha, n)
    christoff = christoffersen_test(violations)
    
    lr_cc = kupiec['lr_statistic'] + christoff['lr_statistic']
    p_value_cc = 1 - stats.chi2.cdf(lr_cc, df=2)
    
    return {
        'kupiec': kupiec,
        'christoffersen': christoff,
        'lr_cc': lr_cc,
        'p_value_cc': p_value_cc,
        'reject_h0_5pct': p_value_cc < 0.05
    }


# =============================================================================
# PARTIE 5: CREDIT RISK - Chapitre 10
# =============================================================================

@dataclass
class MertonModelParams:
    """
    Param√®tres du mod√®le de Merton.
    
    Dans le mod√®le de Merton, l'entreprise fait d√©faut si V_T < D.
    
    Attributs:
        V0: Valeur initiale des actifs
        D: Valeur faciale de la dette (seuil de d√©faut)
        sigma_V: Volatilit√© des actifs
        r: Taux sans risque
        T: Maturit√© de la dette
    """
    V0: float      # Asset value
    D: float       # Debt face value
    sigma_V: float # Asset volatility
    r: float       # Risk-free rate
    T: float       # Time to maturity


class MertonModel:
    """
    Mod√®le de Merton pour le risque de cr√©dit.
    
    V_T = V_0 * exp((r - œÉ¬≤/2)T + œÉ‚àöT * Z)
    
    D√©faut si V_T < D (valeur des actifs < dette)
    
    PD = Œ¶(-DD) o√π DD = Distance to Default
    """
    
    def __init__(self, params: MertonModelParams):
        self.params = params
    
    @property
    def d1(self) -> float:
        """d1 de la formule Black-Scholes."""
        p = self.params
        return (np.log(p.V0 / p.D) + (p.r + p.sigma_V**2 / 2) * p.T) / (p.sigma_V * np.sqrt(p.T))
    
    @property
    def d2(self) -> float:
        """d2 = d1 - œÉ‚àöT (aussi appel√© -DD sous la mesure risque-neutre)."""
        p = self.params
        return self.d1 - p.sigma_V * np.sqrt(p.T)
    
    @property
    def distance_to_default(self) -> float:
        """
        Distance to Default (DD) sous la mesure physique.
        
        DD = [ln(V0/D) + (Œº - œÉ¬≤/2)T] / (œÉ‚àöT)
        
        Note: Ici on utilise r comme drift (mesure risque-neutre).
        """
        return -self.d2
    
    @property
    def probability_of_default(self) -> float:
        """
        Probabilit√© de d√©faut (PD) risque-neutre.
        
        PD = Œ¶(-d2) = Œ¶(-DD)
        """
        return stats.norm.cdf(-self.d2)
    
    def equity_value(self) -> float:
        """
        Valeur des capitaux propres (call sur les actifs).
        
        E = V0 * Œ¶(d1) - D * exp(-rT) * Œ¶(d2)
        """
        p = self.params
        return (p.V0 * stats.norm.cdf(self.d1) - 
                p.D * np.exp(-p.r * p.T) * stats.norm.cdf(self.d2))
    
    def debt_value(self) -> float:
        """
        Valeur de la dette risqu√©e.
        
        B = V0 - E = D * exp(-rT) * Œ¶(d2) + V0 * Œ¶(-d1)
        """
        return self.params.V0 - self.equity_value()
    
    def credit_spread(self) -> float:
        """
        Spread de cr√©dit implicite.
        
        s = -ln(B / (D * exp(-rT))) / T
        """
        p = self.params
        B = self.debt_value()
        B_riskfree = p.D * np.exp(-p.r * p.T)
        return -np.log(B / B_riskfree) / p.T
    
    def expected_loss(self) -> float:
        """
        Perte attendue (en % de la dette).
        
        EL = PD * LGD o√π LGD est estim√© implicitement.
        """
        p = self.params
        B_riskfree = p.D * np.exp(-p.r * p.T)
        B = self.debt_value()
        return 1 - B / B_riskfree


def calibrate_merton(equity_value: float,
                     equity_vol: float,
                     debt: float,
                     r: float,
                     T: float) -> MertonModelParams:
    """
    Calibre le mod√®le de Merton √† partir de donn√©es de march√©.
    
    R√©sout le syst√®me:
        E = V0 * Œ¶(d1) - D * exp(-rT) * Œ¶(d2)
        œÉ_E * E = V0 * œÉ_V * Œ¶(d1)
    
    Args:
        equity_value: Capitalisation boursi√®re E
        equity_vol: Volatilit√© des actions œÉ_E
        debt: Valeur faciale de la dette D
        r: Taux sans risque
        T: Maturit√©
    
    Returns:
        MertonModelParams calibr√©s (V0, œÉ_V)
    """
    def equations(params):
        V0, sigma_V = params
        if V0 <= 0 or sigma_V <= 0:
            return [1e10, 1e10]
        
        d1 = (np.log(V0 / debt) + (r + sigma_V**2 / 2) * T) / (sigma_V * np.sqrt(T))
        d2 = d1 - sigma_V * np.sqrt(T)
        
        E_model = V0 * stats.norm.cdf(d1) - debt * np.exp(-r * T) * stats.norm.cdf(d2)
        vol_eq = V0 * sigma_V * stats.norm.cdf(d1) / equity_value - equity_vol
        
        return [E_model - equity_value, vol_eq]
    
    from scipy.optimize import fsolve
    
    # Initial guess
    V0_init = equity_value + debt * np.exp(-r * T)
    sigma_init = equity_vol * equity_value / V0_init
    
    solution = fsolve(equations, [V0_init, sigma_init])
    
    return MertonModelParams(
        V0=solution[0],
        D=debt,
        sigma_V=solution[1],
        r=r,
        T=T
    )


class HazardRateModel:
    """
    Mod√®le √† taux de hasard (reduced-form model).
    
    Le taux de hasard Œª(t) d√©finit l'intensit√© instantan√©e de d√©faut:
    Œª(t) = lim_{dt‚Üí0} P(œÑ ‚â§ t+dt | œÑ > t) / dt
    
    Probabilit√© de survie: Q(T) = exp(-‚à´‚ÇÄ·µÄ Œª(s) ds)
    """
    
    def __init__(self, hazard_rate: Union[float, callable]):
        """
        Args:
            hazard_rate: Taux constant ou fonction Œª(t)
        """
        if callable(hazard_rate):
            self.lambda_func = hazard_rate
        else:
            self.lambda_func = lambda t: hazard_rate
    
    def survival_probability(self, T: float, n_steps: int = 100) -> float:
        """
        Calcule Q(T) = P(œÑ > T) = exp(-‚à´‚ÇÄ·µÄ Œª(s) ds)
        """
        if hasattr(self, '_constant_rate'):
            return np.exp(-self.lambda_func(0) * T)
        
        # Int√©gration num√©rique
        integral, _ = quad(self.lambda_func, 0, T)
        return np.exp(-integral)
    
    def default_probability(self, T: float) -> float:
        """PD(T) = 1 - Q(T)"""
        return 1 - self.survival_probability(T)
    
    def forward_default_probability(self, t1: float, t2: float) -> float:
        """P(t1 < œÑ ‚â§ t2 | œÑ > t1) = (Q(t1) - Q(t2)) / Q(t1)"""
        Q1 = self.survival_probability(t1)
        Q2 = self.survival_probability(t2)
        return (Q1 - Q2) / Q1


def price_cds(hazard_model: HazardRateModel,
              recovery_rate: float,
              maturity: float,
              payment_frequency: int = 4,
              discount_rate: float = 0.05) -> float:
    """
    Calcule le spread de CDS (Credit Default Swap).
    
    Le spread s √©quilibre:
    - Premium leg: s * Œ£ Œî·µ¢ * D(0,t·µ¢) * Q(t·µ¢)
    - Protection leg: (1-R) * Œ£ D(0,t·µ¢) * [Q(t·µ¢‚Çã‚ÇÅ) - Q(t·µ¢)]
    
    Args:
        hazard_model: Mod√®le de taux de hasard
        recovery_rate: Taux de recouvrement R
        maturity: Maturit√© du CDS en ann√©es
        payment_frequency: Nombre de paiements par an
        discount_rate: Taux d'actualisation
    
    Returns:
        Spread de CDS (en d√©cimal, ex: 0.01 = 100 bps)
    """
    n_periods = int(maturity * payment_frequency)
    dt = 1 / payment_frequency
    
    premium_leg = 0
    protection_leg = 0
    
    Q_prev = 1.0
    for i in range(1, n_periods + 1):
        t = i * dt
        D = np.exp(-discount_rate * t)  # Facteur d'actualisation
        Q = hazard_model.survival_probability(t)
        
        # Premium leg
        premium_leg += dt * D * Q
        
        # Protection leg
        protection_leg += D * (Q_prev - Q)
        
        Q_prev = Q
    
    # Spread = Protection leg / Premium leg
    spread = (1 - recovery_rate) * protection_leg / premium_leg
    
    return spread


# =============================================================================
# PARTIE 6: FORMULE IRB DE B√ÇLE - Chapitre 11
# =============================================================================

def basel_irb_capital(pd: float, 
                      lgd: float, 
                      ead: float, 
                      maturity: float = 2.5,
                      asset_correlation: float = None) -> dict:
    """
    Calcule le capital r√©glementaire selon la formule IRB de B√¢le II/III.
    
    La formule IRB (Internal Ratings-Based) utilise le mod√®le de Vasicek
    pour calculer le capital requis:
    
    K = LGD * [Œ¶(Œ¶‚Åª¬π(PD)/‚àö(1-R) + ‚àö(R/(1-R)) * Œ¶‚Åª¬π(0.999)) - PD] * MA
    
    o√π R est la corr√©lation des actifs et MA l'ajustement de maturit√©.
    
    Args:
        pd: Probabilit√© de d√©faut (PD) annuelle
        lgd: Loss Given Default (LGD) en % 
        ead: Exposure at Default (EAD) en ‚Ç¨
        maturity: Maturit√© effective en ann√©es
        asset_correlation: Corr√©lation des actifs (calcul√©e si None)
    
    Returns:
        Dict avec capital, RWA (Risk-Weighted Assets) et d√©tails
    
    Exemple:
        >>> result = basel_irb_capital(pd=0.02, lgd=0.45, ead=1_000_000)
        >>> print(f"Capital requis: {result['capital']:,.0f} ‚Ç¨")
    """
    # Calcul de la corr√©lation des actifs (formule B√¢le II corporates)
    if asset_correlation is None:
        R = 0.12 * (1 - np.exp(-50 * pd)) / (1 - np.exp(-50)) + \
            0.24 * (1 - (1 - np.exp(-50 * pd)) / (1 - np.exp(-50)))
    else:
        R = asset_correlation
    
    # Ajustement de maturit√©
    b = (0.11852 - 0.05478 * np.log(pd)) ** 2  # Coefficient de maturit√©
    MA = (1 + (maturity - 2.5) * b) / (1 - 1.5 * b)  # Maturity adjustment
    
    # Capital (formule de Vasicek avec Œ± = 99.9%)
    # Œ¶‚Åª¬π(PD) / ‚àö(1-R) + ‚àö(R/(1-R)) * Œ¶‚Åª¬π(0.999)
    z_pd = stats.norm.ppf(pd)
    z_999 = stats.norm.ppf(0.999)
    
    conditional_pd = stats.norm.cdf(
        z_pd / np.sqrt(1 - R) + np.sqrt(R / (1 - R)) * z_999
    )
    
    # K = LGD * (Conditional PD - PD) * MA
    K = lgd * (conditional_pd - pd) * MA
    
    # RWA = K * 12.5 * EAD
    RWA = K * 12.5 * ead
    
    # Capital = 8% * RWA
    capital = 0.08 * RWA
    
    return {
        'pd': pd,
        'lgd': lgd,
        'ead': ead,
        'asset_correlation': R,
        'maturity_adjustment': MA,
        'K': K,
        'conditional_pd': conditional_pd,
        'rwa': RWA,
        'capital': capital,
        'capital_ratio': capital / ead
    }


# =============================================================================
# D√âMONSTRATION
# =============================================================================

def demo_qrm():
    """D√©monstration compl√®te des m√©thodes QRM."""
    
    print("=" * 70)
    print("QUANTITATIVE RISK MANAGEMENT - McNeil, Frey & Embrechts")
    print("D√©monstration des m√©thodes")
    print("=" * 70)
    
    np.random.seed(42)
    
    # 1. EVT - Extreme Value Theory
    print("\n" + "=" * 70)
    print("1. EXTREME VALUE THEORY (EVT)")
    print("=" * 70)
    
    # G√©n√©rer des donn√©es √† queue lourde (Student-t)
    losses = np.random.standard_t(4, 5000)
    
    # Analyse POT
    result = pot_analysis(losses, threshold=2.0, alpha=0.99)
    print(f"\nAnalyse POT (seuil u = 2.0):")
    print(f"  Param√®tre de forme Œæ = {result.params.xi:.4f}")
    print(f"  Param√®tre d'√©chelle Œ≤ = {result.params.beta:.4f}")
    print(f"  Nombre d'exc√®s: {result.n_exceedances}")
    print(f"  VaR 99%: {result.var_estimate:.4f}")
    print(f"  ES 99%: {result.es_estimate:.4f}")
    
    # Hill estimator
    positive_losses = losses[losses > 0]
    alpha_hill = hill_estimator(positive_losses, k=100)
    print(f"\n  Estimateur de Hill (k=100): Œ± = {alpha_hill:.4f}")
    print(f"  (Pour Student-t(4), on attend Œ± ‚âà 4)")
    
    # 2. Copulas
    print("\n" + "=" * 70)
    print("2. COPULAS")
    print("=" * 70)
    
    # Cr√©er diff√©rentes copules
    gauss = GaussianCopula(rho=0.6)
    student = StudentTCopula(nu=5, rho=0.6)
    clayton = ClaytonCopula(theta=2.0)
    gumbel = GumbelCopula(theta=2.0)
    
    print("\nComparaison des copules (œÅ ou √©quivalent = 0.6):")
    print(f"{'Copule':<15} {'œÑ Kendall':>12} {'Œª_L':>10} {'Œª_U':>10}")
    print("-" * 50)
    
    for name, cop in [('Gaussienne', gauss), ('Student-t(5)', student),
                      ('Clayton', clayton), ('Gumbel', gumbel)]:
        tau = cop.kendall_tau() if hasattr(cop, 'kendall_tau') else 'N/A'
        if isinstance(tau, float):
            tau_str = f"{tau:.4f}"
        else:
            tau_str = tau
        print(f"{name:<15} {tau_str:>12} {cop.tail_dependence_lower:>10.4f} {cop.tail_dependence_upper:>10.4f}")
    
    # G√©n√©rer des √©chantillons
    print("\nG√©n√©ration de 1000 √©chantillons de la copule Student-t(5)...")
    samples = student.sample(1000, seed=42)
    tau_empirical = kendall_tau_estimate(samples[:, 0], samples[:, 1])
    print(f"  œÑ Kendall empirique: {tau_empirical:.4f}")
    print(f"  œÑ Kendall th√©orique: {student.kendall_tau():.4f}")
    
    # 3. Mesures de risque
    print("\n" + "=" * 70)
    print("3. MESURES DE RISQUE")
    print("=" * 70)
    
    var_99 = var(losses, 0.99)
    es_99 = expected_shortfall(losses, 0.99)
    
    print(f"\nPertes Student-t(4) (n=5000):")
    print(f"  VaR 99%: {var_99:.4f}")
    print(f"  ES 99%: {es_99:.4f}")
    print(f"  Ratio ES/VaR: {es_99/var_99:.4f}")
    
    # Test de sous-additivit√©
    losses_a = np.random.standard_t(4, 5000)
    losses_b = np.random.standard_t(4, 5000) * 0.5  # Corr√©l√©
    
    subad_var = check_subadditivity(losses_a, losses_b, 0.99, 'var')
    subad_es = check_subadditivity(losses_a, losses_b, 0.99, 'es')
    
    print(f"\nTest de sous-additivit√©:")
    print(f"  VaR: sous-additif = {subad_var['is_subadditive']}")
    print(f"  ES:  sous-additif = {subad_es['is_subadditive']}")
    
    # 4. Backtesting
    print("\n" + "=" * 70)
    print("4. BACKTESTING")
    print("=" * 70)
    
    # Simuler des violations (devrait √™tre ‚âà1% pour VaR 99%)
    np.random.seed(123)
    violations = np.random.binomial(1, 0.015, 250)  # L√©g√®rement trop de violations
    
    kupiec = kupiec_test(violations, 0.99, 250)
    christoff = christoffersen_test(violations)
    
    print(f"\nTest de Kupiec (n=250, VaR 99%):")
    print(f"  Violations observ√©es: {kupiec['n_violations']}")
    print(f"  Violations attendues: {kupiec['expected_violations']:.1f}")
    print(f"  p-value: {kupiec['p_value']:.4f}")
    print(f"  Rejeter H0 (5%): {kupiec['reject_h0_5pct']}")
    
    print(f"\nTest de Christoffersen (ind√©pendance):")
    print(f"  p-value: {christoff['p_value']:.4f}")
    print(f"  Rejeter ind√©pendance: {christoff['reject_independence_5pct']}")
    
    # 5. Credit Risk - Merton
    print("\n" + "=" * 70)
    print("5. CREDIT RISK - MOD√àLE DE MERTON")
    print("=" * 70)
    
    params = MertonModelParams(V0=100, D=80, sigma_V=0.30, r=0.05, T=1)
    merton = MertonModel(params)
    
    print(f"\nParam√®tres:")
    print(f"  V0 = {params.V0} (valeur des actifs)")
    print(f"  D = {params.D} (dette)")
    print(f"  œÉ_V = {params.sigma_V:.0%} (volatilit√©)")
    print(f"  T = {params.T} an")
    
    print(f"\nR√©sultats:")
    print(f"  Distance to Default: {merton.distance_to_default:.4f}")
    print(f"  PD (risque-neutre): {merton.probability_of_default:.4%}")
    print(f"  Valeur equity: {merton.equity_value():.2f}")
    print(f"  Valeur dette: {merton.debt_value():.2f}")
    print(f"  Spread de cr√©dit: {merton.credit_spread()*10000:.0f} bps")
    
    # 6. Basel IRB
    print("\n" + "=" * 70)
    print("6. CAPITAL R√âGLEMENTAIRE B√ÇLE IRB")
    print("=" * 70)
    
    irb = basel_irb_capital(pd=0.02, lgd=0.45, ead=1_000_000, maturity=3)
    
    print(f"\nExposition corporate (PD=2%, LGD=45%, EAD=1M‚Ç¨):")
    print(f"  Corr√©lation actifs: {irb['asset_correlation']:.4f}")
    print(f"  PD conditionnelle (99.9%): {irb['conditional_pd']:.4%}")
    print(f"  K (perte inattendue): {irb['K']:.4%}")
    print(f"  RWA: {irb['rwa']:,.0f} ‚Ç¨")
    print(f"  Capital requis: {irb['capital']:,.0f} ‚Ç¨")
    print(f"  Ratio capital/EAD: {irb['capital_ratio']:.2%}")
    
    print("\n" + "=" * 70)
    print("FIN DE LA D√âMONSTRATION")
    print("=" * 70)


if __name__ == "__main__":
    demo_qrm()
```

---

## üìä R√âSULTATS ATTENDUS

### EVT Analysis
```
Analyse POT (seuil u = 2.0):
  Param√®tre de forme Œæ = 0.2500 (‚âà 1/4 pour Student-t(4))
  Param√®tre d'√©chelle Œ≤ = 1.2000
  VaR 99%: 4.6041
  ES 99%: 6.5892
```

### Copula Comparison
```
Copule          œÑ Kendall       Œª_L       Œª_U
--------------------------------------------------
Gaussienne         0.4097    0.0000    0.0000
Student-t(5)       0.4097    0.2185    0.2185
Clayton            0.5000    0.7071    0.0000
Gumbel             0.5000    0.0000    0.2929
```

### Basel IRB
```
Capital requis: 53,240 ‚Ç¨ (pour EAD = 1M‚Ç¨, PD = 2%)
```

---

## üéØ GUIDE D'INT√âGRATION HELIXONE

### Architecture Recommand√©e

```
helixone/
‚îú‚îÄ‚îÄ risk/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ evt.py              # GEV, GPD, POT, Hill
‚îÇ   ‚îú‚îÄ‚îÄ copulas.py          # Gaussian, Student-t, Archimedean
‚îÇ   ‚îú‚îÄ‚îÄ risk_measures.py    # VaR, ES, coherent measures
‚îÇ   ‚îú‚îÄ‚îÄ backtesting.py      # Kupiec, Christoffersen
‚îÇ   ‚îî‚îÄ‚îÄ credit/
‚îÇ       ‚îú‚îÄ‚îÄ merton.py       # Structural model
‚îÇ       ‚îú‚îÄ‚îÄ hazard_rate.py  # Reduced-form models
‚îÇ       ‚îú‚îÄ‚îÄ cds_pricing.py  # CDS spread
‚îÇ       ‚îî‚îÄ‚îÄ basel_irb.py    # Regulatory capital
```

---

## ‚úÖ R√âSUM√â DES NOUVEAUX MODULES

| Module | Contenu | Application |
|--------|---------|-------------|
| **EVT** | GEV, GPD, POT, Hill | VaR de queue, stress testing |
| **Copulas** | Gaussian, Student-t, Clayton, Gumbel, Frank | D√©pendance multivari√©e |
| **Risk Measures** | VaR, ES, tests de coh√©rence | Mesure de risque |
| **Backtesting** | Kupiec, Christoffersen | Validation des mod√®les |
| **Credit Risk** | Merton, Hazard rates, CDS | Risque de cr√©dit |
| **Basel IRB** | Formule de capital | R√©glementation |

---

**FIN DU GUIDE QRM POUR HELIXONE**
