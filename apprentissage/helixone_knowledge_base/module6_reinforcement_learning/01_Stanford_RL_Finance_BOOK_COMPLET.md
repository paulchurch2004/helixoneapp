# Foundations of Reinforcement Learning with Applications in Finance

> **Source**: https://stanford.edu/~ashlearn/RLForFinanceBook/book.pdf
> **Auteurs**: Ashwin Rao, Tikhon Jelvis
> **Extrait pour la base de connaissances HelixOne**

---

## Table des MatiÃ¨res

### Preface (p.11)
### Summary of Notation (p.15)

## 1. Overview (p.17)

### 1.1. Learning Reinforcement Learning
Reinforcement Learning (RL) is emerging as a practical, powerful technique for solving a variety of complex business problems across industries that involve **Sequential Optimal Decisioning under Uncertainty**. Although RL is classified as a branch of Machine Learning (ML), it tends to be viewed and treated quite differently from other branches of ML (Supervised and Unsupervised Learning). Indeed, RL seems to hold the key to unlocking the promise of AIâ€”machines that adapt their decisions to vagaries in observed information, while continuously steering towards the optimal outcome.

### 1.2. What You Will Learn from This Book
- Theory of Markov Decision Processes (MDPs)â€”a framework for Sequential Optimal Decisioning under Uncertainty
- Power of Bellman Equations
- Dynamic Programming (DP) Algorithms: Policy Iteration, Value Iteration, Backward Induction, Approximate Dynamic Programming
- Generalized Policy Iteration
- RL Algorithms: SARSA, Q-Learning, Gradient TD, DQN, LSPI, Policy Gradient, MCTS
- Multi-Armed Bandits: UCB, Thompson Sampling, Gradient Bandits
- Financial Applications:
  - Dynamic Asset-Allocation to maximize Utility of Consumption
  - Pricing and Hedging of Derivatives in an Incomplete Market
  - Optimal Exercise/Stopping of Path-Dependent American Options
  - Optimal Trade Order Execution (managing Price Impact)
  - Optimal Market-Making (Bid/Ask managing Inventory Risk)

### 1.3. Expected Background
- Python experience (numpy)
- Undergraduate-level Probability (most important foundation)
- Numerical Optimization, Statistics, Linear Algebra
- No Finance background required

### 1.4. Decluttering the Jargon
**Key Terms:**
- **Uncertainty**: Problems involving random variables evolving over time (stochastic processes)
- **Optimal Decisions**: Optimization - maximizing a well-defined quantity (the "goal")
- **Sequential**: Dynamic decisions adjusted to "changing circumstances"
- **Control**: Persistent steering towards the goal
- **Stochastic Control**: The combined framework

### 1.5. Introduction to the MDP Framework

The MDP Framework consists of:
- **Agent**: An AI algorithm
- **Environment**: Abstract entity serving up uncertain outcomes
- **State** (St âˆˆ S): Abstract piece of information at time t
- **Action** (At âˆˆ A): Activity performed by the Agent
- **Reward** (Rt âˆˆ D): Numerical feedback

**Transition probabilities:**
```
p(r, s'|s, a) = P[(Rt+1 = r, St+1 = s') | St = s, At = a]
```

**Return (accumulated rewards):**
```
Gt = Rt+1 + Î³Â·Rt+2 + Î³Â²Â·Rt+3 + ...
```

Where Î³ âˆˆ [0, 1] is the **discount factor**.

**Goal**: Find a Policy Ï€ : S â†’ A that maximizes E[Gt|St = s] for all s âˆˆ S.

**Markov Property:**
- Next State/Reward depends only on Current State (for a given Action)
- Current State encapsulates all relevant information from history
- Current State is a sufficient statistic of the future

### 1.6. Real-World Problems That Fit the MDP Framework
- Self-driving vehicles
- Game of Chess
- Complex Logistical Operations (Warehouse)
- Humanoid robot walking
- Investment portfolio management
- Football game decisions
- Election strategy

### 1.7. The Inherent Difficulty in Solving MDPs
- Large or complex State Space
- Large or complex Action Space
- No direct feedback on "correct" Action
- Time-sequenced complexity (actions influence future states)
- Delayed consequences
- Unknown model of environment
- Exploration vs Exploitation balance

### 1.8. Value Function, Bellman Equations, DP and RL

**Value Function for policy Ï€:**
```
V^Ï€(s) = E_Ï€,p[Gt|St = s] for all s âˆˆ S
```

**Bellman Equation (recursive):**
```
V^Ï€(s) = Î£_{r,s'} p(r, s'|s, Ï€(s)) Â· (r + Î³ Â· V^Ï€(s'))
```

**Optimal Value Function:**
```
V*(s) = max_Ï€ V^Ï€(s) for all s âˆˆ S
```

**Bellman Optimality Equation:**
```
V*(s) = max_a Î£_{r,s'} p(r, s'|s, a) Â· (r + Î³ Â· V*(s'))
```

**Key Problems:**
- **Prediction**: Calculate V^Ï€(s) for a given policy
- **Control**: Calculate V* and Ï€*

**Algorithm Types:**
- **Dynamic Programming**: Planning algorithms (requires knowing p)
- **Reinforcement Learning**: Learning algorithms (learns from interaction)

---

## MODULE I: PROCESSES AND PLANNING ALGORITHMS

## 3. Markov Processes (p.59)

### 3.1. The Concept of State in a Process
A state captures all relevant information needed to predict future behavior.

### 3.2-3.4. Markov Processes Formalism

**Markov Process Definition:**
- State Space S (countable set)
- Transition probability function: P(s'|s) for all s, s' âˆˆ S
- Starting state distribution
- Terminal states (optional)

### 3.8. Markov Reward Processes

**MRP adds:**
- Reward function R(s) or R(s, s')
- Discount factor Î³

**Value Function for MRP:**
```
V(s) = E[Gt|St = s] = E[Rt+1 + Î³Â·Rt+2 + Î³Â²Â·Rt+3 + ... | St = s]
```

**Bellman Equation for MRP:**
```
V(s) = R(s) + Î³ Â· Î£_{s'} P(s'|s) Â· V(s')
```

---

## 4. Markov Decision Processes (p.93)

### 4.3. Formal Definition of MDP
- State Space S
- Action Space A
- Transition probabilities: P(s'|s, a)
- Reward function: R(s, a, s') or R(s, a)
- Discount factor Î³

### 4.4. Policy
**Deterministic Policy**: Ï€ : S â†’ A
**Stochastic Policy**: Ï€(a|s) = P(At = a | St = s)

### 4.9. MDP Value Function for Fixed Policy

**State-Value Function:**
```
V^Ï€(s) = E_Ï€[Gt | St = s]
```

**Action-Value Function (Q-function):**
```
Q^Ï€(s, a) = E_Ï€[Gt | St = s, At = a]
```

**Relationship:**
```
V^Ï€(s) = Î£_a Ï€(a|s) Â· Q^Ï€(s, a)
Q^Ï€(s, a) = R(s, a) + Î³ Â· Î£_{s'} P(s'|s, a) Â· V^Ï€(s')
```

### 4.10. Optimal Value Function and Optimal Policies

**Optimal State-Value:**
```
V*(s) = max_Ï€ V^Ï€(s)
```

**Optimal Action-Value:**
```
Q*(s, a) = max_Ï€ Q^Ï€(s, a)
```

**Optimal Policy from Q*:**
```
Ï€*(s) = argmax_a Q*(s, a)
```

---

## 5. Dynamic Programming Algorithms (p.125)

### 5.1. Planning versus Learning
- **Planning**: Model known, compute optimal policy
- **Learning**: Model unknown, learn from experience

### 5.3. Fixed-Point Theory
Bellman operators are contractions â†’ unique fixed point exists.

### 5.4. Policy Evaluation Algorithm
Iteratively apply Bellman equation until convergence:
```
V_{k+1}(s) = Î£_a Ï€(a|s) Â· [R(s,a) + Î³ Â· Î£_{s'} P(s'|s,a) Â· V_k(s')]
```

### 5.5-5.7. Policy Improvement and Policy Iteration

**Greedy Policy:**
```
Ï€'(s) = argmax_a Q^Ï€(s, a)
```

**Policy Iteration:**
1. Initialize Ï€
2. Policy Evaluation: Compute V^Ï€
3. Policy Improvement: Ï€' = greedy(V^Ï€)
4. If Ï€' â‰  Ï€, set Ï€ = Ï€' and go to step 2
5. Return Ï€*

### 5.8. Value Iteration

**Bellman Optimality Operator:**
```
V_{k+1}(s) = max_a [R(s,a) + Î³ Â· Î£_{s'} P(s'|s,a) Â· V_k(s')]
```

Iterate until convergence.

### 5.13. Backward Induction (Finite Horizon)
For finite-horizon MDPs, solve backwards from terminal time T:
```
V_T(s) = R_T(s)
V_t(s) = max_a [R_t(s,a) + Î£_{s'} P(s'|s,a) Â· V_{t+1}(s')]
```

---

## 6. Function Approximation and ADP (p.163)

### 6.1. Why Function Approximation?
- State space too large for tabular methods
- Generalization to unseen states
- Memory efficiency

### 6.2. Linear Function Approximation
```
V(s; w) = w^T Â· Ï†(s) = Î£_i w_i Â· Ï†_i(s)
```
Where Ï†(s) is a feature vector.

### 6.3. Neural Network Function Approximation
Deep neural networks as universal function approximators.

**Training:**
- Forward propagation
- Loss computation
- Backpropagation
- Gradient descent update

### 6.5-6.6. Approximate DP Algorithms
- Approximate Policy Evaluation
- Approximate Value Iteration
- Fitted Value Iteration

---

## MODULE II: MODELING FINANCIAL APPLICATIONS

## 7. Utility Theory (p.199)

### 7.1. Introduction to Utility
People are typically **risk-averse**: they prefer certain outcomes over uncertain ones with same expected value.

### 7.3. Shape of Utility Function
- **Concave**: Risk-averse (most common)
- **Linear**: Risk-neutral
- **Convex**: Risk-seeking

### 7.4. Risk Premium
The amount of expected return an investor requires to accept uncertainty.

### 7.5. CARA - Constant Absolute Risk Aversion
```
U(x) = -e^{-Î±x} / Î±
```
Where Î± > 0 is the risk-aversion coefficient.

**Properties:**
- Absolute Risk Aversion: A(x) = -U''(x)/U'(x) = Î± (constant)
- Independent of wealth level

### 7.7. CRRA - Constant Relative Risk Aversion
```
U(x) = x^{1-Î³} / (1-Î³)  for Î³ â‰  1
U(x) = log(x)           for Î³ = 1
```

**Properties:**
- Relative Risk Aversion: R(x) = -xÂ·U''(x)/U'(x) = Î³ (constant)
- Scales with wealth

---

## 8. Dynamic Asset-Allocation and Consumption (p.211)

### 8.2. Merton's Portfolio Problem

**Setting:**
- Continuous time [0, T]
- Risk-free asset with return r
- Risky asset following GBM: dS/S = Î¼dt + Ïƒdz
- Wealth W_t
- Consumption rate c_t
- Portfolio allocation Ï€_t (fraction in risky asset)

**Objective:**
```
max E[âˆ«_0^T e^{-Ït} U(c_t) dt + e^{-ÏT} B(W_T)]
```

**Wealth dynamics:**
```
dW = [W(r + Ï€(Î¼-r)) - c]dt + WÏ€Ïƒdz
```

### 8.3. Merton's Solution (CRRA Utility)

**Optimal allocation:**
```
Ï€* = (Î¼ - r) / (Î³ÏƒÂ²)
```
Independent of wealth and time!

**Optimal consumption:**
```
c* = Î½ Â· W
```
Where Î½ depends on parameters.

**Key Insight:** Separation theorem - allocation decision independent of consumption decision.

---

## 9. Derivatives Pricing and Hedging (p.235)

### 9.1. Brief Introduction to Derivatives
- **Forwards**: Agreement to buy/sell at future date
- **European Options**: Right to buy (call) or sell (put) at expiry
- **American Options**: Can exercise any time before expiry

### 9.3-9.5. Fundamental Theorems of Asset Pricing

**1st FTAP:** No-arbitrage âŸº âˆƒ risk-neutral probability measure Q

**2nd FTAP:** Market complete âŸº Q is unique

### 9.6. Derivatives Pricing

**Complete Market:**
```
Price = E^Q[e^{-rT} Â· Payoff]
```

**Incomplete Market:**
Price bounds from super/sub-replication or utility-based pricing.

### 9.8. American Options as MDP

**State:** (t, S_t) or path history
**Action:** Exercise or Continue
**Reward:** Payoff if exercise, 0 otherwise

**Bellman Equation:**
```
V(t, s) = max{g(s), e^{-rÎ”t} Â· E[V(t+Î”t, S_{t+Î”t}) | S_t = s]}
```

Where g(s) is the payoff function.

### 9.10. Pricing/Hedging in Incomplete Market as MDP

**State:** (t, S_t, inventory)
**Action:** Hedge amount
**Objective:** Minimize hedging error + risk penalty

---

## 10. Order-Book Trading Algorithms (p.271)

### 10.1. Basics of Order Book

**Order Types:**
- **Market Order**: Execute immediately at best available price
- **Limit Order**: Execute only at specified price or better

**Order Book:** List of all outstanding limit orders
- **Bid side**: Buy orders
- **Ask side**: Sell orders
- **Spread**: Ask price - Bid price

### 10.2. Optimal Execution

**Problem:** Sell X shares over time [0, T] to maximize proceeds (minimize market impact).

**Market Impact:**
- **Temporary**: Price moves during execution, then reverts
- **Permanent**: Price moves permanently

### 10.2.1. Almgren-Chriss Model

**Assumptions:**
- Linear temporary impact: h(v) = Î·Â·v
- Linear permanent impact: g(v) = Î³Â·v
- Arithmetic random walk for price

**State:** (t, remaining_shares, current_price)
**Action:** Number of shares to sell at time t

**Optimal Solution (mean-variance):**
```
n*_t = (X/T) Â· sinh(Îº(T-t)) / sinh(ÎºT)
```

Where Îº depends on risk aversion and impact parameters.

**Key Insight:** Risk-averse trader front-loads execution.

### 10.3. Optimal Market-Making

**Problem:** Market maker quotes bid/ask prices to maximize profit while managing inventory risk.

### 10.3.1. Avellaneda-Stoikov Model

**State:** (t, S_t, inventory_q)
**Action:** Bid spread Î´^b, Ask spread Î´^a

**Dynamics:**
- Mid-price follows Brownian motion
- Order arrivals are Poisson with intensity Î»(Î´)

**Optimal Quotes:**
```
Î´^a = Î´^b = (1/Î³)Â·log(1 + Î³/k) + (Î³ÏƒÂ²(T-t))/2 Â· (2q + 1)
```

Where:
- Î³: risk aversion
- Ïƒ: volatility
- k: arrival rate parameter
- q: current inventory

**Key Insight:** Skew quotes based on inventory to mean-revert position.

---

## MODULE III: REINFORCEMENT LEARNING ALGORITHMS

## 11. Monte-Carlo and TD for Prediction (p.307)

### 11.3. Monte-Carlo (MC) Prediction

**Idea:** Estimate V^Ï€(s) by averaging returns from visits to state s.

**First-Visit MC:**
```python
for each episode:
    generate episode following Ï€
    for first visit to each state s:
        G = return from that point
        update: V(s) â† V(s) + Î±(G - V(s))
```

**Every-Visit MC:** Same but count all visits.

### 11.4. Temporal-Difference (TD) Prediction

**TD(0) Update:**
```
V(s) â† V(s) + Î±[r + Î³V(s') - V(s)]
```

**TD Target:** r + Î³V(s')
**TD Error:** Î´ = r + Î³V(s') - V(s)

### 11.5. TD versus MC

| Aspect | MC | TD |
|--------|----|----|
| Bias | Unbiased | Biased (bootstrap) |
| Variance | High | Low |
| Convergence | To V^Ï€ | To V^Ï€ (with conditions) |
| Data efficiency | Lower | Higher |
| Requires terminal | Yes | No |

### 11.6. TD(Î») - Eligibility Traces

**n-step Return:**
```
G_t^{(n)} = r_{t+1} + Î³r_{t+2} + ... + Î³^{n-1}r_{t+n} + Î³^n V(s_{t+n})
```

**Î»-Return (weighted average of n-step returns):**
```
G_t^Î» = (1-Î») Î£_{n=1}^âˆ Î»^{n-1} G_t^{(n)}
```

**Eligibility Trace:**
```
e_t(s) = Î³Î»Â·e_{t-1}(s) + ğŸ™(S_t = s)
```

**TD(Î») Update:**
```
V(s) â† V(s) + Î±Â·Î´_tÂ·e_t(s)  for all s
```

---

## 12. Monte-Carlo and TD for Control (p.345)

### 12.2-12.3. MC Control

**GLIE (Greedy in the Limit with Infinite Exploration):**
1. All state-action pairs visited infinitely often
2. Policy converges to greedy

**MC Control with Îµ-greedy:**
```python
for each episode:
    generate episode using Îµ-greedy policy
    for each (s, a) in episode:
        G = return from that point
        Q(s, a) â† Q(s, a) + Î±(G - Q(s, a))
    improve policy: Ï€(s) = Îµ-greedy(Q)
```

### 12.4. SARSA (On-Policy TD Control)

**Update:**
```
Q(s, a) â† Q(s, a) + Î±[r + Î³Q(s', a') - Q(s, a)]
```

Where a' is chosen by current policy from s'.

**Algorithm:**
```python
initialize Q(s, a)
for each episode:
    s = initial state
    a = Îµ-greedy(Q, s)
    while not terminal:
        take action a, observe r, s'
        a' = Îµ-greedy(Q, s')
        Q(s, a) â† Q(s, a) + Î±[r + Î³Q(s', a') - Q(s, a)]
        s, a = s', a'
```

### 12.6. Q-Learning (Off-Policy TD Control)

**Update:**
```
Q(s, a) â† Q(s, a) + Î±[r + Î³Â·max_{a'} Q(s', a') - Q(s, a)]
```

**Key difference from SARSA:** Uses max over actions (greedy w.r.t. Q), not actual next action.

**Properties:**
- Off-policy: learns optimal Q* regardless of behavior policy
- More sample efficient
- Can be unstable with function approximation

---

## 13. Batch RL, Experience-Replay, DQN, LSPI (p.381)

### 13.1. Experience Replay

**Idea:** Store experiences in buffer, sample randomly for updates.

**Benefits:**
- Breaks correlation in sequential data
- Reuses data multiple times
- More stable learning

### 13.4. Deep Q-Networks (DQN)

**Key innovations:**
1. **Experience Replay Buffer**
2. **Target Network:** Separate network for TD target, updated periodically

**Loss function:**
```
L(Î¸) = E[(r + Î³Â·max_{a'} Q(s', a'; Î¸^-) - Q(s, a; Î¸))Â²]
```

Where Î¸^- is the target network parameters.

**Algorithm:**
```python
initialize replay buffer D
initialize Q-network with random weights Î¸
initialize target network Î¸^- = Î¸

for each episode:
    for each step:
        select action (Îµ-greedy)
        execute action, observe r, s'
        store (s, a, r, s') in D
        
        sample minibatch from D
        compute targets: y = r + Î³Â·max_{a'} Q(s', a'; Î¸^-)
        gradient descent on (y - Q(s, a; Î¸))Â²
        
        periodically update Î¸^- = Î¸
```

### 13.5. Least-Squares Policy Iteration (LSPI)

**For linear function approximation:**
```
Q(s, a; w) = w^T Â· Ï†(s, a)
```

**LSTD for Q-function:**
Solve: Aw = b
Where:
```
A = Î£ Ï†(s,a) Â· [Ï†(s,a) - Î³Ï†(s',Ï€(s'))]^T
b = Î£ Ï†(s,a) Â· r
```

**LSPI Algorithm:**
```python
collect data {(s_i, a_i, r_i, s'_i)}
initialize policy Ï€
repeat:
    w = LSTDQ(data, Ï€)  # policy evaluation
    Ï€_new = greedy(w)    # policy improvement
until Ï€ converges
```

### 13.6. RL for American Options

**LSPI approach:**
- Features: polynomials of stock price
- Actions: exercise or continue
- State: (time, stock price, path features)

---

## 14. Policy Gradient Algorithms (p.415)

### 14.1. Motivation

**When to use Policy Gradient:**
- Large/continuous action spaces
- Stochastic policies needed
- Policy easier to represent than value function

### 14.2. Policy Gradient Theorem

**Objective:**
```
J(Î¸) = E_{Ï„~Ï€_Î¸}[R(Ï„)] = E_{s_0}[V^{Ï€_Î¸}(s_0)]
```

**Theorem:**
```
âˆ‡_Î¸ J(Î¸) = E_{Ï€_Î¸}[âˆ‡_Î¸ log Ï€_Î¸(a|s) Â· Q^{Ï€_Î¸}(s, a)]
```

**Score function:** âˆ‡_Î¸ log Ï€_Î¸(a|s)

### 14.4. REINFORCE (Monte-Carlo Policy Gradient)

**Update:**
```
Î¸ â† Î¸ + Î± Â· âˆ‡_Î¸ log Ï€_Î¸(a_t|s_t) Â· G_t
```

**Algorithm:**
```python
for each episode:
    generate trajectory Ï„ = (s_0, a_0, r_1, ..., s_T)
    for t = 0 to T-1:
        G_t = Î£_{k=t}^T Î³^{k-t} r_{k+1}
        Î¸ â† Î¸ + Î± Â· âˆ‡_Î¸ log Ï€_Î¸(a_t|s_t) Â· G_t
```

### 14.6. Actor-Critic

**Idea:** Use value function to reduce variance.

**Advantage function:**
```
A^Ï€(s, a) = Q^Ï€(s, a) - V^Ï€(s)
```

**Actor-Critic update:**
```
Critic: w â† w + Î±_w Â· Î´ Â· âˆ‡_w V(s; w)
Actor:  Î¸ â† Î¸ + Î±_Î¸ Â· Î´ Â· âˆ‡_Î¸ log Ï€_Î¸(a|s)
```

Where Î´ = r + Î³V(s'; w) - V(s; w) is the TD error.

### 14.8. Advanced Policy Gradient Methods

**Natural Policy Gradient:**
Uses Fisher information matrix for more stable updates.

**TRPO (Trust Region Policy Optimization):**
Constrains policy change per update.

**PPO (Proximal Policy Optimization):**
Clips objective to prevent large updates.

---

## MODULE IV: FINISHING TOUCHES

## 15. Multi-Armed Bandits (p.447)

### 15.1. Problem Definition

**Setting:**
- K arms (actions)
- Each arm has unknown reward distribution
- Goal: maximize cumulative reward

**Regret:**
```
Regret_T = TÂ·Î¼* - Î£_{t=1}^T Î¼_{A_t}
```

Where Î¼* is the best arm's mean.

### 15.2. Simple Algorithms

**Îµ-Greedy:**
- With prob Îµ: explore (random arm)
- With prob 1-Îµ: exploit (best arm so far)

**Decaying Îµ:** Îµ_t = 1/t

### 15.4. Upper Confidence Bound (UCB)

**UCB1:**
```
A_t = argmax_a [Q_t(a) + cÂ·âˆš(log(t)/N_t(a))]
```

Where N_t(a) is the number of times arm a was pulled.

**Intuition:** "Optimism in the face of uncertainty"

### 15.5. Thompson Sampling

**Bayesian approach:**
1. Maintain posterior distribution for each arm's mean
2. Sample from each posterior
3. Pull arm with highest sample

**For Bernoulli bandits with Beta prior:**
```python
for each round t:
    for each arm a:
        sample Î¸_a ~ Beta(Î±_a, Î²_a)
    pull arm a* = argmax_a Î¸_a
    update: if reward=1: Î±_{a*} += 1, else: Î²_{a*} += 1
```

---

## 16. Blending Learning and Planning (p.475)

### 16.1. Model-Based RL

**Approach:**
1. Learn environment model from experience
2. Plan using learned model
3. Execute and collect more data

**Dyna Architecture:**
- Direct RL: learn from real experience
- Model learning: fit model to experience
- Planning: simulate with model, update value/policy

### 16.3. Monte-Carlo Tree Search (MCTS)

**Four phases:**
1. **Selection:** Follow tree using UCB until leaf
2. **Expansion:** Add new node
3. **Simulation:** Random rollout to terminal
4. **Backpropagation:** Update statistics

**Used in:** AlphaGo, game playing

---

## 17. Summary and Real-World Considerations (p.487)

### Key Learnings

1. **MDP Framework:** Universal language for sequential decision problems
2. **Bellman Equations:** Foundation of all DP and RL
3. **DP Algorithms:** Exact solutions when model known
4. **RL Algorithms:** Learn from experience when model unknown
5. **Function Approximation:** Scale to large state spaces
6. **Exploration-Exploitation:** Fundamental tradeoff

### Real-World Challenges

- **Sample efficiency:** RL often needs lots of data
- **Safety:** Exploration can be dangerous
- **Reward design:** Hard to specify correctly
- **Partial observability:** Real states often hidden
- **Non-stationarity:** Environment changes over time
- **Sim-to-real gap:** Simulators imperfect

---

## APPENDICES

### Appendix B: Portfolio Theory (p.501)

**Efficient Frontier:** Set of portfolios with maximum return for given risk.

**CAPM:**
```
E[r_i] - r_f = Î²_i Â· (E[r_m] - r_f)
```

### Appendix C: Stochastic Calculus Basics (p.505)

**Brownian Motion Properties:**
- Continuous paths
- Independent increments
- W_t - W_s ~ N(0, t-s)

**Ito's Lemma:**
For f(t, X_t) where dX = Î¼dt + ÏƒdW:
```
df = (âˆ‚f/âˆ‚t + Î¼Â·âˆ‚f/âˆ‚x + Â½ÏƒÂ²Â·âˆ‚Â²f/âˆ‚xÂ²)dt + ÏƒÂ·âˆ‚f/âˆ‚xÂ·dW
```

### Appendix D: Hamilton-Jacobi-Bellman Equation (p.513)

**Continuous-time Bellman:**
```
0 = max_a {f(x,a) + âˆ‚V/âˆ‚t + Î¼Â·âˆ‚V/âˆ‚x + Â½ÏƒÂ²Â·âˆ‚Â²V/âˆ‚xÂ²}
```

### Appendix E: Black-Scholes (p.515)

**Black-Scholes PDE:**
```
âˆ‚V/âˆ‚t + rSÂ·âˆ‚V/âˆ‚S + Â½ÏƒÂ²SÂ²Â·âˆ‚Â²V/âˆ‚SÂ² - rV = 0
```

**Call option price:**
```
C = SÂ·N(dâ‚) - KÂ·e^{-rT}Â·N(dâ‚‚)
```

Where:
```
dâ‚ = [log(S/K) + (r + ÏƒÂ²/2)T] / (ÏƒâˆšT)
dâ‚‚ = dâ‚ - ÏƒâˆšT
```

---

## Key Equations Summary

### Value Functions
```
V^Ï€(s) = E_Ï€[Î£_{k=0}^âˆ Î³^k R_{t+k+1} | S_t = s]
Q^Ï€(s,a) = E_Ï€[Î£_{k=0}^âˆ Î³^k R_{t+k+1} | S_t = s, A_t = a]
```

### Bellman Equations
```
V^Ï€(s) = Î£_a Ï€(a|s) Â· Î£_{s',r} p(s',r|s,a)[r + Î³V^Ï€(s')]
V*(s) = max_a Î£_{s',r} p(s',r|s,a)[r + Î³V*(s')]
```

### TD Learning
```
V(S_t) â† V(S_t) + Î±[R_{t+1} + Î³V(S_{t+1}) - V(S_t)]
```

### Q-Learning
```
Q(S_t,A_t) â† Q(S_t,A_t) + Î±[R_{t+1} + Î³Â·max_a Q(S_{t+1},a) - Q(S_t,A_t)]
```

### Policy Gradient
```
âˆ‡_Î¸ J(Î¸) = E_Ï€[âˆ‡_Î¸ log Ï€_Î¸(a|s) Â· Q^Ï€(s,a)]
```

---

*Document extrait du livre "Foundations of Reinforcement Learning with Applications in Finance" par Ashwin Rao et Tikhon Jelvis (Stanford University). Pour usage Ã©ducatif.*
